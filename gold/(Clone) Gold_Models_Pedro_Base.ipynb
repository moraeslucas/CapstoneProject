{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df86d39f-a797-42c0-8263-4c3e5454cdb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--To check if there are duplicates in table contactos before the join , just for security\n",
    "\n",
    "SELECT\n",
    "  id,\n",
    "  data_criacao_da_lead,\n",
    "  COUNT(*) AS cnt\n",
    "FROM sc_gold.contactos_pbs\n",
    "GROUP BY id, data_criacao_da_lead\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY cnt DESC, id, data_criacao_da_lead;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d42b5d-8fb0-486f-9df7-8375592abeaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE sc_gold.`leads_pbs`;\n",
    "-- or\n",
    "SHOW COLUMNS IN sc_gold.`leads_pbs`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f35e18-59b4-4588-a1f4-00e4e75c0e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  created_time AS lead_created_time\n",
    "FROM sc_silver.leads_pbs\n",
    "LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e7d5ec-1780-42bf-8857-907dee087687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  data_criacao_da_lead\n",
    "FROM sc_silver.contactos_pbs\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722797fb-e955-4e72-a980-49e90d182f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "\n",
    "SELECT * FROM sc_gold.campaigns             LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a83bf42-5883-4081-bd46-59801fb7e35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.leads_pbs             LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92a373b-c31d-4fb5-a69b-eb1ec4835b61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sc_gold.contactos_pbs         LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eabf9ef-a305-4a84-869b-bec6ea220f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.propostas_realizadas  LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9a2714-e79b-4a59-9d70-fcd4575cc79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.deals                 LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfa7f30-b7b1-42b5-afff-e6b1edf03d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.contactos_pbs         LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7404c247-7710-45f4-8af6-8a32adb70f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build and run: CREATE OR REPLACE TABLE sc_gold.contactos_pbs_2 AS SELECT ... FROM sc_gold.contactos_pbs\n",
    "\n",
    "# 1) Read source columns in order\n",
    "cols = spark.sql(\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM information_schema.columns\n",
    "  WHERE table_schema = 'sc_gold'\n",
    "    AND table_name   = 'contactos_pbs'\n",
    "  ORDER BY ordinal_position\n",
    "\"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "# Optional: columns you do NOT want renamed (e.g., keep id as-is)\n",
    "skip = set()   # e.g., {'id'}\n",
    "\n",
    "# 2) Build the select list with aliases\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    new = c if (c.endswith('_contactos') or c in skip) else f\"{c}_contactos\"\n",
    "    select_exprs.append(f\"`{c}` AS `{new}`\")\n",
    "\n",
    "select_sql = \", \".join(select_exprs)\n",
    "\n",
    "# 3) Build the full CREATE TABLE AS SELECT statement\n",
    "create_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE sc_gold.contactos_pbs_2 AS\n",
    "SELECT {select_sql}\n",
    "FROM sc_gold.contactos_pbs\n",
    "\"\"\"\n",
    "\n",
    "print(create_sql)  # shows exactly what will run\n",
    "spark.sql(create_sql)\n",
    "\n",
    "# 4) Quick verification\n",
    "spark.sql(\"DESCRIBE TABLE sc_gold.contactos_pbs_2\").show(truncate=False)\n",
    "spark.sql(\"SELECT * FROM sc_gold.contactos_pbs_2 LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432f8a4a-597b-4463-b7a9-53471c5de947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.contactos_pbs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b1cd2b-2e6a-4cef-9f08-ae524008923c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source and target\n",
    "source_table = \"sc_gold.propostas_realizadas\"\n",
    "target_table = \"sc_gold.propostas_realizadas2\"\n",
    "\n",
    "# 1) Get columns\n",
    "cols = spark.sql(f\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM information_schema.columns\n",
    "  WHERE table_schema = 'sc_gold'\n",
    "    AND table_name   = 'propostas_realizadas'\n",
    "  ORDER BY ordinal_position\n",
    "\"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "# Optional: columns to keep unchanged\n",
    "skip = set()   # e.g., {'id'}\n",
    "\n",
    "# 2) Build select list with renamed aliases\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    new = c if (c.endswith('_propostas') or c in skip) else f\"{c}_propostas\"\n",
    "    select_exprs.append(f\"`{c}` AS `{new}`\")\n",
    "\n",
    "select_sql = \", \".join(select_exprs)\n",
    "\n",
    "# 3) Build CREATE TABLE AS SELECT\n",
    "create_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {target_table} AS\n",
    "SELECT {select_sql}\n",
    "FROM {source_table}\n",
    "\"\"\"\n",
    "\n",
    "print(create_sql)   # Show what will run\n",
    "spark.sql(create_sql)\n",
    "\n",
    "# 4) Verify\n",
    "spark.sql(f\"DESCRIBE TABLE {target_table}\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {target_table} LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f58f63-0892-4b59-8c1c-b6829bc0c796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.propostas_realizadas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae47343-02f0-41d9-8cd3-2cab918d5090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source and target\n",
    "source_table = \"sc_gold.deals\"\n",
    "target_table = \"sc_gold.deals2\"\n",
    "\n",
    "# 1) Get columns\n",
    "cols = spark.sql(f\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM information_schema.columns\n",
    "  WHERE table_schema = 'sc_gold'\n",
    "    AND table_name   = 'deals'\n",
    "  ORDER BY ordinal_position\n",
    "\"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "# Optional: columns to keep unchanged (e.g., primary keys)\n",
    "skip = set()   # Example: {'id'}\n",
    "\n",
    "# 2) Build select list with renamed aliases\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    new = c if (c.endswith('_deals') or c in skip) else f\"{c}_deals\"\n",
    "    select_exprs.append(f\"`{c}` AS `{new}`\")\n",
    "\n",
    "select_sql = \", \".join(select_exprs)\n",
    "\n",
    "# 3) Build CREATE TABLE AS SELECT\n",
    "create_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {target_table} AS\n",
    "SELECT {select_sql}\n",
    "FROM {source_table}\n",
    "\"\"\"\n",
    "\n",
    "print(create_sql)   # Show generated SQL\n",
    "spark.sql(create_sql)\n",
    "\n",
    "# 4) Verify structure and sample data\n",
    "spark.sql(f\"DESCRIBE TABLE {target_table}\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {target_table} LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9770184b-84d8-4485-a093-1662d51c64c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.deals2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa4c88d-917e-4a4a-bebc-1985c7aac121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source and target\n",
    "source_table = \"sc_gold.campaigns\"\n",
    "target_table = \"sc_gold.campaigns2\"\n",
    "\n",
    "# 1) Get columns\n",
    "cols = spark.sql(f\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM information_schema.columns\n",
    "  WHERE table_schema = 'sc_gold'\n",
    "    AND table_name   = 'campaigns'\n",
    "  ORDER BY ordinal_position\n",
    "\"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "# Optional: columns to keep unchanged (like primary keys)\n",
    "skip = set()   # Example: {'id'}\n",
    "\n",
    "# 2) Build select list with renamed aliases\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    new = c if (c.endswith('_campaigns') or c in skip) else f\"{c}_campaigns\"\n",
    "    select_exprs.append(f\"`{c}` AS `{new}`\")\n",
    "\n",
    "select_sql = \", \".join(select_exprs)\n",
    "\n",
    "# 3) Build CREATE TABLE AS SELECT\n",
    "create_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {target_table} AS\n",
    "SELECT {select_sql}\n",
    "FROM {source_table}\n",
    "\"\"\"\n",
    "\n",
    "print(create_sql)   # Preview generated SQL\n",
    "spark.sql(create_sql)\n",
    "\n",
    "# 4) Verify new table\n",
    "spark.sql(f\"DESCRIBE TABLE {target_table}\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {target_table} LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425e8ffd-ca7b-4368-bbd5-8cd16ea9db54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.campaigns2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a30767-95aa-4496-aac3-c4c5e6be7ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS strict_matches\n",
    "FROM sc_gold.leads_pbs l\n",
    "JOIN sc_gold.contactos_pbs_2 c\n",
    "  ON  l.converted_contact = c.id_contactos\n",
    "  AND l.created_time      = c.data_criacao_da_lead_contactos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0865170-1842-497b-9696-551f854c5da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH joined AS (\n",
    "  SELECT\n",
    "    l.*,\n",
    "    c.*\n",
    "  FROM sc_gold.leads_pbs       AS l\n",
    "  JOIN sc_gold.contactos_pbs_2 AS c\n",
    "    ON  l.converted_contact = c.id_contactos\n",
    "   AND l.created_time      = c.data_criacao_da_lead_contactos\n",
    ")\n",
    "SELECT COUNT(*) AS check_count FROM joined;   -- should be ~118329\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62ef095e-3704-4379-b4d6-0cdb1792d368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- (optional) ensure you're in the right place\n",
    "-- USE CATALOG workspace;\n",
    "-- USE SCHEMA sc_gold;\n",
    "\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT\n",
    "  l.*,\n",
    "  c.*\n",
    "FROM sc_gold.leads_pbs       AS l\n",
    "JOIN sc_gold.contactos_pbs_2 AS c\n",
    "  ON  l.converted_contact = c.id_contactos\n",
    "  AND l.created_time      = c.data_criacao_da_lead_contactos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d674cd-2ce7-4a87-b60e-abe1b3f6833d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  f.*,\n",
    "  p.*   -- assumes Propostas columns are already suffixed (e.g., *_propostas)\n",
    "FROM sc_gold.Features_Table AS f\n",
    "LEFT JOIN sc_gold.propostas_realizadas2 AS p\n",
    "  ON f.id_contactos = p.id_contacto_propostas;   -- keep all Feature rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a780d349-2705-42de-938f-1b41e3b5e5ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  f.*,\n",
    "  d.*   -- assumes Deals columns are already suffixed (e.g., *_deals)\n",
    "FROM sc_gold.Features_Table AS f\n",
    "LEFT JOIN sc_gold.deals2 AS d\n",
    "  ON f.id_contacto_propostas = d.id_contacto_deals;   -- keep all Feature rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c51127-a90a-44b0-9f0e-cd9458ef8d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  f.*,\n",
    "  cp.*   -- assumes campaigns columns are already suffixed (e.g., *_campaigns)\n",
    "FROM sc_gold.Features_Table AS f\n",
    "LEFT JOIN sc_gold.campaigns2 AS cp\n",
    "  ON f.campanha_deals = cp.id_campaigns;   -- join via Features_Table.campanha_deals -> campaigns.id_campaigns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9870b82-3a74-412b-8165-0c3fd24a36a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.fact_table AS\n",
    "SELECT\n",
    "    l.id,\n",
    "  l.converted_contact,\n",
    "  l.created_time,\n",
    "  c.id_contactos,\n",
    "  c.data_criacao_da_lead_contactos\n",
    "FROM sc_gold.leads_pbs AS l\n",
    "JOIN sc_gold.contactos_pbs_2 AS c\n",
    "  ON l.converted_contact = c.id_contactos\n",
    " AND l.created_time = c.data_criacao_da_lead_contactos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363d9117-13c3-492b-a5a4-0aecac0aae2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.fact_table AS\n",
    "SELECT\n",
    "  f.*,\n",
    " \n",
    "    p.id_contacto_propostas,\n",
    "  p.id_proposta_realizada_propostas\n",
    "\n",
    "FROM sc_gold.fact_table AS f\n",
    "LEFT JOIN sc_gold.propostas_realizadas2 AS p\n",
    "  ON p.id_contacto_propostas = f.id_contactos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57fc58b1-56bb-4230-a034-35a3e8e45345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Fact_table AS\n",
    "SELECT\n",
    "  f.*,\n",
    "  d.id_deals,\n",
    "  d.id_contacto_deals,\n",
    "  d.campanha_deals\n",
    "FROM sc_gold.Fact_table AS f\n",
    "LEFT JOIN sc_gold.deals2 AS d\n",
    "  ON f.id_contacto_propostas = d.id_contacto_deals;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d63b20-c30f-429d-8a99-f0100c51c67b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# snapshot the current table to a temp view\n",
    "spark.sql(\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM sc_gold.fact_table\")\n",
    "\n",
    "to_drop = {'converted_contact','data_criacao_da_lead_contactos','id_contacto_propostas','id_contacto_deals'}\n",
    "cols = [f\"`{c}`\" for c in spark.table(\"sc_gold.fact_table\").columns if c not in to_drop]\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE sc_gold.fact_table\n",
    "USING DELTA AS\n",
    "SELECT {', '.join(cols)} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c24d33-8dfa-4150-8e7e-5b01aa17d823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1) Summary counts of duplicates on (id, created_time)\n",
    "WITH d AS (\n",
    "  SELECT id, created_time, COUNT(*) AS cnt\n",
    "  FROM sc_gold.leads_pbs\n",
    "  GROUP BY id, created_time\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)                    AS duplicated_keys,          -- number of (id,created_time) combos with >1 row\n",
    "  SUM(cnt)                    AS rows_in_duplicated_keys,  -- total rows participating in duplicates\n",
    "  SUM(cnt) - COUNT(*)         AS extra_rows_to_dedup       -- rows to drop if you keep 1 per key\n",
    "FROM d;\n",
    "\n",
    "-- 2) See the duplicated keys and their sizes (top 100)\n",
    "SELECT id, created_time, COUNT(*) AS cnt\n",
    "FROM sc_gold.leads_pbs\n",
    "GROUP BY id, created_time\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY cnt DESC, id\n",
    "LIMIT 100;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5feabc3e-6365-4c20-bd73-9825cc33590e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"_fivetran_id\":153},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"link_centro_consentimento_contactos\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1757282068950}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SUMMARY: how many duplicate (id, created_time) keys exist in sc_gold.features ( this is normal , can have multiple proposals)\n",
    "WITH d AS (\n",
    "  SELECT id, created_time, COUNT(*) AS cnt\n",
    "  FROM sc_gold.features_table\n",
    "  GROUP BY id, created_time\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)            AS duplicated_keys,\n",
    "  SUM(cnt)            AS rows_in_duplicated_keys,\n",
    "  SUM(cnt) - COUNT(*) AS extra_rows_to_dedup\n",
    "FROM d;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df364f7-ff68-4bd4-b302-064f4f1980ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SUMMARY: how many duplicate (id_propostas, created_time) keys exist\n",
    "WITH d AS (\n",
    "  SELECT id, created_time, COUNT(*) AS cnt\n",
    "  FROM sc_gold.propostas_realizadas\n",
    "  GROUP BY id, created_time\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)            AS duplicated_keys,          -- number of key pairs with >1 row\n",
    "  SUM(cnt)            AS rows_in_duplicated_keys,  -- total rows across those keys\n",
    "  SUM(cnt) - COUNT(*) AS extra_rows_to_dedup       -- rows you’d remove if keeping 1 per key\n",
    "FROM d;\n",
    "\n",
    "-- DETAILS: which keys are duplicated (top 100 by size)\n",
    "SELECT id, created_time, COUNT(*) AS cnt\n",
    "FROM sc_gold.propostas_realizadas\n",
    "GROUP BY id, created_time\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY cnt DESC, id\n",
    "LIMIT 100;\n",
    "\n",
    "-- OPTIONAL: list the actual duplicate rows\n",
    "SELECT t.*\n",
    "FROM sc_gold.propostas_realizadas t\n",
    "JOIN (\n",
    "  SELECT id, created_time\n",
    "  FROM sc_gold.propostas_realizadas\n",
    "  GROUP BY id, created_time\n",
    "  HAVING COUNT(*) > 1\n",
    ") d\n",
    "ON t.id = d.id AND t.created_time = d.created_time\n",
    "ORDER BY t.id, t.created_time;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ec881c-6915-4158-9219-3e9ce2f54a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH d AS (\n",
    "  SELECT\n",
    "    `id`,\n",
    "    `id_deals`,\n",
    "    `id_proposta_realizada_propostas`,\n",
    "    COUNT(*) AS cnt\n",
    "  FROM sc_gold.features_table\n",
    "  GROUP BY `id`, `id_deals`, `id_proposta_realizada_propostas`\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)              AS duplicated_keys,\n",
    "  SUM(d.cnt)            AS rows_in_duplicated_keys,\n",
    "  SUM(d.cnt) - COUNT(*) AS extra_rows_to_dedup\n",
    "FROM d AS d;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6c2d6d-4d0c-4752-be9d-3a6dc59b7811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH d AS (\n",
    "  SELECT\n",
    "    `id`,\n",
    "    `id_deals`,\n",
    "    `id_proposta_realizada_propostas`,\n",
    "    COUNT(*) AS cnt\n",
    "  FROM sc_gold.fact_table\n",
    "  GROUP BY `id`, `id_deals`, `id_proposta_realizada_propostas`\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)              AS duplicated_keys,\n",
    "  SUM(d.cnt)            AS rows_in_duplicated_keys,\n",
    "  SUM(d.cnt) - COUNT(*) AS extra_rows_to_dedup\n",
    "FROM d AS d;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41e3eb5-8f18-463f-8493-cae7ef8c27d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Fact_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5312eeb-6ba6-45c3-a9c2-46ccd98f3d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f46215a-226d-4aca-80df-1de1d1be643b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Build keep list to Ml Models\n",
    "cols_to_delete = [  # the  names we listed\n",
    "    \"_fivetran_id\",\"id\",\"lead_owner\",\"lead_owner_name\",\"consentimento\",\n",
    "    \"id_lead\",\"chave_instalacao_bd_rede\",\"chave_concessao_bd_rede\",\"chave_contrato_concessionario_bd_rede\",\n",
    "    \"chave_agrupamento_performance_vendas\",\"chave_agrupamento_cliente\",\n",
    "    \"data_de_validade_de_consentimento_hyundai\",\"lead_response_time\",\"converted_date_time\",\n",
    "    \"_fivetran_deleted\",\"_fivetran_synced\",\"_fivetran_id_contactos\",\"id_contactos\",\n",
    "    \"contacto_owner_contactos\",\"contacto_owner_name_contactos\",\"created_time_contactos\",\n",
    "    \"id_contacto_contactos\",\"consentimento_contactos\",\"id_lead_conversao_contactos\",\n",
    "    \"data_de_recolha_de_consentimento_hyundai_contactos\",\"chave_concessao_bd_rede_contactos\",\n",
    "    \"chave_instalacao_bd_rede_contactos\",\"chave_contrato_concessionario_bd_rede_contactos\",\n",
    "    \"chave_agrupamento_performance_vendas_contactos\",\"chave_agrupamento_cliente_contactos\",\n",
    "    \"link_centro_consentimento_contactos\",\"_fivetran_deleted_contactos\",\"_fivetran_synced_contactos\",\n",
    "    \"_fivetran_id_propostas\",\"id_propostas\",\"created_time_propostas\",\n",
    "    \"proposta_realizada_owner_propostas\",\"proposta_realizada_owner_name_propostas\",\n",
    "    \"modified_time_propostas\",\"conta_name_propostas\",\"id_contacto_propostas\",\"contacto_name_propostas\",\n",
    "    \"deal_name_propostas\",\"created_by_propostas\",\"modified_by_propostas\",\"descricao_apoio_propostas\",\n",
    "    \"codigo_da_proposta_propostas\",\"descricao_pintura_propostas\",\"data_de_criacao_da_proposta_propostas\",\n",
    "    \"data_de_entrega_da_proposta_propostas\",\"id_conta_propostas\",\"id_negocio_propostas\",\n",
    "    \"id_proposta_realizada_propostas\",\"data_de_validade_da_proposta_propostas\",\"codigo_modelo_propostas\",\n",
    "    \"sufixo_modelo_propostas\",\"codigo_cor_exterior_propostas\",\"codigo_cor_interior_propostas\",\n",
    "    \"concessionario_owner_propostas\",\"data_prevista_matricula_propostas\",\"data_prevista_de_entrega_propostas\",\n",
    "    \"data_da_conclusao_propostas\",\"instalacao_propostas\",\"chave_instalacao_bd_rede_propostas\",\n",
    "    \"chave_concessao_bd_rede_propostas\",\"cod_proposta_propostas\",\"codigo_fabricante_cor_interior_propostas\",\n",
    "    \"codigo_fabricante_cor_exterior_propostas\",\"_fivetran_deleted_propostas\",\"_fivetran_synced_propostas\",\n",
    "    \"_fivetran_id_deals\",\"deal_owner_name_deals\",\"id_contacto_deals\",\n",
    "    \"closing_date_deals\",\"deal_owner_deals\",\"conta_name_deals\",\"contacto_name_deals\",\"id_negocio_deals\",\n",
    "    \"data_prevista_de_reentrada_em_negocio_deals\",\"id_conta_deals\",\"codigo_negocio_deals\",\"data_venda_deals\",\n",
    "    \"nome_contacto_deals\",\"id_deals\",\"data_decisao_negocio_deals\",\"_fivetran_deleted_deals\",\n",
    "    \"_fivetran_synced_deals\",\"_fivetran_id_campaigns\",\"id_campaigns\",\n",
    "    \"campaign_owner_name_campaigns\",\"created_time_campaigns\",\"codigo_campanha_campaigns\",\n",
    "    \"_fivetran_deleted_campaigns\",\"_fivetran_synced_campaigns\"\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # quote names safely\n",
    "\n",
    "# 2) Snapshot and recreate from projection\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "spark.sql(\"DROP VIEW _src\")\n",
    "\n",
    "#You should generally remove columns that:\n",
    "#Are unique identifiers (e.g., _fivetran_id, id, id_lead, id_contactos, id_propostas, id_deals, id_campaigns).\n",
    "#Are timestamps or sync markers from ETL (_fivetran_synced, _fivetran_deleted, etc.).\n",
    "#Contain names of people or owners (lead_owner_name, campaign_owner_name_campaigns, etc.).\n",
    "#Are free-text codes or descriptive identifiers (codigo_campanha_campaigns, codigo_da_proposta_propostas, etc.).\n",
    "#Are internal foreign keys / lookup fields (chave_instalacao_bd_rede, chave_agrupamento_cliente, etc.).\n",
    "#Are emails / links (link_centro_consentimento_contactos).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1b6949-42ec-4c6a-9daa-809e5fdb04d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Built the second delete list after a better analysis \n",
    "cols_to_delete = [\n",
    "    # --- timestamps / dates ---\n",
    "  \n",
    "    \"data_criacao_da_lead_contactos\",\n",
    "    \"data_prevista_de_entrega_deals\",\n",
    "    \"data_prevista_matricula_deals\",\n",
    "    \"data_criacao_da_lead_deals\",\n",
    "\n",
    "    # --- IDs / keys ---\n",
    "    \"created_by\", \"modified_by\",\n",
    "    \"id_classe_propostas\", \"id_model_group_propostas\",\n",
    "    \"id_combustivel_propostas\", \"id_modelo_propostas\",\n",
    "    \"chave_agrupamento_cliente_propostas\",\n",
    "    \"tylacode_propostas\",\n",
    "    \"chave_concessao_bd_rede_deals\", \"chave_instalacao_bd_rede_deals\",\n",
    "    \"chave_contrato_concessionario_bd_rede_deals\",\n",
    "    \"chave_agrupamento_performance_vendas_deals\", \"chave_agrupamento_cliente_deals\",\"campanha_deals\",\n",
    "\n",
    "    # --- descriptive text ---\n",
    "    \"descricao_model_group_propostas\",\n",
    "    \n",
    "\n",
    "    # --- administrative fields ---\n",
    "     \"gestor_area_propostas\",\n",
    "\n",
    "    # --- duplicated / rarely useful flags ---\n",
    "    \"hubleads__nao_comunica_com_cc_\",\n",
    "    \"converted_contact\", \"converted_deal\", \"converted_from_lead_deals\",\n",
    "    \"ccupdated_contactos\", \"myhyundai_contactos\",\n",
    "    \"f2_deals\", \"modelovendaupdate_deals\"\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# 2) Snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df06f4c0-5120-47ed-8b15-8039e685a0fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN month(created_time) IN (12, 1, 2) THEN 'Winter'\n",
    "    WHEN month(created_time) IN (3, 4, 5) THEN 'Spring'\n",
    "    WHEN month(created_time) IN (6, 7, 8) THEN 'Summer'\n",
    "    ELSE 'Autumn'\n",
    "  END AS Season\n",
    "FROM sc_gold.Features_Table;\n",
    "\n",
    "--I created a Season for ML Models ( thinking in the prediction already)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff194c0e-c764-4706-a4ea-3c5e9d0455fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  datediff(created_time_deals, created_time) AS lead_to_deal_days\n",
    "FROM sc_gold.Features_Table;\n",
    "\n",
    "-- this is deals date- leads date to give the number of days that take , for my surprise there are negatives but can be typos or retroactive data entry → A deal was created manually before the lead was officially entered in the CRM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9728670-19b5-4403-8aca-93b0f1a50999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN datediff(created_time_deals, created_time) <= 7 THEN 'Fast'\n",
    "    WHEN datediff(created_time_deals, created_time) <= 30 THEN 'Medium'\n",
    "    ELSE 'Slow'\n",
    "  END AS deal_speed_category\n",
    "FROM sc_gold.Features_Table;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352a5a3f-0cb8-4345-bb8f-9fc00111959b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN created_time_deals < created_time THEN 1\n",
    "    ELSE 0\n",
    "  END AS deal_before_lead\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090334ba-e4a9-4955-8a5c-5a9a59b0bb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN dayofweek(created_time) IN (1,7) THEN 1\n",
    "    ELSE 0\n",
    "  END AS is_weekend_lead\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2063a1bd-e3cd-4732-805c-548510bf3251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN day(created_time_deals) >= 25 THEN 1\n",
    "    ELSE 0\n",
    "  END AS is_month_end_deal\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1562696-b36d-4a96-98af-2a7c2b70a9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN qtd_viaturas_deals = 0 OR qtd_viaturas_deals IS NULL \n",
    "        THEN 0\n",
    "        ELSE num_de_propostas_deals / qtd_viaturas_deals\n",
    "    END AS proposals_per_vehicle\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27b647e-2347-410a-8f97-ef06aca63641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  year(created_time)  AS lead_year,\n",
    "  month(created_time) AS lead_month\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c5568e-ad1f-45da-82d6-7a44c56f8825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.features_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a6f101-4b85-4b98-a910-55d3d117c40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_delete = [\n",
    "    \"created_time\",\n",
    "    \"activities_involved\",\n",
    "    \"created_time_deals\"\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6075ba41-e551-469c-b104-b9db9b8c4982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef8019f-41b0-46bf-9b3f-b042ccc74a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  is_converted,\n",
    "  COUNT(*) AS total_leads\n",
    "FROM sc_gold.leads_pbs\n",
    "GROUP BY is_converted\n",
    "ORDER BY is_converted;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a406e380-36b9-41af-8319-6646bf2200d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \"lead_status\",                    # leakage: deal-stage info\n",
    "    \"lead_status_2\",                  # leakage: deal-stage variant\n",
    "        \"forecast_type_deals\",                  # leakage\n",
    "    \"motivo_fecho_negocio_deals\",           # leakage: reason for closing\n",
    "     \"categoria_negocio_venda_deals\",        # leakage; also mostly null in many exports\n",
    "    \"modelo_de_venda_deals\",                # leakage; often mostly null\n",
    "    \"tipo_de_negocio_deals\",                # leakage; often mostly null\n",
    "        \"tipo_de_pedido\",                 # leakage\n",
    "         \"tipo_de_pedido_contactos\",                 # leakage\n",
    "           \"categoria_proposta_propostas\",                       # leakage\n",
    "           \"activities_involved_deals\",            # leakage: totals up to close\n",
    " \n",
    "\n",
    "    # ----------------------------\n",
    "    # ID-like / keys (non-predictive & high-unique)\n",
    "    # ----------------------------\n",
    "    \"contrato_concessionario_deals\",        # ID-like (deal)\n",
    "    \"contrato_concessionario\",              # ID-like\n",
    "    \"contrato_concessionario_contactos\",    # ID-like\n",
    "    \"concessionario_escolhido\",             # ID-like / chosen dealer\n",
    "    \"estado_do_pedido_propostas\",           # often key-like / admin-ish\n",
    "\n",
    "    # ----------------------------\n",
    "    # Data quality: free text / high null / near-constant / high-cardinality\n",
    "    # ----------------------------\n",
    "    \"descricao_classe_propostas\",           # free text / mostly null\n",
    "    \"subject_propostas\",                    # free text / high-cardinality\n",
    "    \"margem_frota_eni_propostas\",           # mostly null\n",
    "    \"sub_total_com_extras_propostas\",       # mostly null\n",
    "    \"valor_aprovado_propostas\",             # mostly null\n",
    "    \"valor_extras_propostas\",               # mostly null\n",
    "    \"valor_do_apoio_pedido_propostas\",      # mostly null; also key-like in some exports\n",
    "    \"n__matriculas_associadas_deals\"        # near-constant / mostly null (deal-level)\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950e162e-5e6e-4914-a4fa-16b25ea7f4cd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757809256273}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b87046-3c3c-448e-a0dd-64049529f782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "tbl = \"sc_gold.Features_Table\"\n",
    "cols = [\n",
    "    \"concessao\",\n",
    "    \"instalacao\",\n",
    "    \"agrupamento_performance_vendas\",\n",
    "    \"agrupamento_cliente\",\n",
    "    \"concessao_contactos\",\n",
    "    \"instalacao_contactos\",\n",
    "    \"agrupamento_performance_vendas_contactos\",\n",
    "    \"agrupamento_cliente_contactos\",\n",
    "    \"concessao_propostas\",\n",
    "    \"concessao_deals\",\n",
    "    \"instalacao_deals\",\n",
    "    \"agrupamento_performance_vendas_deals\",\n",
    "    \"agrupamento_cliente_deals\",\n",
    "]\n",
    "\n",
    "df = spark.table(tbl)\n",
    "present = [c for c in cols if c in df.columns]\n",
    "exprs = [\n",
    "    (F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)) for c in present\n",
    "]\n",
    "res = df.agg(*exprs).collect()[0].asDict()\n",
    "total = df.count()\n",
    "\n",
    "out = [{\"column\": c, \"null_count\": int(res.get(c, 0)), \"rows_total\": total,\n",
    "        \"null_pct\": f\"{(res.get(c, 0)/total)*100:.2f}%\"} for c in present]\n",
    "for c in cols:\n",
    "    if c not in present:\n",
    "        out.append({\"column\": c, \"null_count\": \"—\", \"rows_total\": total, \"null_pct\": \"not in table\"})\n",
    "\n",
    "spark.createDataFrame(out).orderBy(\"column\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2975a9a6-7462-4ab8-b4b7-a2982216f37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \"lead_status\",                    # leakage: deal-stage info\n",
    "    \"instalacao\",\n",
    "    \"agrupamento_performance_vendas\",\n",
    "    \"agrupamento_cliente\",\n",
    "    \"instalacao_contactos\",\n",
    "    \"agrupamento_performance_vendas_contactos\",\n",
    "    \"agrupamento_cliente_contactos\",\n",
    "    \"concessao_propostas\",\n",
    "    \"concessao_deals\",\n",
    "    \"instalacao_deals\",\n",
    "    \"agrupamento_performance_vendas_deals\",\n",
    "    \"agrupamento_cliente_deals\" ]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179b8217-86cf-44ed-bbcd-acdc09f36885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#since nulls are only 0.07%, filling with the mode (most frequent value) is a very good option.\n",
    "\n",
    "\n",
    "col_name = \"tipo_cliente_contactos\"\n",
    "\n",
    "# get mode (most frequent value)\n",
    "mode_value = (\n",
    "    df.groupBy(col_name)\n",
    "      .count()\n",
    "      .orderBy(F.desc(\"count\"))\n",
    "      .first()[0]\n",
    ")\n",
    "\n",
    "# fill nulls with mode\n",
    "df = df.na.fill({col_name: mode_value})\n",
    "\n",
    "# overwrite the existing table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"sc_gold.Features_Table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36809eaf-f4af-4d3f-bbca-f54fe3f7f9a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# proposals_per_vehicle with small percentage of nulls (2.53%) we are going to use the median, is more robust.\n",
    "#Fill with mean/median\n",
    "#Median = more robust to outliers.\n",
    "\n",
    "col_name = \"proposals_per_vehicle\"\n",
    "\n",
    "# calculate median\n",
    "median_val = df.approxQuantile(col_name, [0.5], 0.01)[0]  # 0.5 = median, tolerance=0.01\n",
    "\n",
    "# print the median\n",
    "print(f\"Median value for {col_name}: {median_val}\")\n",
    "\n",
    "# fill nulls with median\n",
    "df = df.na.fill({col_name: median_val})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16088a91-a52f-4cbd-a735-6a6b4a63e476",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757862983349}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2641fe-21a2-4d17-bdd3-91d556279a3d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"column\":239},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757857561261}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "row_count = df.count()\n",
    "dtypes = dict(df.dtypes)\n",
    "\n",
    "# --- STRICT NULLS ONLY (what you asked) ---\n",
    "null_aggs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    "null_row = df.agg(*null_aggs).collect()[0].asDict()\n",
    "\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    n_null = int(null_row.get(c, 0))\n",
    "    rows.append((c, dtypes[c], n_null, row_count, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"dtype\", \"null_count\", \"rows_total\", \"null_pct\"])\n",
    "display(out.orderBy(F.desc(\"null_pct\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a49f0f-339f-49c6-8c0a-b70ec7a4d02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \"lead_status\",  \n",
    "      \"apoio_total_propostas\",\n",
    "    \"forma_de_pagamento_propostas\",\n",
    "    \"apoio_concessionario_propostas\",\n",
    "    \"comparticipacao_retoma_concessao_propostas\",\n",
    "    \"apoio_retoma_hyundai_propostas\",\n",
    "    \"apoio_financiamento_cetelem_propostas\",\n",
    "    \"formulario_deals\",\n",
    "    \"comparticipacao_campanha_da_concessao_propostas\",\n",
    "    \"lead_conversion_time_deals\",\n",
    "    \"apoio_campanha_hyundai_propostas\",\n",
    "    \"probability_____deals\",\n",
    "    \"distrito\",\n",
    "    \"apoio_hyundai_portugal_propostas\",\n",
    "    \"valor_campanha_propostas\",\n",
    "    \"desconto_total__c__apoio_de_importador__propostas\",\n",
    "    \"categoria_negocio_deals\",\n",
    "        \"distrito_contactos\",\n",
    "    \"estado_ordering_propostas\",\n",
    "    \"tasks_involved_deals\",\n",
    "    \"events_involved_deals\",\n",
    "    \"calls_involved_deals\",\n",
    "    \"desconto_total_propostas\",\n",
    "                 \n",
    "   ]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4802e8ef-484f-43c4-92f0-cab74ceda634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "CHECK_COLS = [\"modelo\", \"modelo_contactos\", \"modelo_propostas\", \"modelos_deals\"]\n",
    "\n",
    "df = spark.table(TBL)\n",
    "present = [c for c in CHECK_COLS if c in df.columns]\n",
    "missing = [c for c in CHECK_COLS if c not in df.columns]\n",
    "\n",
    "row_count = df.count()\n",
    "# null counts for present columns\n",
    "nulls = (df.agg(*[F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in present])\n",
    "           .collect()[0]\n",
    "           .asDict())\n",
    "\n",
    "# build result rows\n",
    "rows = []\n",
    "for c in present:\n",
    "    n_null = int(nulls.get(c, 0))\n",
    "    rows.append((c, row_count, n_null, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"rows_total\", \"null_count\", \"null_pct\"])\n",
    "display(out.orderBy(\"column\"))\n",
    "\n",
    "if missing:\n",
    "    print(\"Not found in table (skipped):\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a960154-8fde-45c8-80b1-7bf5cb31c5fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \n",
    "    \"modelo_propostas\",\n",
    "    \"modelos_deals\", \n",
    "    \"formulario\",\n",
    "    \"tasks_only_deals\",\n",
    "    \"events_only_deals\",\n",
    "    \"calls_only_deals\",\n",
    "    \"tasks_involved\",\n",
    "    \"events_involved\",\n",
    "    \"calls_involved\",\n",
    "               \n",
    "   ]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "267bcd05-ee19-4397-a8af-a75fe02e3fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "row_count = df.count()\n",
    "dtypes = dict(df.dtypes)\n",
    "\n",
    "# --- STRICT NULLS ONLY (what you asked) ---\n",
    "null_aggs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    "null_row = df.agg(*null_aggs).collect()[0].asDict()\n",
    "\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    n_null = int(null_row.get(c, 0))\n",
    "    rows.append((c, dtypes[c], n_null, row_count, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"dtype\", \"null_count\", \"rows_total\", \"null_pct\"])\n",
    "display(out.orderBy(F.desc(\"null_pct\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344f3d25-3859-4c36-bdce-1e08adb520a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # DEAL / POST-OUTCOME LEAKAGE  (*_deals)\n",
    "    # ----------------------------\n",
    "    \"formulario_contactos\",\n",
    "    \"is_converted\",\n",
    "    \"tasks_involved\",\n",
    "    \"events_involved\",\n",
    "    \"calls_involded\",\n",
    "    \"tasks_involved_contactos\",\n",
    "    \"events_involved_contactos\",\n",
    "    \"calls_involded_contactos\",\n",
    "    \"activities_involved_contactos\",\n",
    "    \"email_opt_out_contactos\",\n",
    "       \"pais_contactos\",\n",
    "            \"tipo_cliente_propostas\",\n",
    "      \"isv_propostas\",\n",
    "    \"pintura_propostas\",\n",
    "    \"base_tributavel_propostas\",\n",
    "    \"legalizacao_propostas\",\n",
    "    \"iuc_propostas\",\n",
    "    \"preco_total_propostas\",\n",
    "    \"iva_propostas\",\n",
    "    \"sub_total_com_desconto_propostas\",\n",
    "    \"taxa_propostas\",\n",
    "    \"total_final_propostas\",\n",
    "    \"s_g_p_u__propostas\",\n",
    "     \"tasks_only_deals\",\n",
    "    \"events_only_deals\",\n",
    "    \"calls_only_deals\",\n",
    "    \"caracterizacao_contactos\",\n",
    "        \"caracterizacao_deals\",\n",
    "    \"n__matriculas_por_associar_deals\",\n",
    "    \"tipo_cliente_negocio_deals\",\n",
    "    \"sales_cycle_duration_deals\",\n",
    "    \"calls_involved_contactos\",\n",
    "    \"concessao\",\n",
    "    \"origem_contactos\",\n",
    "    \"valor_base_propostas\",\n",
    "    \"preco_base_propostas\",\n",
    "    \"age_tier_deals\",\n",
    "    \"num_de_propostas_deals\",\n",
    "    \"qtd_viaturas_propostas\",\n",
    "    \"qtd_viaturas_deals\",\n",
    "    \"age_in_days_deals\",\n",
    "    \"modelo\",\n",
    "]\n",
    " \n",
    "\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2530c400-abeb-4f1c-ad1d-f558e80ce072",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"column\":229},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759516550584}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "row_count = df.count()\n",
    "dtypes = dict(df.dtypes)\n",
    "\n",
    "# --- STRICT NULLS ONLY (what you asked) ---\n",
    "null_aggs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    "null_row = df.agg(*null_aggs).collect()[0].asDict()\n",
    "\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    n_null = int(null_row.get(c, 0))\n",
    "    rows.append((c, dtypes[c], n_null, row_count, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"dtype\", \"null_count\", \"rows_total\", \"null_pct\"])\n",
    "display(out.orderBy(F.desc(\"null_pct\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf90b7a-ca82-4cbd-8881-be403fe3c2cc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759518806010}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sc_gold.features_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87cb7a20-ed55-411d-93d6-640ea4fe9815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "row_count = df.count()\n",
    "dtypes = dict(df.dtypes)\n",
    "\n",
    "# --- STRICT NULLS ONLY (what you asked) ---\n",
    "null_aggs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    "null_row = df.agg(*null_aggs).collect()[0].asDict()\n",
    "\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    n_null = int(null_row.get(c, 0))\n",
    "    rows.append((c, dtypes[c], n_null, row_count, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"dtype\", \"null_count\", \"rows_total\", \"null_pct\"])\n",
    "display(out.orderBy(F.desc(\"null_pct\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775873ab-10b8-48ff-83d5-5c46d7d89824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TABLE = \"sc_gold.features_table\"\n",
    "DIFF_COL = \"lead_to_deal_days\"\n",
    "\n",
    "# Load table\n",
    "sdf = spark.table(TABLE)\n",
    "\n",
    "# ---- Counts ----\n",
    "total = sdf.count()\n",
    "neg_count = sdf.filter(F.col(DIFF_COL) < 0).count()\n",
    "pos_count = total - neg_count\n",
    "neg_pct = round(100 * neg_count / total, 1) if total else 0.0\n",
    "pos_pct = round(100 * pos_count / total, 1) if total else 0.0\n",
    "\n",
    "print(f\"Total rows: {total}\")\n",
    "print(f\"Positive count: {pos_count} ({pos_pct}%)\")\n",
    "print(f\"Negative count: {neg_count} ({neg_pct}%)\")\n",
    "\n",
    "# ---- Distribution buckets for positives ----\n",
    "pos = (\n",
    "    sdf.filter(F.col(DIFF_COL) >= 0)\n",
    "       .withColumn(\n",
    "           \"pos_bucket\",\n",
    "           F.when(F.col(DIFF_COL) < 1, \"0–1d\")\n",
    "            .when(F.col(DIFF_COL) < 7, \"1–7d\")\n",
    "            .when(F.col(DIFF_COL) < 30, \"7–30d\")\n",
    "            .when(F.col(DIFF_COL) < 90, \"30–90d\")\n",
    "            .when(F.col(DIFF_COL) < 180, \"90–180d\")\n",
    "            .when(F.col(DIFF_COL) < 365, \"180–365d\")\n",
    "            .otherwise(\">365d\")\n",
    "       )\n",
    ")\n",
    "print(\"\\nPositive distribution by bucket:\")\n",
    "pos.groupBy(\"pos_bucket\").count().orderBy(\"pos_bucket\").show(truncate=False)\n",
    "\n",
    "# ---- Distribution buckets for negatives ----\n",
    "neg = (\n",
    "    sdf.filter(F.col(DIFF_COL) < 0)\n",
    "       .withColumn(\n",
    "           \"neg_bucket\",\n",
    "           F.when(F.col(DIFF_COL) >= -1, \"<1d early\")\n",
    "            .when(F.col(DIFF_COL) >= -7, \"1–7d early\")\n",
    "            .when(F.col(DIFF_COL) >= -30, \"7–30d early\")\n",
    "            .when(F.col(DIFF_COL) >= -90, \"30–90d early\")\n",
    "            .when(F.col(DIFF_COL) >= -180, \"90–180d early\")\n",
    "            .when(F.col(DIFF_COL) >= -365, \"180–365d early\")\n",
    "            .otherwise(\">365d early\")\n",
    "       )\n",
    ")\n",
    "print(\"\\nNegative distribution by bucket:\")\n",
    "neg.groupBy(\"neg_bucket\").count().orderBy(\"neg_bucket\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f09f73-823a-4bb0-9a6e-64381e2e34b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks / PySpark + matplotlib\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TABLE   = \"sc_gold.features_table\"\n",
    "DIFF_COL = \"lead_to_deal_days\"\n",
    "\n",
    "# 1) Load and keep only numeric, non-null diffs\n",
    "sdf = spark.table(TABLE).select(F.col(DIFF_COL).cast(\"double\").alias(\"diff_days\")).na.drop()\n",
    "\n",
    "# 2) Counts / percentages\n",
    "total = sdf.count()\n",
    "neg_count = sdf.filter(\"diff_days < 0\").count()\n",
    "pos_count = total - neg_count\n",
    "\n",
    "neg_pct = round(100 * neg_count / total, 1) if total else 0.0\n",
    "pos_pct = round(100 * pos_count / total, 1) if total else 0.0\n",
    "\n",
    "print(f\"Total rows: {total}\")\n",
    "print(f\"Positive (>=0) count: {pos_count}  ({pos_pct}%)\")\n",
    "print(f\"Negative (<0) count: {neg_count}  ({neg_pct}%)\")\n",
    "\n",
    "# 3) Bring to Pandas for plotting\n",
    "pdf = sdf.toPandas()\n",
    "\n",
    "# 4) Distribution graph (two histograms overlaid)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(pdf.loc[pdf[\"diff_days\"]>=0, \"diff_days\"], bins=60, alpha=0.7, label=\"Positives (deal after lead)\")\n",
    "plt.hist(pdf.loc[pdf[\"diff_days\"]<0,  \"diff_days\"], bins=60, alpha=0.7, label=\"Negatives (deal before lead)\")\n",
    "plt.xlabel(\"Days difference (deal_created - lead_created)\")\n",
    "plt.ylabel(\"Number of records\")\n",
    "plt.title(\"Distribution of lead_to_deal_days (positives vs negatives)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (Optional) Zoomed view for 0–100 days to see the main cluster clearly\n",
    "plt.figure(figsize=(10,5))\n",
    "subset = pdf[(pdf[\"diff_days\"] >= 0) & (pdf[\"diff_days\"] <= 100)]\n",
    "plt.hist(subset[\"diff_days\"], bins=50, alpha=0.9)\n",
    "plt.xlabel(\"Days difference (0–100 days)\")\n",
    "plt.ylabel(\"Number of records\")\n",
    "plt.title(\"Positive differences (0–100 days)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a8cdb9-1a7c-4346-a081-3e04f884b59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TABLE = \"sc_gold.features_table\"\n",
    "DIFF_COL = \"lead_to_deal_days\"\n",
    "\n",
    "# Load only negatives\n",
    "neg = spark.table(TABLE).select(F.col(DIFF_COL).cast(\"double\").alias(\"diff_days\")) \\\n",
    "         .filter(\"diff_days < 0\").na.drop()\n",
    "\n",
    "# Count how many\n",
    "neg_count = neg.count()\n",
    "print(f\"Negative rows: {neg_count}\")\n",
    "\n",
    "# Bring to pandas for plotting\n",
    "pdf_neg = neg.toPandas()\n",
    "\n",
    "# Histogram for negatives\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(pdf_neg[\"diff_days\"], bins=50, alpha=0.8, color=\"red\")\n",
    "plt.xlabel(\"Days difference (deal_created - lead_created)\")\n",
    "plt.ylabel(\"Number of records\")\n",
    "plt.title(\"Distribution of NEGATIVE lead_to_deal_days (deal before lead)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d95c62-2051-4af4-8988-00786f4ad851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    deal_before_lead,\n",
    "    COUNT(*) AS count,\n",
    "    ROUND(100 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage\n",
    "FROM sc_gold.features_table\n",
    "GROUP BY deal_before_lead\n",
    "ORDER BY deal_before_lead;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade8c6e5-e855-4c1e-bd03-fed7ccd30e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove invalid rows where deal happens before lead\n",
    "# These cases have no operational meaning, so we delete them\n",
    "\n",
    "src_tbl = \"sc_gold.features_table\"\n",
    "\n",
    "# Read the table\n",
    "df = spark.table(src_tbl)\n",
    "\n",
    "# Filter out rows where deal_before_lead = 1\n",
    "df_filtered = df.filter(df.deal_before_lead == 0)\n",
    "\n",
    "# Drop the 'deal_before_lead' column (no longer needed)\n",
    "df_filtered = df_filtered.drop(\"deal_before_lead\")\n",
    "\n",
    "# Overwrite the existing table with the cleaned data\n",
    "df_filtered.write.mode(\"overwrite\").saveAsTable(src_tbl)\n",
    "\n",
    "print(\"✅ Rows with deal_before_lead = 1 removed and column dropped successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3e4974-7e44-4acd-891c-08df028179a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Drop only the 'deal_before_lead' column ===\n",
    "\n",
    "src_tbl = \"sc_gold.features_table\"\n",
    "\n",
    "# Define the column to delete\n",
    "cols_to_delete = [\"deal_before_lead\",\"pais\"]\n",
    "\n",
    "# Get the full list of columns dynamically\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "\n",
    "# Keep everything except the column to delete\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# Snapshot current table into a temp view\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "# Overwrite the table without the dropped column\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql}\n",
    "FROM _src\n",
    "\"\"\")\n",
    "\n",
    "# Drop the temporary view\n",
    "spark.sql(\"DROP VIEW _src\")\n",
    "\n",
    "print(\"✅ Column 'deal_before_lead' dropped successfully from table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204ff107-d05a-4828-91a5-1cf2760e720c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759591471139}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sc_gold.features_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a60bd8c-7ebe-417f-92a6-a4bb2a9e7fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Filter dataset to include only records with a filled 'stage_deals'\n",
    "# Ensures proposals that had follow-up toward a deal are kept\n",
    "# ===============================================================\n",
    "\n",
    "src_tbl = \"sc_gold.features_table\"\n",
    "\n",
    "# Read the table\n",
    "df = spark.table(src_tbl)\n",
    "\n",
    "# Filter only rows with non-null 'stage_deals'\n",
    "df_filtered = df.filter(df.stage_deals.isNotNull())\n",
    "\n",
    "# Overwrite the existing table with the filtered data\n",
    "df_filtered.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(src_tbl)\n",
    "\n",
    "print(\"✅ Filter applied successfully — kept only rows with non-null 'stage_deals'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f24ca8-8c85-4a52-b14d-3b6bf5b548dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "row_count = df.count()\n",
    "dtypes = dict(df.dtypes)\n",
    "\n",
    "# --- STRICT NULLS ONLY (what you asked) ---\n",
    "null_aggs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    "null_row = df.agg(*null_aggs).collect()[0].asDict()\n",
    "\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    n_null = int(null_row.get(c, 0))\n",
    "    rows.append((c, dtypes[c], n_null, row_count, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"dtype\", \"null_count\", \"rows_total\", \"null_pct\"])\n",
    "display(out.orderBy(F.desc(\"null_pct\")))\n",
    "display(out, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aafc69d5-bbe7-4536-8214-3aba33964150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "src_tbl = \"sc_gold.features_table\"\n",
    "df = spark.table(src_tbl)\n",
    "\n",
    "# Normalize common variants for stage labels (optional – add/remove as needed)\n",
    "venda_labels  = ['Vendido', 'Venda-aprovado', 'Venda Aprovado', 'Venda Aprovada']\n",
    "emcurso_labels = ['Em Curso', 'Em curso']\n",
    "adiado_labels  = ['Adiado']\n",
    "\n",
    "# Fill only when motivo_da_perda_deals is NULL\n",
    "filled_col = (\n",
    "    F.when(\n",
    "        F.col(\"motivo_da_perda_deals\").isNull() & F.col(\"stage_deals\").isin(*venda_labels),\n",
    "        F.lit(\"Não aplicável (venda)\")\n",
    "    )\n",
    "    .when(\n",
    "        F.col(\"motivo_da_perda_deals\").isNull() & F.col(\"stage_deals\").isin(*emcurso_labels),\n",
    "        F.lit(\"Sem decisão (em curso)\")\n",
    "    )\n",
    "    .when(\n",
    "        F.col(\"motivo_da_perda_deals\").isNull() & F.col(\"stage_deals\").isin(*adiado_labels),\n",
    "        F.lit(\"Sem decisão (adiado)\")\n",
    "    )\n",
    "    .otherwise(F.col(\"motivo_da_perda_deals\"))\n",
    ")\n",
    "\n",
    "df_out = df.withColumn(\"motivo_da_perda_deals\", filled_col)\n",
    "\n",
    "# Overwrite the table with the updated values\n",
    "df_out.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(src_tbl)\n",
    "\n",
    "print(\"✅ Filled 'motivo_da_perda_deals' conditionally based on 'stage_deals'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e39b49-5f42-4fc0-a7ec-220794b70cc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "src_tbl = \"sc_gold.features_table\"\n",
    "df = spark.table(src_tbl)\n",
    "\n",
    "# Create binary flag: 1 if model name exists in campaign name, else 0\n",
    "df = df.withColumn(\n",
    "    \"model_in_campaign\",\n",
    "    F.when(\n",
    "        F.lower(F.col(\"campaign_name_campaigns\")).contains(F.lower(F.col(\"modelo_contactos\"))),\n",
    "        F.lit(1)\n",
    "    ).otherwise(F.lit(0))\n",
    ")\n",
    "\n",
    "# Overwrite table with new column\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(src_tbl)\n",
    "\n",
    "print(\"✅ Column 'model_in_campaign' added successfully (1 = model appears in campaign).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b142b35f-1fb4-4807-839c-97421bbeffd3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759600450745}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.features_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "973c4aee-4bad-45bf-af32-e08091cd48da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    modelo_contactos,\n",
    "    campaign_name_campaigns,\n",
    "    model_in_campaign\n",
    "FROM sc_gold.features_table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce465333-1ec0-4b7d-8ec0-39c9417c2a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "src_tbl = \"sc_gold.features_table\"\n",
    "df = spark.table(src_tbl)\n",
    "\n",
    "# === Calculate median pvp_propostas by modelo_contactos ===\n",
    "median_by_model = (\n",
    "    df.groupBy(\"modelo_contactos\")\n",
    "      .agg(F.expr(\"percentile_approx(pvp_propostas, 0.5)\").alias(\"median_pvp\"))\n",
    ")\n",
    "\n",
    "# Join the median back to the main dataframe\n",
    "df = df.join(median_by_model, on=\"modelo_contactos\", how=\"left\")\n",
    "\n",
    "# Fill nulls in pvp_propostas with the model-specific median\n",
    "df = df.withColumn(\n",
    "    \"pvp_propostas\",\n",
    "    F.when(F.col(\"pvp_propostas\").isNull(), F.col(\"median_pvp\"))\n",
    "     .otherwise(F.col(\"pvp_propostas\"))\n",
    ")\n",
    "\n",
    "# Drop the helper median column\n",
    "df = df.drop(\"median_pvp\")\n",
    "\n",
    "# Save updates\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(src_tbl)\n",
    "\n",
    "print(\"✅ Filled nulls in 'pvp_propostas' with model-specific median values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7187fdb2-fae1-41ad-8772-4f6ed237d2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window  # ✅ this import was missing\n",
    "\n",
    "# Define source table\n",
    "src_tbl = \"sc_gold.features_table\"\n",
    "df = spark.table(src_tbl)\n",
    "\n",
    "# === 1. Calculate mode 'combustivel_propostas' by 'modelo_contactos' ===\n",
    "# Get the most frequent fuel type per model\n",
    "mode_by_model = (\n",
    "    df.groupBy(\"modelo_contactos\", \"combustivel_propostas\")\n",
    "      .count()\n",
    "      .withColumn(\"rn\", F.row_number().over(\n",
    "          Window.partitionBy(\"modelo_contactos\").orderBy(F.desc(\"count\"))\n",
    "      ))\n",
    "      .filter(F.col(\"rn\") == 1)\n",
    "      .select(\"modelo_contactos\", \"combustivel_propostas\")\n",
    "      .withColumnRenamed(\"combustivel_propostas\", \"mode_combustivel\")\n",
    ")\n",
    "\n",
    "# === 2. Join mode back to main dataframe ===\n",
    "df = df.join(mode_by_model, on=\"modelo_contactos\", how=\"left\")\n",
    "\n",
    "# === 3. Fill nulls in 'combustivel_propostas' with the model-specific mode ===\n",
    "df = df.withColumn(\n",
    "    \"combustivel_propostas\",\n",
    "    F.when(F.col(\"combustivel_propostas\").isNull(), F.col(\"mode_combustivel\"))\n",
    "     .otherwise(F.col(\"combustivel_propostas\"))\n",
    ")\n",
    "\n",
    "# === 4. Drop helper column ===\n",
    "df = df.drop(\"mode_combustivel\")\n",
    "\n",
    "# === 5. Save updated table ===\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(src_tbl)\n",
    "\n",
    "print(\"✅ Filled nulls in 'combustivel_propostas' using model-specific mode (by 'modelo_contactos').\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5935f3-9cee-44e2-b9d6-7c5a81c5f61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *,\n",
    "    CASE \n",
    "        WHEN salutation_contactos IN ('Sr', 'Sr.', 'Dr.', 'Eng.', 'Prof.') THEN 1\n",
    "        WHEN salutation_contactos IN ('Sr.ª', 'Dra.', 'Dr.ª') THEN 0\n",
    "        ELSE NULL\n",
    "    END AS is_male\n",
    "FROM sc_gold.features_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600e6a5c-220a-457c-9de6-b6c71692ce21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE sc_gold.features_table\n",
    "SET is_male = CASE\n",
    "  WHEN salutation_contactos IN ('Sr','Sr.','Dr.','Eng.','Prof.') THEN 1\n",
    "  WHEN salutation_contactos IN ('Sr.ª','Dra.','Dr.ª') THEN 0\n",
    "  ELSE NULL\n",
    "END;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e987cc4-b158-4404-b14f-27beb5b4fc1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    salutation_contactos,\n",
    "    CASE \n",
    "        WHEN salutation_contactos IN ('Sr', 'Sr.', 'Dr.', 'Eng.', 'Prof.') THEN 1\n",
    "        WHEN salutation_contactos IN ('Sr.ª', 'Dra.', 'Dr.ª') THEN 0\n",
    "        ELSE NULL\n",
    "    END AS is_male\n",
    "FROM sc_gold.features_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2800b0ea-5437-4e77-b6a4-68ab13029e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Drop only the 'deal_before_lead' column ===\n",
    "\n",
    "src_tbl = \"sc_gold.features_table\"\n",
    "\n",
    "# Define the column to delete\n",
    "cols_to_delete = [\"salutation_contactos\",\"overall_sales_duration_deals\"]\n",
    "\n",
    "# Get the full list of columns dynamically\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "\n",
    "# Keep everything except the column to delete\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# Snapshot current table into a temp view\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "# Overwrite the table without the dropped column\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql}\n",
    "FROM _src\n",
    "\"\"\")\n",
    "\n",
    "# Drop the temporary view\n",
    "spark.sql(\"DROP VIEW _src\")\n",
    "\n",
    "print(\"✅ Column 'salutation_contactos' dropped successfully from table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a98ef39e-0e8d-4a0c-89d7-c1e924a0252a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759679462612}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.features_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0deddf0-2b16-4690-9352-c5a092c2f6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "row_count = df.count()\n",
    "dtypes = dict(df.dtypes)\n",
    "\n",
    "# --- STRICT NULLS ONLY (what you asked) ---\n",
    "null_aggs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    "null_row = df.agg(*null_aggs).collect()[0].asDict()\n",
    "\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    n_null = int(null_row.get(c, 0))\n",
    "    rows.append((c, dtypes[c], n_null, row_count, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"dtype\", \"null_count\", \"rows_total\", \"null_pct\"])\n",
    "display(out.orderBy(F.desc(\"null_pct\")))\n",
    "display(out, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd87a566-0e11-4af4-a3f4-5da3bf32496c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (no column exclusions, no saving) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Prepare types (use ALL columns)\n",
    "# - numeric: numbers + booleans (treated as numeric 0/1)\n",
    "# - datetimes: convert to epoch seconds (new *_ts numeric columns)\n",
    "# - everything else: categorical (OHE)\n",
    "\n",
    "# Detect native datetime dtypes and convert to numeric copies\n",
    "date_cols = pdf.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns.tolist()\n",
    "for c in date_cols:\n",
    "    pdf[f\"{c}_ts\"] = pd.to_datetime(pdf[c], errors=\"coerce\").view(\"int64\") / 1e9  # seconds\n",
    "# Optional: try to catch obvious date-like strings (kept light to avoid surprises)\n",
    "# (commented; enable if needed)\n",
    "# for c in pdf.select_dtypes(include=[\"object\"]).columns:\n",
    "#     try_dt = pd.to_datetime(pdf[c], errors=\"coerce\", utc=False)\n",
    "#     if try_dt.notna().sum() > 0 and (try_dt.notna().mean() > 0.8):\n",
    "#         pdf[f\"{c}_ts\"] = try_dt.view(\"int64\") / 1e9\n",
    "\n",
    "# Build column groups\n",
    "num_cols = pdf.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "cat_cols = pdf.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(f\"Detected -> numeric={len(num_cols)}, categorical={len(cat_cols)}, datetime={len(date_cols)} (converted to *_ts)\")\n",
    "\n",
    "# 3) Preprocess (no exclusions)\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "# sklearn ≥1.2 uses 'sparse_output' arg, older uses 'sparse'\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "\n",
    "transformers = []\n",
    "if len(num_cols):\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if len(cat_cols):\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(**ohe_kw))\n",
    "    ]), cat_cols))\n",
    "\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable columns: table has neither numeric/boolean nor categorical fields.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 4) Encode -> SVD (<=20) -> Standardize\n",
    "Xs = preprocess.fit_transform(pdf)  # sparse (likely) after OHE\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(f\"Encoded shape: {Xs.shape} | Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 5) k-scan (silhouette + inertia) and pick best k\n",
    "k_range = range(2, min(9, X.shape[0]))  # guard for tiny samples\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  silhouette={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.2f}s\")\n",
    "print(f\"Total k-scan time: {time.perf_counter() - t0:.2f}s\")\n",
    "\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"])\n",
    "best_row = metrics_df.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).iloc[0]\n",
    "best_k = int(best_row.k)\n",
    "print(\"\\nK-scan (sorted by silhouette):\\n\", metrics_df.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).to_string(index=False))\n",
    "print(f\"\\n>>> Best k = {best_k}  (silhouette={best_row.silhouette:.4f})\")\n",
    "\n",
    "# 6) Final model + attach labels (in-memory only)\n",
    "final_km = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = final_km.labels_\n",
    "\n",
    "# 7) Quick summary\n",
    "sizes = pdf[\"cluster\"].value_counts(dropna=False).sort_index().to_frame(\"count\")\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "print(\"\\nCluster sizes:\\n\", sizes)\n",
    "\n",
    "# Optional: preview by cluster\n",
    "print(\"\\nPreview (first 5 rows with cluster):\")\n",
    "print(pdf.head(5)[[\"cluster\"] + [c for c in pdf.columns if c != \"cluster\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c18862-571c-4208-8558-2e41f346d38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==== CLUSTER PROFILING & VISUALS (drop-in, run after your K-Means cell) ====\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def quick_readout(df, cluster_col, value_col, buckets=(30, 60, 90), unit=\"d\"):\n",
    "    \"\"\"\n",
    "    Print median and cumulative bucket percentages (<=b1, <=b2, >b_last) for a numeric KPI.\n",
    "    \"\"\"\n",
    "    if value_col not in df.columns:\n",
    "        print(f\"[quick_readout] Column '{value_col}' not found.\")\n",
    "        return\n",
    "    print(\"=== Quick readout per cluster ===\")\n",
    "    for k in sorted(df[cluster_col].dropna().unique()):\n",
    "        g = df[df[cluster_col] == k][value_col].astype(float)\n",
    "        if g.empty:\n",
    "            print(f\"Cluster {k}: (no rows)\")\n",
    "            continue\n",
    "        med = np.nanmedian(g)\n",
    "        pcs = []\n",
    "        for b in buckets:\n",
    "            pcs.append((f\"<= {b}{unit}\", (g <= b).mean() * 100))\n",
    "        pcs.append((f\"> {buckets[-1]}{unit}\", (g > buckets[-1]).mean() * 100))\n",
    "        pcs_str = \"  \".join([f\"{lab}={pct:.1f}%\" for lab, pct in pcs])\n",
    "        print(f\"Cluster {k}: median={med:.1f}{unit}  |  {pcs_str}\")\n",
    "\n",
    "def top_cats(df, cluster_col, cat_col, topn=5):\n",
    "    \"\"\"\n",
    "    Top-N categories inside each cluster for one categorical column.\n",
    "    Coerces values to safe strings to avoid Arrow conversion errors.\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    for k, g in df.groupby(cluster_col):\n",
    "        s = g[cat_col].astype(\"object\").where(lambda x: x.notna(), other=\"<<NULL>>\")\n",
    "        s = s.map(lambda x: str(x))\n",
    "        s = s.map(lambda x: x if len(x) <= 200 else (x[:197] + \"...\"))\n",
    "        vc = s.value_counts(normalize=True, dropna=False).head(topn)\n",
    "        for val, frac in vc.items():\n",
    "            out_rows.append([cat_col, int(k), val, float(round(frac*100, 2))])\n",
    "    top = pd.DataFrame(out_rows, columns=[\"column\", \"cluster\", \"value\", \"pct\"])\n",
    "    return top.astype({\"column\":\"string\",\"value\":\"string\",\"cluster\":\"int64\",\"pct\":\"float64\"})\n",
    "\n",
    "# ---------- 1) Cluster sizes ----------\n",
    "sizes = pdf[\"cluster\"].value_counts(dropna=False).sort_index()\n",
    "sizes_df = sizes.to_frame(\"count\").assign(pct=(sizes / sizes.sum() * 100).round(2))\n",
    "display(sizes_df)\n",
    "\n",
    "plt.figure()\n",
    "sizes.plot(kind=\"bar\")\n",
    "plt.title(\"Cluster sizes\"); plt.xlabel(\"cluster\"); plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 2) Quick readout for your key KPI (edit the column name if needed) ----------\n",
    "target_numeric = \"lead_to_deal_days\"  # <- change this KPI if you want\n",
    "if target_numeric in pdf.columns:\n",
    "    quick_readout(pdf, \"cluster\", target_numeric, buckets=(30, 60, 90), unit=\"d\")\n",
    "else:\n",
    "    print(f\"[info] '{target_numeric}' not in table; skipping quick readout.\")\n",
    "\n",
    "# ---------- 3) Numeric profile (median/mean/std) ----------\n",
    "num_cols = [c for c in pdf.select_dtypes(include=[\"number\", \"bool\"]).columns if c != \"cluster\"]\n",
    "if num_cols:\n",
    "    prof_num = pdf.groupby(\"cluster\")[num_cols].agg([\"median\", \"mean\", \"std\"])\n",
    "    prof_num.columns = [f\"{c}_{stat}\" for c, stat in prof_num.columns]\n",
    "    display(prof_num)\n",
    "\n",
    "    # Heatmap of standardized medians (z-score) — slide-friendly\n",
    "    med = pdf.groupby(\"cluster\")[num_cols].median(numeric_only=True)\n",
    "    med_std = (med - med.mean()) / med.std(ddof=0)\n",
    "    keep = med_std.columns[:15]  # show first 15 numerics for readability\n",
    "    plt.figure(figsize=(min(12, 0.6*len(keep)+3), 0.5*len(med_std)+3))\n",
    "    plt.imshow(med_std[keep], aspect=\"auto\")\n",
    "    plt.yticks(range(len(med_std.index)), med_std.index)\n",
    "    plt.xticks(range(len(keep)), keep, rotation=60, ha=\"right\")\n",
    "    plt.title(\"Standardized medians by cluster (z-score)\")\n",
    "    plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"[info] No numeric columns to profile.\")\n",
    "\n",
    "# ---------- 4) Top categories per cluster (robust) ----------\n",
    "cat_cols = pdf.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "if cat_cols:\n",
    "    frames = [top_cats(pdf, \"cluster\", c, topn=5) for c in cat_cols]\n",
    "    topcats = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=[\"column\",\"cluster\",\"value\",\"pct\"])\n",
    "    # If 'display' throws Arrow errors in your workspace, use print(topcats.to_string(index=False)) instead\n",
    "    display(topcats)\n",
    "else:\n",
    "    print(\"[info] No categorical columns detected.\")\n",
    "\n",
    "# ---------- 5) 2-D projection scatter (SVD on preprocessed features) ----------\n",
    "# Uses existing Xs (sparse encoded matrix) if available; falls back to X if that exists.\n",
    "try:\n",
    "    if \"Xs\" in globals():\n",
    "        svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "        X2 = svd2.fit_transform(Xs)  # type: ignore\n",
    "    elif \"X\" in globals():\n",
    "        # If you only have the reduced matrix, reuse it (pad to 2D if needed)\n",
    "        X2 = X if X.shape[1] >= 2 else np.pad(X, ((0,0),(0,2-X.shape[1])), mode=\"constant\")\n",
    "    else:\n",
    "        raise NameError(\"Neither Xs nor X found\")\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for k in sorted(pdf[\"cluster\"].unique()):\n",
    "        idx = (pdf[\"cluster\"] == k).values\n",
    "        plt.scatter(X2[idx,0], X2[idx,1], s=8, alpha=0.6, label=f\"c{k}\")\n",
    "    plt.legend(title=\"cluster\", ncol=2)\n",
    "    plt.title(\"2-D projection colored by cluster\")\n",
    "    plt.xlabel(\"Comp 1\"); plt.ylabel(\"Comp 2\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"[scatter] Skipped 2-D projection:\", e)\n",
    "\n",
    "# ---------- 6) Silhouette distribution per cluster ----------\n",
    "try:\n",
    "    if \"X\" in globals():\n",
    "        sil = silhouette_samples(X, pdf[\"cluster\"].values)\n",
    "        tmp = pd.DataFrame({\"cluster\": pdf[\"cluster\"].values, \"silhouette\": sil})\n",
    "        ax = tmp.boxplot(by=\"cluster\", column=\"silhouette\", grid=False, figsize=(6,4))\n",
    "        plt.suptitle(\"\")\n",
    "        plt.title(\"Silhouette by cluster\")\n",
    "        plt.xlabel(\"cluster\"); plt.ylabel(\"silhouette\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[silhouette] Skipped (matrix X not found).\")\n",
    "except Exception as e:\n",
    "    print(\"[silhouette] Skipped due to error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9467f4-e9eb-4861-9a4e-41d7fcabaea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metrics_df already built earlier as: columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"]\n",
    "# 1) Print best k and silhouette (overall)\n",
    "best_row = metrics_df.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).iloc[0]\n",
    "best_k = int(best_row[\"k\"])\n",
    "best_sil = float(best_row[\"silhouette\"])\n",
    "print(f\"Best k = {best_k}  |  silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# 2) Elbow curve (inertia) and silhouette vs k\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\")\n",
    "ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\")\n",
    "ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110b2e29-7d90-4334-808c-52bf6776ad0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Best-practice K-Means (DECIMAL→DOUBLE without duplicates, car value kept, robust profiling) ===\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn\n",
    "from packaging import version\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# ---------- 1) Load Spark DF and CAST DECIMALs to DOUBLE *replacing* originals (avoid duplicates) ----------\n",
    "cast_map = {\n",
    "    \"pvp_propostas\": \"DOUBLE\",\n",
    "    \"proposals_per_vehicle\": \"DOUBLE\",\n",
    "    \"lead_to_deal_days\": \"DOUBLE\",\n",
    "    \"overall_sales_duration_deals\": \"DOUBLE\",\n",
    "}\n",
    "sdf = spark.table(SRC_TBL)\n",
    "cast_cols  = [F.col(c).cast(t).alias(c) for c, t in cast_map.items()]\n",
    "other_cols = [F.col(c) for c in sdf.columns if c not in cast_map]\n",
    "sdf_cast = sdf.select(*(cast_cols + other_cols))  # replaces originals, no duplicates\n",
    "pdf = sdf_cast.toPandas()\n",
    "pdf = pdf.loc[:, ~pdf.columns.duplicated()]       # belt-and-suspenders\n",
    "print(\"Rows:\", len(pdf), \"| Cols:\", len(pdf.columns))\n",
    "existing = set(pdf.columns)\n",
    "\n",
    "# ---------- 2) Feature sets ----------\n",
    "# Strong numerics (log-scale the heavy-tailed ones)\n",
    "log_num_desired = [\"pvp_propostas\", \"lead_to_deal_days\", \"overall_sales_duration_deals\"]\n",
    "lin_num_desired = [\"proposals_per_vehicle\"]\n",
    "\n",
    "# Compact behavior categoricals\n",
    "cat_cols_desired = [\n",
    "    \"lead_source\", \"deal_speed_category\", \"stage_deals\",\n",
    "    \"tipo_cliente_contactos\", \"combustivel_propostas\", \"origem_do_negocio_deals\"\n",
    "]\n",
    "\n",
    "# Skip noisy/redundant (modeling only)\n",
    "manual_drop = [\n",
    "    \"concessao_contactos\", \"versao_propostas\", \"campaign_name_campaigns\",\n",
    "    \"modelo_contactos\", \"proposta_realizada_stage_propostas\", \"motivo_da_perda_deals\"\n",
    "]\n",
    "\n",
    "# Keep only those that exist\n",
    "log_num = [c for c in log_num_desired if c in existing]\n",
    "lin_num = [c for c in lin_num_desired if c in existing]\n",
    "cat_cols = [c for c in cat_cols_desired if c in existing]\n",
    "drop_cols = [c for c in manual_drop if c in existing]\n",
    "Xdf = pdf.drop(columns=drop_cols)\n",
    "print(\"Numerics log-scaled:\", log_num)\n",
    "print(\"Numerics linear:\", lin_num)\n",
    "print(\"Categoricals:\", cat_cols)\n",
    "if drop_cols: print(\"Dropped (modeling only):\", drop_cols)\n",
    "assert (log_num or lin_num or cat_cols), \"No usable features.\"\n",
    "\n",
    "# ---------- 3) Preprocess (median impute; __MISSING__ bucket for cats; rare bucketing if available) ----------\n",
    "def _log1p(x): return np.log1p(x)\n",
    "\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "ohe_extra = {\"min_frequency\": 0.02, \"max_categories\": 25} if version.parse(sklearn.__version__) >= version.parse(\"1.1\") else {}\n",
    "\n",
    "stages = []\n",
    "if log_num:\n",
    "    stages.append((\n",
    "        \"num_log\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"log\", FunctionTransformer(_log1p, feature_names_out=\"one-to-one\")),\n",
    "            (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "        ]),\n",
    "        log_num\n",
    "    ))\n",
    "if lin_num:\n",
    "    stages.append((\n",
    "        \"num_lin\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "        ]),\n",
    "        lin_num\n",
    "    ))\n",
    "if cat_cols:\n",
    "    stages.append((\n",
    "        \"cat\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "            (\"ohe\", OneHotEncoder(**ohe_kw, **ohe_extra)),\n",
    "        ]),\n",
    "        cat_cols\n",
    "    ))\n",
    "\n",
    "preprocess = ColumnTransformer(stages, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# ---------- 4) Encode -> SVD (12 comps) -> whiten ----------\n",
    "Xs = preprocess.fit_transform(Xdf)                        # sparse matrix after OHE\n",
    "n_comp = max(2, min(12, Xs.shape[1]-1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "expl = svd.explained_variance_ratio_.sum()\n",
    "X = StandardScaler().fit_transform(X)                    # final whitening for K-Means\n",
    "print(f\"Sparse shape: {Xs.shape} | Reduced: {X.shape} | SVD explained variance: {expl:.3f}\")\n",
    "\n",
    "# ---------- 5) k-scan (Elbow + Silhouette) ----------\n",
    "metrics = []\n",
    "for k in range(2, 10):\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=30, max_iter=250, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_)))\n",
    "    print(f\"k={k} | silhouette={sil:.4f} | inertia={km.inertia_:.0f}\")\n",
    "\n",
    "m = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\"])\n",
    "best = m.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).iloc[0]\n",
    "best_k, best_sil = int(best.k), float(best.silhouette)\n",
    "print(f\"\\nBest k = {best_k} | silhouette = {best_sil:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(m.k, m.inertia, marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\"); ax.set_title(\"Elbow Curve (Inertia)\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(m.k, m.silhouette, marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\"); ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 6) Final model + labels (in-memory only) ----------\n",
    "final = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=60, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = final.labels_\n",
    "\n",
    "# ---------- 7) Profiling ----------\n",
    "\n",
    "def quick_readout(df, cluster_col, value_col, buckets=(30, 60, 90), unit=\"d\"):\n",
    "    if value_col not in df.columns:\n",
    "        print(f\"[quick_readout] '{value_col}' not found.\"); return\n",
    "    print(\"=== Quick readout per cluster ===\")\n",
    "    for k in sorted(df[cluster_col].dropna().unique()):\n",
    "        g = df[df[cluster_col]==k][value_col].astype(float)\n",
    "        if g.empty: print(f\"Cluster {k}: (no rows)\"); continue\n",
    "        med = np.nanmedian(g)\n",
    "        parts = [(f\"<= {b}{unit}\", (g<=b).mean()*100) for b in buckets] + [(f\"> {buckets[-1]}{unit}\", (g>buckets[-1]).mean()*100)]\n",
    "        print(f\"Cluster {k}: median={med:.1f}{unit} | \" + \"  \".join([f\"{a}={b:.1f}%\" for a,b in parts]))\n",
    "\n",
    "def top_cats(df, cluster_col, cat_col, topn=5):\n",
    "    out=[]\n",
    "    for k,g in df.groupby(cluster_col):\n",
    "        s=(g[cat_col].astype(\"object\").where(lambda x: x.notna(),\"__MISSING__\")\n",
    "           .map(str).map(lambda x: x if len(x)<=200 else x[:197]+\"...\"))\n",
    "        vc=s.value_counts(normalize=True, dropna=False).head(topn)\n",
    "        for val, frac in vc.items(): out.append([cat_col,int(k),val,float(round(frac*100,2))])\n",
    "    res=pd.DataFrame(out, columns=[\"column\",\"cluster\",\"value\",\"pct\"])\n",
    "    return res.astype({\"column\":\"string\",\"value\":\"string\",\"cluster\":\"int64\",\"pct\":\"float64\"})\n",
    "\n",
    "# Sizes\n",
    "sizes = pdf[\"cluster\"].value_counts(dropna=False).sort_index()\n",
    "display(sizes.to_frame(\"count\").assign(pct=(sizes/len(pdf)*100).round(2)))\n",
    "\n",
    "# Quick KPI readouts\n",
    "if \"lead_to_deal_days\" in pdf.columns:\n",
    "    quick_readout(pdf, \"cluster\", \"lead_to_deal_days\", buckets=(30,60,90), unit=\"d\")\n",
    "\n",
    "if \"pvp_propostas\" in pdf.columns:\n",
    "    med_price = pdf.groupby(\"cluster\")[\"pvp_propostas\"].median()\n",
    "    print(\"\\nMedian car value (pvp_propostas) by cluster:\\n\", med_price)\n",
    "\n",
    "# Numeric heatmap (only model numerics; guard for dtype)\n",
    "num_for_prof = [c for c in (log_num + lin_num) if c in pdf.columns]\n",
    "if num_for_prof:\n",
    "    med = pdf.groupby(\"cluster\")[num_for_prof].median(numeric_only=True)\n",
    "    cols = [c for c in num_for_prof if c in med.columns]\n",
    "    if cols:\n",
    "        med_std = (med[cols] - med[cols].mean()) / med[cols].std(ddof=0)\n",
    "        plt.figure(figsize=(min(12, 0.6*len(cols)+3), 0.5*len(med_std)+3))\n",
    "        plt.imshow(med_std[cols], aspect=\"auto\")\n",
    "        plt.yticks(range(len(med_std.index)), med_std.index)\n",
    "        plt.xticks(range(len(cols)), cols, rotation=60, ha=\"right\")\n",
    "        plt.title(\"Cluster z-score profiles (numeric medians)\")\n",
    "        plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Top categories per cluster (only the ones used in the model)\n",
    "if cat_cols:\n",
    "    topcats = pd.concat([top_cats(pdf, \"cluster\", c, topn=5) for c in cat_cols], ignore_index=True)\n",
    "    display(topcats)\n",
    "\n",
    "# 2-D projection & silhouette by cluster\n",
    "try:\n",
    "    svd2 = TruncatedSVD(n_components=2, random_state=SEED)\n",
    "    X2 = svd2.fit_transform(Xs)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for k in sorted(pdf[\"cluster\"].unique()):\n",
    "        idx = (pdf[\"cluster\"]==k).values\n",
    "        plt.scatter(X2[idx,0], X2[idx,1], s=8, alpha=0.6, label=f\"c{k}\")\n",
    "    plt.legend(title=\"cluster\", ncol=2); plt.title(\"2-D projection (SVD) by cluster\")\n",
    "    plt.xlabel(\"Comp 1\"); plt.ylabel(\"Comp 2\"); plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"[scatter] skipped:\", e)\n",
    "\n",
    "try:\n",
    "    sil = silhouette_samples(X, pdf[\"cluster\"].values)\n",
    "    print(\"Overall silhouette:\", round(float(sil.mean()), 4))\n",
    "    tmp = pd.DataFrame({\"cluster\": pdf[\"cluster\"].values, \"silhouette\": sil})\n",
    "    ax = tmp.boxplot(by=\"cluster\", column=\"silhouette\", grid=False, figsize=(6,4))\n",
    "    plt.suptitle(\"\"); plt.title(\"Silhouette by cluster\"); plt.xlabel(\"cluster\"); plt.ylabel(\"silhouette\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"[silhouette] skipped:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8df743f-5a3f-43b1-aea2-1dac5c75f09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf.groupby(\"cluster\")[[\"pvp_propostas\",\"lead_to_deal_days\"]].median().plot.bar(figsize=(8,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c86b37e-eb85-493d-8e57-fef2073b49a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sizes = pdf[\"cluster\"].value_counts(normalize=True).mul(100).round(2)\n",
    "sizes_df = sizes.reset_index().rename(columns={\"index\":\"cluster\", \"cluster\":\"pct\"})\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data=sizes_df, x=\"cluster\", y=\"pct\", palette=\"viridis\")\n",
    "plt.title(\"Cluster Distribution (%)\")\n",
    "plt.ylabel(\"Share of Total (%)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "for i,v in enumerate(sizes_df[\"pct\"]):\n",
    "    plt.text(i, v+0.5, f\"{v:.1f}%\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee71392-1aad-482f-99b7-a3f3be63c5bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [\"pvp_propostas\", \"lead_to_deal_days\", \"overall_sales_duration_deals\", \"proposals_per_vehicle\"]\n",
    "cluster_summary = pdf.groupby(\"cluster\")[num_cols].median().round(1)\n",
    "cluster_summary[\"count\"] = pdf[\"cluster\"].value_counts().sort_index()\n",
    "cluster_summary[\"%\"] = (cluster_summary[\"count\"] / cluster_summary[\"count\"].sum() * 100).round(1)\n",
    "display(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cbfe1b4-7d25-4376-a5ab-0a7f48bdd7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Compute standardized medians\n",
    "med = pdf.groupby(\"cluster\")[num_cols].median()\n",
    "med_std = pd.DataFrame(StandardScaler().fit_transform(med), \n",
    "                       index=med.index, columns=med.columns)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(med_std, annot=True, cmap=\"Spectral\", center=0, fmt=\".2f\")\n",
    "plt.title(\"Cluster Profiles (Z-Score of Medians)\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57918b53-aae1-4231-9551-b8939a04a5a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_lead = (\n",
    "    pdf.groupby([\"cluster\",\"lead_source\"])\n",
    "       .size()\n",
    "       .groupby(level=0, group_keys=False)\n",
    "       .apply(lambda x: (x / x.sum() * 100).nlargest(5))\n",
    "       .reset_index(name=\"pct\")\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top_lead, x=\"pct\", y=\"lead_source\", hue=\"cluster\", palette=\"tab10\")\n",
    "plt.title(\"Top 5 Lead Sources per Cluster (%)\")\n",
    "plt.xlabel(\"Share within Cluster (%)\")\n",
    "plt.ylabel(\"Lead Source\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42704c24-c677-4f95-badb-5054ea39d969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "\n",
    "sil_vals = silhouette_samples(X, pdf[\"cluster\"])\n",
    "pdf[\"silhouette\"] = sil_vals\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(data=pdf, x=\"cluster\", y=\"silhouette\", palette=\"coolwarm\")\n",
    "plt.title(f\"Silhouette by Cluster (overall mean = {np.mean(sil_vals):.3f})\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f871df2-909e-45dd-9acc-d44019a8e84f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "X2 = svd2.fit_transform(Xs)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x=X2[:,0], y=X2[:,1], hue=pdf[\"cluster\"], palette=\"Set2\", alpha=0.4, s=20)\n",
    "plt.title(\"2D Projection of Clusters (SVD components)\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473bddf3-3bce-4f4c-ac71-7f8536ad150a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === IMPROVED CLUSTER PROFILING VISUALS ===\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1️⃣ Median numeric summary per cluster ---\n",
    "num_cols = [\"pvp_propostas\", \"lead_to_deal_days\", \"overall_sales_duration_deals\", \"proposals_per_vehicle\"]\n",
    "num_cols = [c for c in num_cols if c in pdf.columns]\n",
    "\n",
    "cluster_summary = pdf.groupby(\"cluster\")[num_cols].median().round(1)\n",
    "cluster_summary[\"count\"] = pdf[\"cluster\"].value_counts().sort_index()\n",
    "cluster_summary[\"%\"] = (cluster_summary[\"count\"] / cluster_summary[\"count\"].sum() * 100).round(1)\n",
    "\n",
    "print(\"=== Numeric medians per cluster ===\")\n",
    "display(cluster_summary)\n",
    "\n",
    "# --- 2️⃣ Heatmap of normalized medians (z-score by column) ---\n",
    "z = (cluster_summary[num_cols] - cluster_summary[num_cols].mean()) / cluster_summary[num_cols].std(ddof=0)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(z, annot=True, cmap=\"RdYlGn\", center=0, linewidths=0.5, fmt=\".2f\")\n",
    "plt.title(\"Cluster Profiles (Z-Score of Numeric Medians)\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# --- 3️⃣ Relative differences heatmap (percent difference from mean) ---\n",
    "rel = ((cluster_summary[num_cols] / cluster_summary[num_cols].mean() - 1) * 100).round(1)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(rel, annot=True, cmap=\"coolwarm\", center=0, linewidths=0.5, fmt=\".1f\")\n",
    "plt.title(\"Cluster Profiles (% Difference from Mean)\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# --- 4️⃣ Distributions of key numeric variables ---\n",
    "for c in num_cols:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.kdeplot(data=pdf, x=c, hue=\"cluster\", common_norm=False, fill=True, alpha=0.3)\n",
    "    plt.title(f\"Distribution of {c} by Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "# --- 5️⃣ Improved 2D Projection with transparency ---\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "X2 = svd2.fit_transform(Xs)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "palette = sns.color_palette(\"tab10\", n_colors=len(pdf[\"cluster\"].unique()))\n",
    "sns.scatterplot(\n",
    "    x=X2[:,0], y=X2[:,1],\n",
    "    hue=pdf[\"cluster\"].astype(str),\n",
    "    palette=palette, s=10, alpha=0.3, linewidth=0\n",
    ")\n",
    "plt.title(\"2D SVD Projection Colored by Cluster\")\n",
    "plt.xlabel(\"Component 1\"); plt.ylabel(\"Component 2\")\n",
    "plt.legend(title=\"Cluster\", ncol=2, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ddc2bf-bc36-4264-9397-ee0ac2f20c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Best-practice K-Means (no casts; pvp_propostas DROPPED for model, kept for profiling) ===\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# ---------- 1) Load ----------\n",
    "pdf = spark.table(SRC_TBL).toPandas()\n",
    "pdf = pdf.loc[:, ~pdf.columns.duplicated()]  # safety\n",
    "print(\"Rows:\", len(pdf), \"| Cols:\", len(pdf.columns))\n",
    "existing = set(pdf.columns)\n",
    "\n",
    "# ---------- 2) Feature sets ----------\n",
    "# pvp_propostas is NOT used for modeling (low variance) but we keep it for profiling\n",
    "car_value_col = \"pvp_propostas\" if \"pvp_propostas\" in existing else None\n",
    "\n",
    "# Strong numerics for the model\n",
    "log_num_desired_model = [\"lead_to_deal_days\", \"overall_sales_duration_deals\"]  # log1p + robust scale\n",
    "lin_num_desired_model = [\"proposals_per_vehicle\"]                               # robust scale\n",
    "\n",
    "# Compact behavior categoricals\n",
    "cat_cols_desired = [\n",
    "    \"lead_source\", \"deal_speed_category\", \"stage_deals\",\n",
    "    \"tipo_cliente_contactos\", \"combustivel_propostas\", \"origem_do_negocio_deals\"\n",
    "]\n",
    "\n",
    "# Drop irrelevant (modeling only)\n",
    "manual_drop = [\n",
    "    \"concessao_contactos\", \"versao_propostas\", \"campaign_name_campaigns\",\n",
    "    \"modelo_contactos\", \"proposta_realizada_stage_propostas\", \"motivo_da_perda_deals\"\n",
    "]\n",
    "\n",
    "# Keep only existing\n",
    "log_num = [c for c in log_num_desired_model if c in existing]\n",
    "lin_num = [c for c in lin_num_desired_model if c in existing]\n",
    "cat_cols = [c for c in cat_cols_desired if c in existing]\n",
    "drop_cols = [c for c in manual_drop if c in existing]\n",
    "\n",
    "Xdf = pdf.drop(columns=drop_cols)\n",
    "print(\"Numerics (log) used in MODEL:\", log_num)\n",
    "print(\"Numerics (linear) used in MODEL:\", lin_num)\n",
    "print(\"Categoricals used in MODEL:\", cat_cols)\n",
    "if drop_cols: print(\"Dropped (modeling only):\", drop_cols)\n",
    "if car_value_col: print(\"Car value KEPT for profiling only:\", car_value_col)\n",
    "assert (log_num or lin_num or cat_cols), \"No usable features.\"\n",
    "\n",
    "# ---------- 3) Preprocess ----------\n",
    "def _log1p(x): return np.log1p(x)\n",
    "\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "ohe_extra = {\"min_frequency\": 0.02, \"max_categories\": 25} if version.parse(sklearn.__version__) >= version.parse(\"1.1\") else {}\n",
    "\n",
    "stages = []\n",
    "if log_num:\n",
    "    stages.append((\n",
    "        \"num_log\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"log\", FunctionTransformer(_log1p, feature_names_out=\"one-to-one\")),\n",
    "            (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "        ]),\n",
    "        log_num\n",
    "    ))\n",
    "if lin_num:\n",
    "    stages.append((\n",
    "        \"num_lin\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "        ]),\n",
    "        lin_num\n",
    "    ))\n",
    "if cat_cols:\n",
    "    stages.append((\n",
    "        \"cat\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "            (\"ohe\", OneHotEncoder(**ohe_kw, **ohe_extra)),\n",
    "        ]),\n",
    "        cat_cols\n",
    "    ))\n",
    "\n",
    "preprocess = ColumnTransformer(stages, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# ---------- 4) Encode -> SVD (12 comps) -> whiten ----------\n",
    "Xs = preprocess.fit_transform(Xdf)                        # sparse after OHE\n",
    "n_comp = max(2, min(12, Xs.shape[1]-1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "expl = svd.explained_variance_ratio_.sum()\n",
    "X = StandardScaler().fit_transform(X)                    # final whitening for KMeans\n",
    "print(f\"Sparse shape: {Xs.shape} | Reduced: {X.shape} | SVD explained variance: {expl:.3f}\")\n",
    "\n",
    "# ---------- 5) k-scan (Elbow + Silhouette) ----------\n",
    "metrics = []\n",
    "for k in range(2, 10):\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=30, max_iter=250, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_)))\n",
    "    print(f\"k={k} | silhouette={sil:.4f} | inertia={km.inertia_:.0f}\")\n",
    "\n",
    "m = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\"])\n",
    "best = m.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).iloc[0]\n",
    "best_k, best_sil = int(best.k), float(best.silhouette)\n",
    "print(f\"\\nBest k = {best_k} | silhouette = {best_sil:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(m.k, m.inertia, marker=\"o\"); ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\")\n",
    "ax.set_title(\"Elbow Curve (Inertia)\"); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(m.k, m.silhouette, marker=\"o\"); ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\")\n",
    "ax.set_title(\"Silhouette vs k\"); plt.show()\n",
    "\n",
    "# ---------- 6) Final model + labels ----------\n",
    "final = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=60, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = final.labels_\n",
    "\n",
    "# ---------- 7) Profiling ----------\n",
    "def quick_readout(df, cluster_col, value_col, buckets=(30, 60, 90), unit=\"d\"):\n",
    "    if value_col not in df.columns:\n",
    "        return\n",
    "    print(\"=== Quick readout per cluster ===\")\n",
    "    for k in sorted(df[cluster_col].dropna().unique()):\n",
    "        g = df[df[cluster_col]==k][value_col].astype(float)\n",
    "        if g.empty: continue\n",
    "        med = np.nanmedian(g)\n",
    "        parts = [(f\"<= {b}{unit}\", (g<=b).mean()*100) for b in buckets] + [(f\"> {buckets[-1]}{unit}\", (g>buckets[-1]).mean()*100)]\n",
    "        print(f\"Cluster {k}: median={med:.1f}{unit} | \" + \"  \".join([f\"{a}={b:.1f}%\" for a,b in parts]))\n",
    "\n",
    "# Sizes\n",
    "sizes = pdf[\"cluster\"].value_counts(dropna=False).sort_index()\n",
    "display(sizes.to_frame(\"count\").assign(pct=(sizes/len(pdf)*100).round(2)))\n",
    "\n",
    "# Behavioral KPI readout\n",
    "if \"lead_to_deal_days\" in pdf.columns:\n",
    "    quick_readout(pdf, \"cluster\", \"lead_to_deal_days\", buckets=(30,60,90), unit=\"d\")\n",
    "\n",
    "# Car value (profiling only)\n",
    "if car_value_col:\n",
    "    med_price = pdf.groupby(\"cluster\")[car_value_col].median()\n",
    "    print(f\"\\nMedian car value ({car_value_col}) by cluster:\\n\", med_price)\n",
    "\n",
    "# Numeric heatmap (model numerics only)\n",
    "model_num_cols = [c for c in (log_num + lin_num) if c in pdf.columns]\n",
    "if model_num_cols:\n",
    "    med = pdf.groupby(\"cluster\")[model_num_cols].median(numeric_only=True)\n",
    "    cols = [c for c in model_num_cols if c in med.columns]\n",
    "    if cols:\n",
    "        med_std = (med[cols] - med[cols].mean()) / med[cols].std(ddof=0)\n",
    "        plt.figure(figsize=(min(12, 0.6*len(cols)+3), 0.5*len(med_std)+3))\n",
    "        plt.imshow(med_std[cols], aspect=\"auto\")\n",
    "        plt.yticks(range(len(med_std.index)), med_std.index)\n",
    "        plt.xticks(range(len(cols)), cols, rotation=60, ha=\"right\")\n",
    "        plt.title(\"Cluster z-score profiles (numeric medians) — MODEL FEATURES ONLY\")\n",
    "        plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2-D projection & silhouette by cluster\n",
    "try:\n",
    "    svd2 = TruncatedSVD(n_components=2, random_state=SEED)\n",
    "    X2 = svd2.fit_transform(Xs)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for k in sorted(pdf[\"cluster\"].unique()):\n",
    "        idx = (pdf[\"cluster\"]==k).values\n",
    "        plt.scatter(X2[idx,0], X2[idx,1], s=8, alpha=0.6, label=f\"c{k}\")\n",
    "    plt.legend(title=\"cluster\", ncol=2); plt.title(\"2-D projection (SVD) by cluster\")\n",
    "    plt.xlabel(\"Comp 1\"); plt.ylabel(\"Comp 2\"); plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"[scatter] skipped:\", e)\n",
    "\n",
    "try:\n",
    "    sil = silhouette_samples(X, pdf[\"cluster\"].values)\n",
    "    print(\"Overall silhouette:\", round(float(sil.mean()), 4))\n",
    "    tmp = pd.DataFrame({\"cluster\": pdf[\"cluster\"].values, \"silhouette\": sil})\n",
    "    ax = tmp.boxplot(by=\"cluster\", column=\"silhouette\", grid=False, figsize=(6,4))\n",
    "    plt.suptitle(\"\"); plt.title(\"Silhouette by cluster\"); plt.xlabel(\"cluster\"); plt.ylabel(\"silhouette\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"[silhouette] skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e30fe93-604f-4233-afa4-97f7c37d22c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Improved K-Means (add key categoricals) ===\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn\n",
    "from packaging import version\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# ---------- 1) Load data ----------\n",
    "sdf = spark.table(SRC_TBL)\n",
    "pdf = sdf.toPandas()\n",
    "pdf = pdf.loc[:, ~pdf.columns.duplicated()]\n",
    "print(\"Rows:\", len(pdf), \"| Cols:\", len(pdf.columns))\n",
    "existing = set(pdf.columns)\n",
    "\n",
    "# ---------- 2) Feature sets ----------\n",
    "# Numeric variables (log + linear)\n",
    "log_num = [\"lead_to_deal_days\", \"overall_sales_duration_deals\"]\n",
    "lin_num = [\"proposals_per_vehicle\"]\n",
    "\n",
    "# Categorical variables to add\n",
    "cat_cols = [\"lead_source\", \"combustivel_propostas\", \"deal_speed_category\"]\n",
    "\n",
    "# Ensure columns exist\n",
    "log_num = [c for c in log_num if c in existing]\n",
    "lin_num = [c for c in lin_num if c in existing]\n",
    "cat_cols = [c for c in cat_cols if c in existing]\n",
    "print(\"Numerics log-scaled:\", log_num)\n",
    "print(\"Numerics linear:\", lin_num)\n",
    "print(\"Categoricals:\", cat_cols)\n",
    "assert (log_num or lin_num or cat_cols), \"No usable features.\"\n",
    "\n",
    "# ---------- 3) Preprocessing ----------\n",
    "def _log1p(x): return np.log1p(x)\n",
    "\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "ohe_extra = {\"min_frequency\": 0.02, \"max_categories\": 25} if version.parse(sklearn.__version__) >= version.parse(\"1.1\") else {}\n",
    "\n",
    "stages = []\n",
    "if log_num:\n",
    "    stages.append((\n",
    "        \"num_log\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"log\", FunctionTransformer(_log1p, feature_names_out=\"one-to-one\")),\n",
    "            (\"sc\", RobustScaler(with_centering=True, with_scaling=True)),\n",
    "        ]),\n",
    "        log_num\n",
    "    ))\n",
    "if lin_num:\n",
    "    stages.append((\n",
    "        \"num_lin\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\", RobustScaler(with_centering=True, with_scaling=True)),\n",
    "        ]),\n",
    "        lin_num\n",
    "    ))\n",
    "if cat_cols:\n",
    "    stages.append((\n",
    "        \"cat\",\n",
    "        Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "            (\"ohe\", OneHotEncoder(**ohe_kw, **ohe_extra)),\n",
    "        ]),\n",
    "        cat_cols\n",
    "    ))\n",
    "\n",
    "preprocess = ColumnTransformer(stages, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# ---------- 4) Encode -> SVD -> whiten ----------\n",
    "Xs = preprocess.fit_transform(pdf)\n",
    "n_comp = max(2, min(12, Xs.shape[1]-1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "expl = svd.explained_variance_ratio_.sum()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(f\"Sparse shape: {Xs.shape} | Reduced: {X.shape} | SVD explained variance: {expl:.3f}\")\n",
    "\n",
    "# ---------- 5) k-scan (Elbow + Silhouette) ----------\n",
    "metrics = []\n",
    "for k in range(2, 10):\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=30, max_iter=250, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_)))\n",
    "    print(f\"k={k} | silhouette={sil:.4f} | inertia={km.inertia_:.0f}\")\n",
    "\n",
    "m = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\"])\n",
    "best = m.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).iloc[0]\n",
    "best_k, best_sil = int(best.k), float(best.silhouette)\n",
    "print(f\"\\nBest k = {best_k} | silhouette = {best_sil:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(6,8))\n",
    "ax[0].plot(m.k, m.inertia, marker=\"o\")\n",
    "ax[0].set_xlabel(\"k\"); ax[0].set_ylabel(\"Inertia (lower is better)\"); ax[0].set_title(\"Elbow Curve (Inertia)\")\n",
    "ax[1].plot(m.k, m.silhouette, marker=\"o\")\n",
    "ax[1].set_xlabel(\"k\"); ax[1].set_ylabel(\"Silhouette (higher is better)\"); ax[1].set_title(\"Silhouette vs k\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------- 6) Final K-Means ----------\n",
    "final = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=60, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = final.labels_\n",
    "\n",
    "# ---------- Profiling ----------\n",
    "def quick_readout(df, cluster_col, value_col, buckets=(30, 60, 90), unit=\"d\"):\n",
    "    print(\"=== Quick readout per cluster ===\")\n",
    "    for k in sorted(df[cluster_col].dropna().unique()):\n",
    "        g = df[df[cluster_col] == k][value_col].astype(float)\n",
    "        if g.empty:\n",
    "            continue\n",
    "        med = np.nanmedian(g)\n",
    "        parts = [(f\"<= {b}{unit}\", (g <= b).mean() * 100) for b in buckets]\n",
    "        parts.append((f\"> {buckets[-1]}{unit}\", (g > buckets[-1]).mean() * 100))\n",
    "        # ✅ fixed line (no escaping needed)\n",
    "        print(\n",
    "            f\"Cluster {k}: median={med:.1f}{unit} | \"\n",
    "            + \"  \".join([f\"{a}={b:.1f}%\" for (a, b) in parts])\n",
    "        )\n",
    "\n",
    "# Heatmap for numeric medians\n",
    "num_cols = [c for c in (log_num + lin_num) if c in pdf.columns]\n",
    "if num_cols:\n",
    "    med = pdf.groupby(\"cluster\")[num_cols].median(numeric_only=True)\n",
    "    med_std = (med - med.mean()) / med.std(ddof=0)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.imshow(med_std, aspect=\"auto\", cmap=\"coolwarm\")\n",
    "    plt.yticks(range(len(med_std.index)), med_std.index)\n",
    "    plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Cluster z-score profiles (numeric medians)\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Top categorical breakdowns\n",
    "def top_cats(df, cluster_col, cat_col, topn=5):\n",
    "    out = []\n",
    "    for k, g in df.groupby(cluster_col):\n",
    "        s = g[cat_col].astype(str).replace(\"nan\", \"__MISSING__\")\n",
    "        vcs = s.value_counts(normalize=True).head(topn)\n",
    "        for val, frac in vcs.items():\n",
    "            out.append([cat_col, int(k), val, float(round(frac * 100, 2))])\n",
    "    return pd.DataFrame(out, columns=[\"column\", \"cluster\", \"value\", \"pct\"])\n",
    "\n",
    "for c in cat_cols:\n",
    "    display(top_cats(pdf, \"cluster\", c, topn=5))\n",
    "\n",
    "# 2D SVD Projection\n",
    "svd2 = TruncatedSVD(n_components=2, random_state=SEED)\n",
    "X2 = svd2.fit_transform(Xs)\n",
    "plt.figure(figsize=(6, 5))\n",
    "for k in sorted(pdf[\"cluster\"].unique()):\n",
    "    idx = (pdf[\"cluster\"] == k).values\n",
    "    plt.scatter(X2[idx, 0], X2[idx, 1], s=8, alpha=0.5, label=f\"c{k}\")\n",
    "plt.legend(title=\"cluster\", ncol=2)\n",
    "plt.title(\"2D Projection by Cluster\")\n",
    "plt.xlabel(\"Comp 1\")\n",
    "plt.ylabel(\"Comp 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Silhouette per cluster\n",
    "sil = silhouette_samples(X, pdf[\"cluster\"].values)\n",
    "print(\"Overall silhouette:\", round(float(sil.mean()), 4))\n",
    "tmp = pd.DataFrame({\"cluster\": pdf[\"cluster\"].values, \"silhouette\": sil})\n",
    "tmp.boxplot(by=\"cluster\", column=\"silhouette\", grid=False, figsize=(6, 4))\n",
    "plt.suptitle(\"\")\n",
    "plt.title(\"Silhouette by cluster\")\n",
    "plt.xlabel(\"cluster\")\n",
    "plt.ylabel(\"silhouette\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adaacaf6-b8ed-41b1-b352-64c462d4a010",
     "showTitle": false,
     "tableResultSettingsMap": {
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"column\":140},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759691351375}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === K-Means with lead_source + deal_speed_category (combustivel_propostas DROPPED) ===\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# ---------- 1) Load ----------\n",
    "pdf = spark.table(SRC_TBL).toPandas()\n",
    "pdf = pdf.loc[:, ~pdf.columns.duplicated()]\n",
    "print(\"Rows:\", len(pdf), \"| Cols:\", len(pdf.columns))\n",
    "existing = set(pdf.columns)\n",
    "\n",
    "# ---------- 2) Features ----------\n",
    "# Numerics for behavior\n",
    "log_num = [c for c in [\"lead_to_deal_days\", \"overall_sales_duration_deals\"] if c in existing]\n",
    "lin_num = [c for c in [\"proposals_per_vehicle\"] if c in existing]\n",
    "\n",
    "# Categoricals (DROP combustivel_propostas)\n",
    "cat_cols = [c for c in [\"lead_source\", \"deal_speed_category\"] if c in existing]\n",
    "\n",
    "print(\"Numerics log-scaled:\", log_num)\n",
    "print(\"Numerics linear:\", lin_num)\n",
    "print(\"Categoricals:\", cat_cols)\n",
    "assert (log_num or lin_num or cat_cols), \"No usable features.\"\n",
    "\n",
    "# ---------- 3) Preprocess ----------\n",
    "def _log1p(x): return np.log1p(x)\n",
    "\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "ohe_extra = {\"min_frequency\": 0.02, \"max_categories\": 25} if version.parse(sklearn.__version__) >= version.parse(\"1.1\") else {}\n",
    "\n",
    "num_log_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log\", FunctionTransformer(_log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "\n",
    "num_lin_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "    (\"ohe\", OneHotEncoder(**ohe_kw, **ohe_extra)),\n",
    "])\n",
    "\n",
    "transformers = []\n",
    "if log_num: transformers.append((\"num_log\", num_log_pipe, log_num))\n",
    "if lin_num: transformers.append((\"num_lin\", num_lin_pipe, lin_num))\n",
    "if cat_cols: transformers.append((\"cat\", cat_pipe, cat_cols))\n",
    "\n",
    "# Optional: re-balance numeric vs categorical influence\n",
    "# Increase/decrease 'cat' weight (e.g., 0.6) to prevent dominance\n",
    "weights = {\"num_log\": 1.0, \"num_lin\": 1.0, \"cat\": 0.7}  # <- tweak if desired\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers,\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,\n",
    "    transformer_weights={k: v for k, v in weights.items() if any(t[0] == k for t in transformers)}\n",
    ")\n",
    "\n",
    "# ---------- 4) Encode -> SVD -> whiten ----------\n",
    "Xs = preprocess.fit_transform(pdf)\n",
    "n_comp = max(2, min(12, Xs.shape[1]-1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "expl = svd.explained_variance_ratio_.sum()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(f\"Sparse shape: {Xs.shape} | Reduced: {X.shape} | SVD explained variance: {expl:.3f}\")\n",
    "\n",
    "# ---------- 5) k-scan ----------\n",
    "metrics = []\n",
    "for k in range(2, 11):\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=30, max_iter=250, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_)))\n",
    "    print(f\"k={k} | silhouette={sil:.4f} | inertia={km.inertia_:.0f}\")\n",
    "\n",
    "m = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\"])\n",
    "best = m.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).iloc[0]\n",
    "best_k, best_sil = int(best.k), float(best.silhouette)\n",
    "print(f\"\\nBest k = {best_k} | silhouette = {best_sil:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(6,8))\n",
    "ax[0].plot(m.k, m.inertia, marker=\"o\"); ax[0].set_xlabel(\"k\"); ax[0].set_ylabel(\"Inertia\"); ax[0].set_title(\"Elbow Curve\")\n",
    "ax[1].plot(m.k, m.silhouette, marker=\"o\"); ax[1].set_xlabel(\"k\"); ax[1].set_ylabel(\"Silhouette\"); ax[1].set_title(\"Silhouette vs k\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------- 6) Final model ----------\n",
    "final = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=60, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = final.labels_\n",
    "\n",
    "# ---------- 7) Profiling ----------\n",
    "def quick_readout(df, cluster_col, value_col, buckets=(30,60,90), unit=\"d\"):\n",
    "    print(\"=== Quick readout per cluster ===\")\n",
    "    for k in sorted(df[cluster_col].dropna().unique()):\n",
    "        g = df[df[cluster_col]==k][value_col].astype(float)\n",
    "        if g.empty: continue\n",
    "        med = np.nanmedian(g)\n",
    "        parts = [(f\"<= {b}{unit}\", (g<=b).mean()*100) for b in buckets] + [(f\"> {buckets[-1]}{unit}\", (g>buckets[-1]).mean()*100)]\n",
    "        print(f\"Cluster {k}: median={med:.1f}{unit} | \" + \"  \".join([f\"{a}={b:.1f}%\" for a,b in parts]))\n",
    "\n",
    "sizes = pdf[\"cluster\"].value_counts().sort_index()\n",
    "display(sizes.to_frame(\"count\").assign(pct=(sizes/len(pdf)*100).round(2)))\n",
    "\n",
    "if \"lead_to_deal_days\" in pdf.columns:\n",
    "    quick_readout(pdf, \"cluster\", \"lead_to_deal_days\")\n",
    "\n",
    "# Numeric heatmap\n",
    "num_cols = [c for c in (log_num + lin_num) if c in pdf.columns]\n",
    "if num_cols:\n",
    "    med = pdf.groupby(\"cluster\")[num_cols].median(numeric_only=True)\n",
    "    med_std = (med - med.mean()) / med.std(ddof=0)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.imshow(med_std, aspect=\"auto\", cmap=\"coolwarm\")\n",
    "    plt.yticks(range(len(med_std.index)), med_std.index)\n",
    "    plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Cluster z-score profiles (numeric medians)\")\n",
    "    plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Top categorical breakdowns\n",
    "def top_cats(df, cluster_col, cat_col, topn=5):\n",
    "    out=[]\n",
    "    for k,g in df.groupby(cluster_col):\n",
    "        s=g[cat_col].astype(str).replace(\"nan\",\"__MISSING__\")\n",
    "        vc=s.value_counts(normalize=True).head(topn)\n",
    "        for val, frac in vc.items(): out.append([cat_col,int(k),val,float(round(frac*100,2))])\n",
    "    return pd.DataFrame(out, columns=[\"column\",\"cluster\",\"value\",\"pct\"])\n",
    "\n",
    "for c in cat_cols:\n",
    "    display(top_cats(pdf, \"cluster\", c, topn=5))\n",
    "\n",
    "# 2D projection\n",
    "svd2 = TruncatedSVD(n_components=2, random_state=SEED)\n",
    "X2 = svd2.fit_transform(Xs)\n",
    "plt.figure(figsize=(6,5))\n",
    "for k in sorted(pdf[\"cluster\"].unique()):\n",
    "    idx = (pdf[\"cluster\"]==k).values\n",
    "    plt.scatter(X2[idx,0], X2[idx,1], s=8, alpha=0.5, label=f\"c{k}\")\n",
    "plt.legend(title=\"cluster\", ncol=2); plt.title(\"2D Projection by Cluster\")\n",
    "plt.xlabel(\"Comp 1\"); plt.ylabel(\"Comp 2\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Silhouette per cluster\n",
    "sil = silhouette_samples(X, pdf[\"cluster\"].values)\n",
    "print(\"Overall silhouette:\", round(float(sil.mean()), 4))\n",
    "tmp = pd.DataFrame({\"cluster\": pdf[\"cluster\"].values, \"silhouette\": sil})\n",
    "tmp.boxplot(by=\"cluster\", column=\"silhouette\", grid=False, figsize=(6,4))\n",
    "plt.suptitle(\"\"); plt.title(\"Silhouette by cluster\"); plt.xlabel(\"cluster\"); plt.ylabel(\"silhouette\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922e89e3-206d-4574-b542-d6fd3ea6191b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FINAL K-MEANS (combustivel_propostas DROPPED) + BETTER GRAPHS ===\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\n",
    "\n",
    "SEED   = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# ------------------ Load ------------------\n",
    "pdf = spark.table(SRC_TBL).toPandas()\n",
    "pdf = pdf.loc[:, ~pdf.columns.duplicated()]\n",
    "print(\"Rows:\", len(pdf), \"| Cols:\", len(pdf.columns))\n",
    "existing = set(pdf.columns)\n",
    "\n",
    "# ------------------ Features (combustivel_propostas is intentionally DROPPED) ------------------\n",
    "# Numerics (behavior)\n",
    "log_num = [c for c in [\"lead_to_deal_days\", \"overall_sales_duration_deals\"] if c in existing]\n",
    "lin_num = [c for c in [\"proposals_per_vehicle\"] if c in existing]\n",
    "\n",
    "# Categoricals to USE\n",
    "cat_cols = [c for c in [\"lead_source\", \"deal_speed_category\"] if c in existing]\n",
    "\n",
    "print(\"Numerics (log):   \", log_num)\n",
    "print(\"Numerics (linear):\", lin_num)\n",
    "print(\"Categoricals:     \", cat_cols)\n",
    "assert (log_num or lin_num or cat_cols), \"No usable features.\"\n",
    "\n",
    "# ------------------ Preprocess ------------------\n",
    "def _log1p(x): return np.log1p(x)\n",
    "\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "\n",
    "# reduce cardinality noise for cats if sklearn>=1.1\n",
    "ohe_extra = {\"min_frequency\": 0.02, \"max_categories\": 25} if version.parse(sklearn.__version__) >= version.parse(\"1.1\") else {}\n",
    "\n",
    "num_log_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log\", FunctionTransformer(_log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "\n",
    "num_lin_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "    (\"ohe\", OneHotEncoder(**ohe_kw, **ohe_extra)),\n",
    "])\n",
    "\n",
    "transformers = []\n",
    "if log_num: transformers.append((\"num_log\", num_log_pipe, log_num))\n",
    "if lin_num: transformers.append((\"num_lin\", num_lin_pipe, lin_num))\n",
    "if cat_cols: transformers.append((\"cat\", cat_pipe, cat_cols))\n",
    "\n",
    "# Slightly down-weight categoricals so numerics still drive shape\n",
    "weights = {\"num_log\": 1.0, \"num_lin\": 1.0, \"cat\": 0.75}\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers,\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,\n",
    "    transformer_weights={k:v for k,v in weights.items() if any(t[0]==k for t in transformers)}\n",
    ")\n",
    "\n",
    "# ------------------ Encode -> SVD -> Whiten ------------------\n",
    "Xs = preprocess.fit_transform(pdf)          # sparse after OHE\n",
    "n_comp = max(2, min(8, Xs.shape[1]-1))      # 8 comps by default (tweak 6..12)\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "expl = svd.explained_variance_ratio_.sum()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(f\"Sparse: {Xs.shape} | Reduced: {X.shape} | SVD explained variance: {expl:.3f}\")\n",
    "\n",
    "# ------------------ k-scan ------------------\n",
    "metrics = []\n",
    "k_range = range(2, 13)\n",
    "for k in k_range:\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=40, max_iter=300, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_)))\n",
    "    print(f\"k={k:2d} | silhouette={sil:.4f} | inertia={km.inertia_:.0f}\")\n",
    "\n",
    "m = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\"])\n",
    "best = m.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).iloc[0]\n",
    "best_k, best_sil = int(best.k), float(best.silhouette)\n",
    "print(f\"\\nBest k = {best_k} | silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# -------- Plots: Elbow + Silhouette --------\n",
    "fig, ax = plt.subplots(2,1,figsize=(6,8))\n",
    "ax[0].plot(m.k, m.inertia, marker=\"o\"); ax[0].set_title(\"Elbow Curve\"); ax[0].set_xlabel(\"k\"); ax[0].set_ylabel(\"Inertia\")\n",
    "ax[1].plot(m.k, m.silhouette, marker=\"o\"); ax[1].set_title(\"Silhouette vs k\"); ax[1].set_xlabel(\"k\"); ax[1].set_ylabel(\"Silhouette\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------------ Final model ------------------\n",
    "final = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=60, max_iter=400, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = final.labels_\n",
    "\n",
    "# ------------------ Better Profiling & Graphs ------------------\n",
    "def quick_readout(df, cluster_col, value_col, buckets=(30,60,90), unit=\"d\"):\n",
    "    print(\"=== Quick readout per cluster ===\")\n",
    "    for k in sorted(df[cluster_col].dropna().unique()):\n",
    "        g = df[df[cluster_col]==k][value_col].astype(float)\n",
    "        if g.empty: continue\n",
    "        med = np.nanmedian(g)\n",
    "        parts = [(f\"<= {b}{unit}\", (g<=b).mean()*100) for b in buckets] + [(f\"> {buckets[-1]}{unit}\", (g>buckets[-1]).mean()*100)]\n",
    "        print(f\"Cluster {k}: median={med:.1f}{unit} | \" + \"  \".join([f\"{a}={b:.1f}%\" for a,b in parts]))\n",
    "\n",
    "# 1) Cluster sizes (bar chart)\n",
    "sizes = pdf[\"cluster\"].value_counts().sort_index()\n",
    "sizes_df = sizes.to_frame(\"count\").assign(pct=(sizes/len(pdf)*100).round(2))\n",
    "display(sizes_df)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(sizes.index.astype(str), sizes.values)\n",
    "plt.title(\"Cluster sizes\"); plt.xlabel(\"cluster\"); plt.ylabel(\"count\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2) KPI readout for lead_to_deal_days (edit if you want another KPI)\n",
    "if \"lead_to_deal_days\" in pdf.columns:\n",
    "    quick_readout(pdf, \"cluster\", \"lead_to_deal_days\", buckets=(30,60,90), unit=\"d\")\n",
    "\n",
    "# 3) Numeric medians -> z-score heatmap\n",
    "num_cols = [c for c in (log_num + lin_num) if c in pdf.columns]\n",
    "if num_cols:\n",
    "    med = pdf.groupby(\"cluster\")[num_cols].median(numeric_only=True)\n",
    "    med_std = (med - med.mean()) / med.std(ddof=0)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(med_std, aspect=\"auto\", cmap=\"coolwarm\")\n",
    "    plt.yticks(range(len(med_std.index)), med_std.index)\n",
    "    plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Cluster z-score profiles (numeric medians)\")\n",
    "    plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4) Stacked bars for categoricals (proportions by cluster)\n",
    "def stacked_bar(df, cat_col, cluster_col=\"cluster\", topn=None, title=None):\n",
    "    s = df[[cluster_col, cat_col]].copy()\n",
    "    s[cat_col] = s[cat_col].astype(\"object\").where(s[cat_col].notna(), \"__MISSING__\")\n",
    "    # optional: keep topn categories (collapse rest)\n",
    "    if topn:\n",
    "        top_vals = s[cat_col].value_counts().nlargest(topn).index\n",
    "        s.loc[~s[cat_col].isin(top_vals), cat_col] = \"Other\"\n",
    "    tab = (s.groupby([cluster_col, cat_col]).size()\n",
    "             .groupby(level=0).apply(lambda x: x/x.sum()*100)\n",
    "             .unstack(fill_value=0)\n",
    "             .sort_index())\n",
    "    plt.figure(figsize=(max(7, 0.5*len(tab.columns)+4), 4))\n",
    "    bottoms = np.zeros(len(tab))\n",
    "    for c in tab.columns:\n",
    "        plt.bar(tab.index.astype(str), tab[c].values, bottom=bottoms, label=c)\n",
    "        bottoms += tab[c].values\n",
    "    plt.legend(title=cat_col, bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "    plt.title(title or f\"{cat_col}: % distribution by cluster\")\n",
    "    plt.xlabel(\"cluster\"); plt.ylabel(\"%\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "for c in cat_cols:\n",
    "    stacked_bar(pdf, c, topn=8, title=f\"{c} — % within cluster\")\n",
    "\n",
    "# 5) 2-D SVD projection colored by cluster\n",
    "svd2 = TruncatedSVD(n_components=2, random_state=SEED)\n",
    "X2 = svd2.fit_transform(Xs)\n",
    "plt.figure(figsize=(6,5))\n",
    "for k in sorted(pdf[\"cluster\"].unique()):\n",
    "    idx = (pdf[\"cluster\"]==k).values\n",
    "    plt.scatter(X2[idx,0], X2[idx,1], s=8, alpha=0.6, label=f\"c{k}\")\n",
    "plt.legend(title=\"cluster\", ncol=2, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.title(\"2D Projection by Cluster\")\n",
    "plt.xlabel(\"Comp 1\"); plt.ylabel(\"Comp 2\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 6) Silhouette by cluster (boxplot)\n",
    "sil = silhouette_samples(X, pdf[\"cluster\"].values)\n",
    "print(\"Overall silhouette:\", round(float(sil.mean()), 4))\n",
    "tmp = pd.DataFrame({\"cluster\": pdf[\"cluster\"].values, \"silhouette\": sil})\n",
    "tmp.boxplot(by=\"cluster\", column=\"silhouette\", grid=False, figsize=(6,4))\n",
    "plt.suptitle(\"\"); plt.title(\"Silhouette by cluster\"); plt.xlabel(\"cluster\"); plt.ylabel(\"silhouette\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458d04c1-39a2-401f-8bfc-aefaa286e7cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "if \"cluster\" in pdf.columns:\n",
    "    sizes = (pdf[\"cluster\"]\n",
    "             .value_counts(normalize=True)\n",
    "             .mul(100)\n",
    "             .rename(\"pct\")\n",
    "             .reset_index()\n",
    "             .rename(columns={\"index\":\"cluster\"}))\n",
    "    # sort clusters numerically if possible\n",
    "    try:\n",
    "        sizes[\"cluster\"] = sizes[\"cluster\"].astype(int)\n",
    "        sizes = sizes.sort_values(\"cluster\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    ax = sns.barplot(data=sizes, x=\"cluster\", y=\"pct\", palette=\"viridis\")\n",
    "    ax.set_title(\"Cluster Distribution (%)\")\n",
    "    ax.set_ylabel(\"Share of Total (%)\")\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.text(p.get_x()+p.get_width()/2, p.get_height()+0.5,\n",
    "                f\"{p.get_height():.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"[info] 'cluster' column not found; skipping cluster distribution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5188ebff-3c97-41dd-aaa7-1382278e0948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "need = {\"cluster\",\"lead_source\"}\n",
    "if need.issubset(pdf.columns):\n",
    "    # within-cluster % and pick top 5 per cluster\n",
    "    top_lead = (pdf.groupby([\"cluster\",\"lead_source\"])\n",
    "                  .size()\n",
    "                  .groupby(level=0, group_keys=False)\n",
    "                  .apply(lambda s: (s/s.sum()*100).nlargest(5))\n",
    "                  .reset_index(name=\"pct\"))\n",
    "\n",
    "    # sort clusters numerically if possible\n",
    "    try:\n",
    "        top_lead[\"cluster\"] = top_lead[\"cluster\"].astype(int)\n",
    "        top_lead = top_lead.sort_values([\"cluster\",\"pct\"], ascending=[True, False])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=top_lead, x=\"pct\", y=\"lead_source\", hue=\"cluster\", palette=\"tab10\")\n",
    "    plt.title(\"Top 5 Lead Sources per Cluster (%)\")\n",
    "    plt.xlabel(\"Share within Cluster (%)\")\n",
    "    plt.ylabel(\"Lead Source\")\n",
    "    plt.legend(title=\"Cluster\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(f\"[info] Missing columns {need - set(pdf.columns)}; skipping top lead chart.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd8aa97-836f-4452-809b-bb2b41b491ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"cluster\" in pdf.columns:\n",
    "    # If you already computed X (reduced/whitened matrix) use that; else use Xs (encoded) or bail.\n",
    "    try:\n",
    "        sil_vals = silhouette_samples(X, pdf[\"cluster\"])\n",
    "    except NameError:\n",
    "        sil_vals = silhouette_samples(Xs, pdf[\"cluster\"])  # fallback if X not available\n",
    "\n",
    "    pdf[\"silhouette\"] = sil_vals\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.boxplot(data=pdf, x=\"cluster\", y=\"silhouette\", palette=\"coolwarm\")\n",
    "    plt.title(f\"Silhouette by Cluster (overall mean = {np.mean(sil_vals):.3f})\")\n",
    "    plt.ylabel(\"Silhouette Coefficient\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"[info] 'cluster' column not found; skipping silhouette plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558af789-6a70-4b80-aec4-48c7b1e2c3de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Use the sparse encoded matrix Xs if you have it; otherwise use X (already reduced) and pad if needed\n",
    "try:\n",
    "    svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X2 = svd2.fit_transform(Xs)     # preferred: start from encoded sparse matrix\n",
    "except NameError:\n",
    "    # if X exists but might be >2D or 1D\n",
    "    X2 = X if X.shape[1] >= 2 else np.pad(X, ((0,0),(0,2-X.shape[1])), mode=\"constant\")\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x=X2[:,0], y=X2[:,1], hue=pdf[\"cluster\"], palette=\"Set2\", alpha=0.45, s=18, linewidth=0)\n",
    "plt.title(\"2D Projection of Clusters (SVD components)\")\n",
    "plt.xlabel(\"Component 1\"); plt.ylabel(\"Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0eba485-7858-43b8-83ec-49ee007bca61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FINAL K-MEANS (combustivel_propostas DROPPED) + METRICS (Silhouette, DBI, CH, Dunn) ===\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\n",
    "\n",
    "SEED   = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# ------------------ Load ------------------\n",
    "pdf = spark.table(SRC_TBL).toPandas()\n",
    "pdf = pdf.loc[:, ~pdf.columns.duplicated()]\n",
    "print(\"Rows:\", len(pdf), \"| Cols:\", len(pdf.columns))\n",
    "existing = set(pdf.columns)\n",
    "\n",
    "# ------------------ Features (combustivel_propostas is intentionally DROPPED) ------------------\n",
    "# Numerics (behavior)\n",
    "log_num = [c for c in [\"lead_to_deal_days\", \"overall_sales_duration_deals\"] if c in existing]\n",
    "lin_num = [c for c in [\"proposals_per_vehicle\"] if c in existing]\n",
    "\n",
    "# Categoricals to USE\n",
    "cat_cols = [c for c in [\"lead_source\", \"deal_speed_category\"] if c in existing]\n",
    "\n",
    "print(\"Numerics (log):   \", log_num)\n",
    "print(\"Numerics (linear):\", lin_num)\n",
    "print(\"Categoricals:     \", cat_cols)\n",
    "assert (log_num or lin_num or cat_cols), \"No usable features.\"\n",
    "\n",
    "# ------------------ Preprocess ------------------\n",
    "def _log1p(x): return np.log1p(x)\n",
    "\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "\n",
    "# reduce cardinality noise for cats if sklearn>=1.1\n",
    "ohe_extra = {\"min_frequency\": 0.02, \"max_categories\": 25} if version.parse(sklearn.__version__) >= version.parse(\"1.1\") else {}\n",
    "\n",
    "num_log_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log\", FunctionTransformer(_log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "\n",
    "num_lin_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "    (\"ohe\", OneHotEncoder(**ohe_kw, **ohe_extra)),\n",
    "])\n",
    "\n",
    "transformers = []\n",
    "if log_num: transformers.append((\"num_log\", num_log_pipe, log_num))\n",
    "if lin_num: transformers.append((\"num_lin\", num_lin_pipe, lin_num))\n",
    "if cat_cols: transformers.append((\"cat\", cat_pipe, cat_cols))\n",
    "\n",
    "# Slightly down-weight categoricals so numerics still drive shape\n",
    "weights = {\"num_log\": 1.0, \"num_lin\": 1.0, \"cat\": 0.75}\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers,\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,\n",
    "    transformer_weights={k: v for k, v in weights.items() if any(t[0] == k for t in transformers)}\n",
    ")\n",
    "\n",
    "# ------------------ Encode -> SVD -> Whiten ------------------\n",
    "Xs = preprocess.fit_transform(pdf)          # sparse after OHE\n",
    "n_comp = max(2, min(8, Xs.shape[1]-1))      # 8 comps by default (tweak 6..12)\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "expl = svd.explained_variance_ratio_.sum()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(f\"Sparse: {Xs.shape} | Reduced: {X.shape} | SVD explained variance: {expl:.3f}\")\n",
    "\n",
    "# ------------------ k-scan ------------------\n",
    "metrics = []\n",
    "k_range = range(2, 13)\n",
    "for k in k_range:\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=40, max_iter=300, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_)))\n",
    "    print(f\"k={k:2d} | silhouette={sil:.4f} | inertia={km.inertia_:.0f}\")\n",
    "\n",
    "m = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\"])\n",
    "best = m.sort_values([\"silhouette\",\"k\"], ascending=[False, True]).iloc[0]\n",
    "best_k, best_sil = int(best.k), float(best.silhouette)\n",
    "print(f\"\\nBest k = {best_k} | silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# -------- Plots: Elbow + Silhouette --------\n",
    "fig, ax = plt.subplots(2,1,figsize=(6,8))\n",
    "ax[0].plot(m.k, m.inertia, marker=\"o\"); ax[0].set_title(\"Elbow Curve\"); ax[0].set_xlabel(\"k\"); ax[0].set_ylabel(\"Inertia\")\n",
    "ax[1].plot(m.k, m.silhouette, marker=\"o\"); ax[1].set_title(\"Silhouette vs k\"); ax[1].set_xlabel(\"k\"); ax[1].set_ylabel(\"Silhouette\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------------ Final model ------------------\n",
    "final = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=60, max_iter=400, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = final.labels_\n",
    "\n",
    "# ------------------ Validation metrics (DBI, CH, Dunn, Silhouette overall) ------------------\n",
    "labels = pdf[\"cluster\"].values\n",
    "\n",
    "# Davies–Bouldin (lower is better)\n",
    "dbi = davies_bouldin_score(X, labels)\n",
    "\n",
    "# Calinski–Harabasz (higher is better)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "\n",
    "# Dunn Index (higher is better)\n",
    "def dunn_index(points: np.ndarray, labels: np.ndarray) -> float:\n",
    "    points = np.asarray(points); labels = np.asarray(labels)\n",
    "    uniq = np.unique(labels)\n",
    "\n",
    "    # max intra-cluster diameter\n",
    "    max_intra = 0.0\n",
    "    for k in uniq:\n",
    "        pts = points[labels == k]\n",
    "        if len(pts) > 1:\n",
    "            d = cdist(pts, pts, metric=\"euclidean\")\n",
    "            max_intra = max(max_intra, float(d.max()))\n",
    "\n",
    "    # min inter-cluster separation\n",
    "    min_inter = np.inf\n",
    "    for i, ki in enumerate(uniq[:-1]):\n",
    "        Xi = points[labels == ki]\n",
    "        for kj in uniq[i+1:]:\n",
    "            Xj = points[labels == kj]\n",
    "            d = cdist(Xi, Xj, metric=\"euclidean\")\n",
    "            val = float(d.min())\n",
    "            if val < min_inter:\n",
    "                min_inter = val\n",
    "\n",
    "    return float(min_inter / (max_intra + 1e-12))\n",
    "\n",
    "dunn = dunn_index(X, labels)\n",
    "sil_overall = silhouette_score(X, labels)\n",
    "\n",
    "print(\"\\n=== Cluster Quality Metrics ===\")\n",
    "print(f\"Silhouette (overall)     : {sil_overall:.4f}  (higher is better)\")\n",
    "print(f\"Davies–Bouldin Index     : {dbi:.4f}  (lower is better)\")\n",
    "print(f\"Calinski–Harabasz Score  : {ch:.2f}  (higher is better)\")\n",
    "print(f\"Dunn Index               : {dunn:.4f}  (higher is better)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8d4e98-6d54-471f-a57b-c6425901d7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === SELF-CONTAINED DBSCAN on sc_gold.Features_Table (matches your K-Means preprocessing) ===\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import cdist\n",
    "from time import perf_counter\n",
    "\n",
    "# quiet down threadpoolctl noise (Databricks quirk)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\n",
    "\n",
    "SEED   = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# ------------------ Load ------------------\n",
    "pdf = spark.table(SRC_TBL).toPandas()\n",
    "pdf = pdf.loc[:, ~pdf.columns.duplicated()]\n",
    "existing = set(pdf.columns)\n",
    "print(\"Rows:\", len(pdf), \"| Cols:\", len(pdf.columns))\n",
    "\n",
    "# ------------------ Features (same as your K-Means; combustivel_propostas intentionally DROPPED) ------------------\n",
    "log_num = [c for c in [\"lead_to_deal_days\", \"overall_sales_duration_deals\"] if c in existing]\n",
    "lin_num = [c for c in [\"proposals_per_vehicle\"] if c in existing]\n",
    "cat_cols = [c for c in [\"lead_source\", \"deal_speed_category\"] if c in existing]\n",
    "assert (log_num or lin_num or cat_cols), \"No usable features.\"\n",
    "\n",
    "print(\"Numerics (log):   \", log_num)\n",
    "print(\"Numerics (linear):\", lin_num)\n",
    "print(\"Categoricals:     \", cat_cols)\n",
    "\n",
    "# ------------------ Preprocess (IDENTICAL to your last cell) ------------------\n",
    "def _log1p(x): return np.log1p(x)\n",
    "\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "\n",
    "ohe_extra = {\"min_frequency\": 0.02, \"max_categories\": 25} if version.parse(sklearn.__version__) >= version.parse(\"1.1\") else {}\n",
    "\n",
    "num_log_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log\", FunctionTransformer(_log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "\n",
    "num_lin_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "    (\"ohe\", OneHotEncoder(**ohe_kw, **ohe_extra)),\n",
    "])\n",
    "\n",
    "transformers = []\n",
    "if log_num: transformers.append((\"num_log\", num_log_pipe, log_num))\n",
    "if lin_num: transformers.append((\"num_lin\", num_lin_pipe, lin_num))\n",
    "if cat_cols: transformers.append((\"cat\", cat_pipe, cat_cols))\n",
    "\n",
    "weights = {\"num_log\": 1.0, \"num_lin\": 1.0, \"cat\": 0.75}  # same balance\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers,\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,\n",
    "    transformer_weights={k:v for k,v in weights.items() if any(t[0]==k for t in transformers)}\n",
    ")\n",
    "\n",
    "# ------------------ Encode -> SVD -> Whiten ------------------\n",
    "Xs = preprocess.fit_transform(pdf)                 # sparse after OHE\n",
    "n_comp = max(2, min(8, Xs.shape[1]-1))            # same default 8 comps\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "expl = svd.explained_variance_ratio_.sum()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(f\"Sparse: {Xs.shape} | Reduced: {X.shape} | SVD explained variance: {expl:.3f}\")\n",
    "\n",
    "# ------------------ FAST DBSCAN (single fit; eps from k-distance sample) ------------------\n",
    "rng = np.random.default_rng(SEED)\n",
    "n, d = X.shape\n",
    "min_samples = max(2*d, 10)               # rule-of-thumb in low-dim space\n",
    "\n",
    "sample_k = min(10000, n)                 # <=10k to avoid OOM\n",
    "idx_s   = rng.choice(n, sample_k, replace=False) if n > sample_k else np.arange(n)\n",
    "Xsamp   = X[idx_s]\n",
    "\n",
    "t0 = perf_counter()\n",
    "nn = NearestNeighbors(n_neighbors=min_samples, metric='euclidean')\n",
    "nn.fit(Xsamp)\n",
    "dists, _ = nn.kneighbors(Xsamp)\n",
    "kth = np.sort(dists[:, -1])              # distance to k-th neighbor\n",
    "eps = float(np.percentile(kth, 95))      # robust heuristic\n",
    "print(f\"[DBSCAN] min_samples={min_samples}, eps≈{eps:.4f} (95th pct k-distance on {len(Xsamp)} sample)\")\n",
    "\n",
    "t1 = perf_counter()\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "labels = db.fit_predict(X)\n",
    "t2 = perf_counter()\n",
    "\n",
    "pdf[\"cluster_dbscan\"] = labels\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise    = int(np.sum(labels == -1))\n",
    "print(f\"[DBSCAN] clusters={n_clusters}, noise={n_noise} ({n_noise/n*100:.1f}%), fit_time={t2-t1:.1f}s, total={t2-t0:.1f}s\")\n",
    "\n",
    "# ------------------ Metrics on non-noise clusters ------------------\n",
    "mask = labels != -1\n",
    "if mask.sum() > 0 and len(np.unique(labels[mask])) > 1:\n",
    "    sil = silhouette_score(X[mask], labels[mask])\n",
    "    dbi = davies_bouldin_score(X[mask], labels[mask])\n",
    "    chi = calinski_harabasz_score(X[mask], labels[mask])\n",
    "    print(f\"Silhouette (no-noise): {sil:.4f}  |  DBI: {dbi:.4f}  |  Calinski–Harabasz: {chi:.1f}\")\n",
    "else:\n",
    "    sil = dbi = chi = None\n",
    "    print(\"Not enough non-noise clusters for metrics.\")\n",
    "\n",
    "# ------------------ Quick visuals ------------------\n",
    "# (a) k-distance curve with eps line\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(np.arange(len(kth)), kth)\n",
    "plt.axhline(eps, color='r', ls='--', lw=1)\n",
    "plt.title(\"k-distance curve (sample)\")\n",
    "plt.ylabel(f\"dist to {min_samples}-NN\"); plt.xlabel(\"sorted samples\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (b) DBSCAN cluster sizes (including noise)\n",
    "sizes = pd.Series(labels).value_counts().sort_index()\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(sizes.index.astype(str), sizes.values, color=\"tab:blue\")\n",
    "plt.title(\"DBSCAN cluster sizes (-1 = noise)\")\n",
    "plt.xlabel(\"label\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a8413a-9f8e-486e-9323-3820674955d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn-extra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ab743c-a4af-40b4-90ff-1bc4964ce6c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === SELF-CONTAINED PAM (CLARA-style) on sc_gold.Features_Table ===\n",
    "# - Same features & preprocessing as your K-Means cell\n",
    "# - Subsampled medoids (CLARA) + full assignment => fast on ~90k rows\n",
    "# - Prints Silhouette / DBI / CH and shows two compact plots\n",
    "\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, silhouette_samples\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Quiet some noisy runtime warnings on DBR\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\n",
    "\n",
    "# --- KMedoids from scikit-learn-extra ---\n",
    "try:\n",
    "    from sklearn_extra.cluster import KMedoids\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Please run `%pip install scikit-learn-extra` once in a separate cell, then re-run this.\") from e\n",
    "\n",
    "SEED    = 42\n",
    "RNG     = np.random.default_rng(SEED)\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# ------------------ 1) Load ------------------\n",
    "pdf = spark.table(SRC_TBL).toPandas()\n",
    "pdf = pdf.loc[:, ~pdf.columns.duplicated()]\n",
    "existing = set(pdf.columns)\n",
    "print(f\"Rows: {len(pdf)} | Cols: {len(pdf.columns)}\")\n",
    "\n",
    "# ------------------ 2) Features (same as K-Means; combustivel_propostas implicitly dropped) ------------------\n",
    "log_num = [c for c in [\"lead_to_deal_days\", \"overall_sales_duration_deals\"] if c in existing]\n",
    "lin_num = [c for c in [\"proposals_per_vehicle\"] if c in existing]\n",
    "cat_cols = [c for c in [\"lead_source\", \"deal_speed_category\"] if c in existing]\n",
    "assert (log_num or lin_num or cat_cols), \"No usable features.\"\n",
    "\n",
    "print(\"Numerics (log):   \", log_num)\n",
    "print(\"Numerics (linear):\", lin_num)\n",
    "print(\"Categoricals:     \", cat_cols)\n",
    "\n",
    "# ------------------ 3) Preprocess (identical shape to your K-Means) ------------------\n",
    "def _log1p(x): return np.log1p(x)\n",
    "\n",
    "ohe_kw = {\"handle_unknown\": \"ignore\"}\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kw[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kw[\"sparse\"] = True\n",
    "\n",
    "ohe_extra = {\"min_frequency\": 0.02, \"max_categories\": 25} if version.parse(sklearn.__version__) >= version.parse(\"1.1\") else {}\n",
    "\n",
    "num_log_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log\", FunctionTransformer(_log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "num_lin_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "    (\"ohe\", OneHotEncoder(**ohe_kw, **ohe_extra)),\n",
    "])\n",
    "\n",
    "transformers = []\n",
    "if log_num: transformers.append((\"num_log\", num_log_pipe, log_num))\n",
    "if lin_num: transformers.append((\"num_lin\", num_lin_pipe, lin_num))\n",
    "if cat_cols: transformers.append((\"cat\", cat_pipe, cat_cols))\n",
    "\n",
    "# keep cats slightly lighter so numerics guide geometry\n",
    "weights = {\"num_log\": 1.0, \"num_lin\": 1.0, \"cat\": 0.75}\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers,\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,\n",
    "    transformer_weights={k:v for k,v in weights.items() if any(t[0]==k for t in transformers)}\n",
    ")\n",
    "\n",
    "Xs = preprocess.fit_transform(pdf)                 # sparse after OHE\n",
    "n_comp = max(2, min(8, Xs.shape[1]-1))            # same default as before\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(f\"Sparse: {Xs.shape} | Reduced: {X.shape} | SVD explained variance: {svd.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# ------------------ 4) CLARA-style PAM ------------------\n",
    "# Fast medoids on subsamples, then assign all points to nearest medoid\n",
    "K_PAM        =  best_k if 'best_k' in globals() else 8   # set explicitly if you prefer\n",
    "SAMPLE_SIZE  =  18000                                    # 12–20k is good for ~90k rows\n",
    "N_SUBSAMPLES =  4                                        # 3–6 subsamples keeps it fast\n",
    "METRIC       = \"euclidean\"\n",
    "\n",
    "def assign_in_chunks(X_all, medoids, metric=\"euclidean\", chunk=20000):\n",
    "    \"\"\"Nearest-medoid assignment in memory-safe chunks.\"\"\"\n",
    "    n = X_all.shape[0]\n",
    "    out = np.empty(n, dtype=int)\n",
    "    for i in range(0, n, chunk):\n",
    "        j = min(i+chunk, n)\n",
    "        D = pairwise_distances(X_all[i:j], medoids, metric=metric)\n",
    "        out[i:j] = D.argmin(axis=1)\n",
    "    return out\n",
    "\n",
    "def clara_kmedoids(X_emb, k, sample_size, n_subsamples, metric=\"euclidean\", rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    best_cost, best_medoids = np.inf, None\n",
    "    for _ in range(n_subsamples):\n",
    "        idx = rng.choice(X_emb.shape[0], size=min(sample_size, X_emb.shape[0]), replace=False)\n",
    "        Xs = X_emb[idx]\n",
    "        # medoids on subsample\n",
    "        pam = KMedoids(n_clusters=k, metric=metric, method=\"alternate\",\n",
    "                       init=\"k-medoids++\", max_iter=300, random_state=int(rng.integers(1e9)))\n",
    "        pam.fit(Xs)\n",
    "        med_sub = Xs[pam.medoid_indices_]\n",
    "        # evaluate cost on ALL points cheaply\n",
    "        D_full = pairwise_distances(X_emb, med_sub, metric=metric)\n",
    "        cost = float(D_full.min(axis=1).sum())\n",
    "        if cost < best_cost:\n",
    "            best_cost, best_medoids = cost, med_sub\n",
    "    # final assignment for all points\n",
    "    labels_all = assign_in_chunks(X_emb, best_medoids, metric=metric, chunk=20000)\n",
    "    return labels_all, best_medoids\n",
    "\n",
    "pam_labels, pam_medoids = clara_kmedoids(X, K_PAM, SAMPLE_SIZE, N_SUBSAMPLES, metric=METRIC, rng=RNG)\n",
    "pdf[\"pam_cluster\"] = pam_labels\n",
    "\n",
    "# ------------------ 5) Metrics ------------------\n",
    "sil = silhouette_score(X, pam_labels) if K_PAM >= 2 else np.nan\n",
    "dbi = davies_bouldin_score(X, pam_labels) if K_PAM >= 2 else np.nan\n",
    "ch  = calinski_harabasz_score(X, pam_labels) if K_PAM >= 2 else np.nan\n",
    "print(f\"[PAM-CLARA] k={K_PAM} | Sil={sil:.4f} | DBI={dbi:.4f} | CH={ch:.1f}\")\n",
    "\n",
    "# ------------------ 6) Compact visuals ------------------\n",
    "# (a) Cluster sizes\n",
    "sizes = pd.Series(pam_labels).value_counts().sort_index()\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.bar(sizes.index.astype(str), sizes.values)\n",
    "plt.title(\"PAM (CLARA) cluster sizes\"); plt.xlabel(\"cluster\"); plt.ylabel(\"count\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# (b) Silhouette by cluster (boxplot)\n",
    "sil_s = silhouette_samples(X, pam_labels)\n",
    "tmp = pd.DataFrame({\"cluster\": pam_labels, \"silhouette\": sil_s})\n",
    "ax = tmp.boxplot(by=\"cluster\", column=\"silhouette\", grid=False, figsize=(8,4))\n",
    "plt.suptitle(\"\"); plt.title(f\"PAM (CLARA) — Silhouette by cluster (overall={sil:.3f})\")\n",
    "plt.xlabel(\"cluster\"); plt.ylabel(\"silhouette\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d249209-16b1-4d67-a326-86e68cc9c92b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========= DBSCAN — richer visuals =========\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# Safety: handle degenerate/noise-only cases\n",
    "has_clusters = (labels != -1).sum() > 0 and np.unique(labels[labels!=-1]).size >= 1\n",
    "has_2plus_clusters = (labels != -1).sum() > 0 and np.unique(labels[labels!=-1]).size >= 2\n",
    "\n",
    "# Helper: 2D scatter with sampling for large n\n",
    "def scatter_2d(X2, labels, title, max_points=40000, alpha=0.25, s=6):\n",
    "    n = X2.shape[0]\n",
    "    if n > max_points:\n",
    "        rng = np.random.default_rng(42)\n",
    "        idx = rng.choice(n, size=max_points, replace=False)\n",
    "    else:\n",
    "        idx = np.arange(n)\n",
    "\n",
    "    lab_s = labels[idx]\n",
    "    Xs = X2[idx]\n",
    "    uniq = np.unique(lab_s)\n",
    "\n",
    "    plt.figure(figsize=(6.5, 5))\n",
    "    for u in uniq:\n",
    "        m = lab_s == u\n",
    "        # noise = -1 in gray\n",
    "        if u == -1:\n",
    "            plt.scatter(Xs[m, 0], Xs[m, 1], s=s, alpha=alpha, label=\"noise (-1)\", c=\"tab:gray\")\n",
    "        else:\n",
    "            plt.scatter(Xs[m, 0], Xs[m, 1], s=s, alpha=alpha, label=f\"c{int(u)}\")\n",
    "    plt.legend(loc=\"best\", fontsize=8, ncol=2, frameon=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"SVD-1 (whitened)\"); plt.ylabel(\"SVD-2 (whitened)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# (1) 2D projection of clusters (SVD-1 vs SVD-2)\n",
    "scatter_2d(X[:, :2], labels, title=f\"DBSCAN — 2D projection (eps≈{eps:.3f}, min_samples={min_samples})\")\n",
    "\n",
    "# (2) Core vs Border vs Noise breakdown\n",
    "# A point is 'core' if DBSCAN.core_sample_indices_ contains it; 'border' otherwise (non-noise)\n",
    "role = np.full(len(labels), \"noise\", dtype=object)\n",
    "if hasattr(db, \"core_sample_indices_\"):\n",
    "    core_idx = np.zeros(len(labels), dtype=bool)\n",
    "    core_idx[db.core_sample_indices_] = True\n",
    "    role[(labels!=-1) & core_idx] = \"core\"\n",
    "    role[(labels!=-1) & ~core_idx] = \"border\"\n",
    "\n",
    "counts = pd.Series(role).value_counts().reindex([\"core\",\"border\",\"noise\"]).fillna(0).astype(int)\n",
    "plt.figure(figsize=(5.5,3))\n",
    "plt.bar(counts.index.astype(str), counts.values)\n",
    "plt.title(\"DBSCAN — point roles\")\n",
    "plt.xlabel(\"role\"); plt.ylabel(\"count\")\n",
    "for i,v in enumerate(counts.values): plt.text(i, v, f\"{v:,}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (3) Silhouette distribution (non-noise clusters only)\n",
    "if has_2plus_clusters:\n",
    "    mask = labels != -1\n",
    "    sil_s = silhouette_samples(X[mask], labels[mask])\n",
    "    tmp = pd.DataFrame({\"cluster\": labels[mask], \"silhouette\": sil_s})\n",
    "    ax = tmp.boxplot(by=\"cluster\", column=\"silhouette\", grid=False, figsize=(8,4))\n",
    "    plt.suptitle(\"\")\n",
    "    plt.title(f\"DBSCAN — silhouette by cluster (overall={silhouette_score(X[mask], labels[mask]):.3f})\")\n",
    "    plt.xlabel(\"cluster\"); plt.ylabel(\"silhouette\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"DBSCAN silhouette plot skipped (need ≥2 non-noise clusters).\")\n",
    "\n",
    "# (4) Distance-to-nearest core point (intuition for density)\n",
    "# Uses kNN distances you already computed (kth). Show the eps line and a zoom-in.\n",
    "plt.figure(figsize=(6.2,3.4))\n",
    "plt.plot(np.arange(len(kth)), kth)\n",
    "plt.axhline(eps, color=\"r\", ls=\"--\", lw=1, label=f\"eps={eps:.3f}\")\n",
    "plt.title(f\"k-distance (k={min_samples}) with eps line\")\n",
    "plt.xlabel(\"sorted samples\"); plt.ylabel(\"distance\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (5) Per-cluster size bars with percentage\n",
    "sizes = pd.Series(labels).value_counts().sort_index()\n",
    "plt.figure(figsize=(6.5,3.2))\n",
    "plt.bar(sizes.index.astype(str), sizes.values)\n",
    "plt.title(\"DBSCAN — cluster sizes (-1 = noise)\")\n",
    "plt.xlabel(\"label\"); plt.ylabel(\"count\")\n",
    "tot = sizes.sum()\n",
    "for i,(lab,cnt) in enumerate(sizes.items()):\n",
    "    pct = 100.0*cnt/tot\n",
    "    plt.text(i, cnt, f\"{cnt:,}\\n({pct:.1f}%)\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87cbb27e-9ec3-411b-9391-8bcd8b0a548f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========= PAM (CLARA) — richer visuals =========\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_samples, pairwise_distances\n",
    "\n",
    "K = len(np.unique(pam_labels))\n",
    "\n",
    "# (1) 2D projection with medoids highlighted\n",
    "def scatter_2d_with_medoids(X2, labels, medoids2, title, max_points=40000, alpha=0.25, s=6):\n",
    "    n = X2.shape[0]\n",
    "    if n > max_points:\n",
    "        rng = np.random.default_rng(42)\n",
    "        idx = rng.choice(n, size=max_points, replace=False)\n",
    "    else:\n",
    "        idx = np.arange(n)\n",
    "\n",
    "    lab_s = labels[idx]\n",
    "    Xs = X2[idx]\n",
    "    uniq = np.unique(lab_s)\n",
    "\n",
    "    plt.figure(figsize=(6.8,5.2))\n",
    "    for u in uniq:\n",
    "        m = lab_s == u\n",
    "        plt.scatter(Xs[m, 0], Xs[m, 1], s=s, alpha=alpha, label=f\"c{int(u)}\")\n",
    "    # overlay medoids\n",
    "    plt.scatter(medoids2[:,0], medoids2[:,1], s=120, marker=\"X\", edgecolors=\"k\", linewidths=1.0, label=\"medoids\")\n",
    "    plt.legend(loc=\"best\", fontsize=8, ncol=2, frameon=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"SVD-1 (whitened)\"); plt.ylabel(\"SVD-2 (whitened)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "scatter_2d_with_medoids(X[:, :2], pam_labels, pam_medoids[:, :2],\n",
    "                        title=f\"PAM (CLARA) — 2D projection (k={K})\")\n",
    "\n",
    "# (2) Distance-to-own-medoid distribution per cluster (compact violin-like boxplots)\n",
    "# Good to see cluster compactness and outliers\n",
    "D = pairwise_distances(X, pam_medoids, metric=\"euclidean\")\n",
    "d_self = D[np.arange(D.shape[0]), pam_labels]\n",
    "tmp = pd.DataFrame({\"cluster\": pam_labels, \"dist_to_medoid\": d_self})\n",
    "\n",
    "ax = tmp.boxplot(by=\"cluster\", column=\"dist_to_medoid\", grid=False, figsize=(8,4))\n",
    "plt.suptitle(\"\")\n",
    "plt.title(\"PAM — distance to own medoid by cluster\")\n",
    "plt.xlabel(\"cluster\"); plt.ylabel(\"distance to medoid\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (3) Silhouette by cluster (denser summary) — complements your boxplot\n",
    "sil_s = silhouette_samples(X, pam_labels) if K >= 2 else None\n",
    "if sil_s is not None:\n",
    "    tmp2 = pd.DataFrame({\"cluster\": pam_labels, \"silhouette\": sil_s})\n",
    "    means = tmp2.groupby(\"cluster\")[\"silhouette\"].mean().reindex(range(K), fill_value=np.nan)\n",
    "    plt.figure(figsize=(6.4,3.0))\n",
    "    plt.bar(np.arange(K).astype(str), means.values)\n",
    "    plt.title(f\"PAM — mean silhouette by cluster (overall={sil_s.mean():.3f})\")\n",
    "    plt.xlabel(\"cluster\"); plt.ylabel(\"mean silhouette\")\n",
    "    for i,v in enumerate(means.values):\n",
    "        if np.isfinite(v): plt.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"PAM silhouette bar skipped (need k ≥ 2).\")\n",
    "\n",
    "# (4) Cluster sizes with percentages (k already shown, but with % it’s handy)\n",
    "sizes = pd.Series(pam_labels).value_counts().sort_index()\n",
    "plt.figure(figsize=(6.5,3.2))\n",
    "plt.bar(sizes.index.astype(str), sizes.values)\n",
    "plt.title(\"PAM — cluster sizes\")\n",
    "plt.xlabel(\"cluster\"); plt.ylabel(\"count\")\n",
    "tot = sizes.sum()\n",
    "for i,(lab,cnt) in enumerate(sizes.items()):\n",
    "    pct = 100.0*cnt/tot\n",
    "    plt.text(i, cnt, f\"{cnt:,}\\n({pct:.1f}%)\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1249cf8-27ed-407f-917a-7419c3f4ed13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========= PAM (CLARA): cluster vs lead & speed =========\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "PAM_COL   = \"pam_cluster\"\n",
    "LEAD_CAT  = \"lead_source\"\n",
    "SPEED_CAT = \"deal_speed_category\"\n",
    "LEAD_NUM  = \"lead_to_deal_days\"\n",
    "TOP_K     = 10\n",
    "\n",
    "assert PAM_COL in pdf.columns, f\"Missing {PAM_COL} — run the PAM cell first.\"\n",
    "\n",
    "def _exists(col): return col in pdf.columns\n",
    "\n",
    "def _stacked_pct_bar(df, cluster_col, cat_col, top_k=10, title=\"\"):\n",
    "    vc_overall = df[cat_col].fillna(\"__MISSING__\").value_counts()\n",
    "    keep = set(vc_overall.head(top_k).index)\n",
    "    cat_clean = df[cat_col].fillna(\"__MISSING__\").where(df[cat_col].fillna(\"__MISSING__\").isin(keep), other=\"Other\")\n",
    "    tmp = (pd.crosstab(df[cluster_col], cat_clean, normalize=\"index\") * 100.0).sort_index()\n",
    "    tmp = tmp[sorted(tmp.columns, key=lambda c: (c==\"Other\", -tmp[c].sum()))]\n",
    "    ax = tmp.plot(kind=\"bar\", stacked=True, figsize=(9,4), width=0.85, legend=True)\n",
    "    ax.set_title(title); ax.set_xlabel(\"cluster\"); ax.set_ylabel(\"share (%)\")\n",
    "    ax.legend(bbox_to_anchor=(1.02,1), loc=\"upper left\", title=cat_col)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "dfp = pdf.copy()\n",
    "\n",
    "# (1) cluster × lead_source\n",
    "if _exists(LEAD_CAT):\n",
    "    _stacked_pct_bar(dfp, PAM_COL, LEAD_CAT,\n",
    "                     top_k=TOP_K, title=\"PAM — cluster × lead_source (% share)\")\n",
    "else:\n",
    "    print(f\"⚠️ '{LEAD_CAT}' not found, skipped.\")\n",
    "\n",
    "# (2) cluster × deal_speed_category\n",
    "if _exists(SPEED_CAT):\n",
    "    _stacked_pct_bar(dfp, PAM_COL, SPEED_CAT,\n",
    "                     top_k=min(8, TOP_K), title=\"PAM — cluster × deal_speed_category (% share)\")\n",
    "else:\n",
    "    print(f\"⚠️ '{SPEED_CAT}' not found, skipped.\")\n",
    "\n",
    "# (3) numeric: lead_to_deal_days by cluster (boxplot + medians)\n",
    "if _exists(LEAD_NUM) and dfp[PAM_COL].nunique() >= 1:\n",
    "    order = sorted(dfp[PAM_COL].unique())\n",
    "    data = [dfp.loc[dfp[PAM_COL]==c, LEAD_NUM].dropna().values for c in order]\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.boxplot(data, showfliers=False, labels=[str(c) for c in order])\n",
    "    plt.title(\"PAM — lead_to_deal_days by cluster\"); plt.xlabel(\"cluster\"); plt.ylabel(\"lead_to_deal_days\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    med = dfp.groupby(PAM_COL)[LEAD_NUM].median().sort_index()\n",
    "    cnt = dfp.groupby(PAM_COL)[LEAD_NUM].count().reindex(med.index).fillna(0).astype(int)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.bar(med.index.astype(str), med.values)\n",
    "    for i,(m,c) in enumerate(zip(med.values, cnt.values)):\n",
    "        plt.text(i, m, f\"n={c}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    plt.title(\"PAM — median lead_to_deal_days by cluster\")\n",
    "    plt.xlabel(\"cluster\"); plt.ylabel(\"median days\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    if not _exists(LEAD_NUM):\n",
    "        print(f\"ℹ️ '{LEAD_NUM}' not found; numeric lead plot skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac9844a-8400-4ebb-8e5c-a58a76fbd032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========= DBSCAN: cluster vs lead & speed =========\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# config — tweak as you like\n",
    "DBSCAN_COL = \"cluster_dbscan\"        # from your DBSCAN cell\n",
    "LEAD_CAT   = \"lead_source\"           # categorical \"lead\"\n",
    "SPEED_CAT  = \"deal_speed_category\"   # categorical \"speed\"\n",
    "LEAD_NUM   = \"lead_to_deal_days\"     # numeric \"lead speed\" proxy (optional)\n",
    "TOP_K      = 10                      # show top-K categories + \"Other\" in stacked bars\n",
    "INCLUDE_NOISE = False                # True to include label -1 as a cluster\n",
    "\n",
    "assert DBSCAN_COL in pdf.columns, f\"Missing {DBSCAN_COL} — run the DBSCAN cell first.\"\n",
    "\n",
    "def _exists(col): return col in pdf.columns\n",
    "\n",
    "def _stacked_pct_bar(df, cluster_col, cat_col, top_k=10, title=\"\"):\n",
    "    # prepare: keep top-K categories overall, bucket the rest as \"Other\"\n",
    "    vc_overall = df[cat_col].fillna(\"__MISSING__\").value_counts()\n",
    "    keep = set(vc_overall.head(top_k).index)\n",
    "    cat_clean = df[cat_col].fillna(\"__MISSING__\").where(df[cat_col].fillna(\"__MISSING__\").isin(keep), other=\"Other\")\n",
    "    tmp = (pd.crosstab(df[cluster_col], cat_clean, normalize=\"index\") * 100.0).sort_index()\n",
    "    tmp = tmp[sorted(tmp.columns, key=lambda c: (c==\"Other\", -tmp[c].sum()))]  # \"Other\" last\n",
    "    # plot\n",
    "    ax = tmp.plot(kind=\"bar\", stacked=True, figsize=(9,4), width=0.85, legend=True)\n",
    "    ax.set_title(title); ax.set_xlabel(\"cluster\"); ax.set_ylabel(\"share (%)\")\n",
    "    ax.legend(bbox_to_anchor=(1.02,1), loc=\"upper left\", title=cat_col)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# choose subset (optionally drop noise)\n",
    "dfd = pdf.copy()\n",
    "if not INCLUDE_NOISE:\n",
    "    dfd = dfd[dfd[DBSCAN_COL] != -1]\n",
    "\n",
    "# (1) stacked % bars: cluster × lead_source\n",
    "if _exists(LEAD_CAT):\n",
    "    _stacked_pct_bar(dfd, DBSCAN_COL, LEAD_CAT,\n",
    "                     top_k=TOP_K, title=\"DBSCAN — cluster × lead_source (% share)\")\n",
    "else:\n",
    "    print(f\"⚠️ '{LEAD_CAT}' not found, skipped.\")\n",
    "\n",
    "# (2) stacked % bars: cluster × deal_speed_category\n",
    "if _exists(SPEED_CAT):\n",
    "    _stacked_pct_bar(dfd, DBSCAN_COL, SPEED_CAT,\n",
    "                     top_k=min(8, TOP_K), title=\"DBSCAN — cluster × deal_speed_category (% share)\")\n",
    "else:\n",
    "    print(f\"⚠️ '{SPEED_CAT}' not found, skipped.\")\n",
    "\n",
    "# (3) numeric distributions: lead_to_deal_days by cluster (boxplot + medians)\n",
    "if _exists(LEAD_NUM) and not dfd.empty and dfd[DBSCAN_COL].nunique() >= 1:\n",
    "    # Boxplot\n",
    "    order = sorted(dfd[DBSCAN_COL].unique())\n",
    "    data = [dfd.loc[dfd[DBSCAN_COL]==c, LEAD_NUM].dropna().values for c in order]\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.boxplot(data, showfliers=False, labels=[str(c) for c in order])\n",
    "    plt.title(\"DBSCAN — lead_to_deal_days by cluster\"); plt.xlabel(\"cluster\"); plt.ylabel(\"lead_to_deal_days\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Medians bar (with counts)\n",
    "    med = dfd.groupby(DBSCAN_COL)[LEAD_NUM].median().sort_index()\n",
    "    cnt = dfd.groupby(DBSCAN_COL)[LEAD_NUM].count().reindex(med.index).fillna(0).astype(int)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.bar(med.index.astype(str), med.values)\n",
    "    for i,(m,c) in enumerate(zip(med.values, cnt.values)):\n",
    "        plt.text(i, m, f\"n={c}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    plt.title(\"DBSCAN — median lead_to_deal_days by cluster\")\n",
    "    plt.xlabel(\"cluster\"); plt.ylabel(\"median days\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    if not _exists(LEAD_NUM):\n",
    "        print(f\"ℹ️ '{LEAD_NUM}' not found; numeric lead plot skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d6ead8-3783-4cdf-9800-3546bea1661b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# DBSCAN clustering on sc_gold.features_table (safe + robust)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# === PARAMETERS ===\n",
    "SOURCE_TABLE = \"sc_gold.features_table\"\n",
    "RESULT_TABLE = \"sc_gold.features_table_dbscan\"   # output (won't alter the source)\n",
    "EPS = 0.5\n",
    "MIN_SAMPLES = 5\n",
    "# ===================\n",
    "\n",
    "# 1) Read source + helper id (does not modify source)\n",
    "sdf = spark.read.table(SOURCE_TABLE).withColumn(\"__row_id\", F.monotonically_increasing_id())\n",
    "sdf_original = sdf\n",
    "\n",
    "# 2) Drop columns only for modeling (temporary)\n",
    "cols_to_drop = [\n",
    "    \"concessao_contactos\",\n",
    "    \"campaign_name_campaigns\",\n",
    "    \"motivo_da_perda_deals\",\n",
    "    \"proposta_realizada_stage_propostas\",\n",
    "]\n",
    "existing_to_drop = [c for c in cols_to_drop if c in sdf.columns]\n",
    "sdf_model = sdf.drop(*existing_to_drop)\n",
    "\n",
    "# 3) TEMP null handling in Spark (in-memory only)\n",
    "#    - numeric -> median (cast to double to avoid Decimal issues)\n",
    "#    - categorical -> \"missing\"\n",
    "string_cols  = [c for c, t in sdf_model.dtypes if t == \"string\"]\n",
    "numeric_cols = [c for c, t in sdf_model.dtypes\n",
    "                if t in (\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\") or t.startswith(\"decimal\")]\n",
    "\n",
    "# Cast numerics to double so percentile_approx returns double\n",
    "for c in numeric_cols:\n",
    "    sdf_model = sdf_model.withColumn(c, F.col(c).cast(\"double\"))\n",
    "\n",
    "# Fill numeric with median\n",
    "if numeric_cols:\n",
    "    median_exprs = [F.percentile_approx(F.col(c), 0.5).alias(c) for c in numeric_cols]\n",
    "    median_vals  = sdf_model.select(*median_exprs).first().asDict()\n",
    "    medians      = {c: float(v) for c, v in median_vals.items() if v is not None}\n",
    "    if medians:\n",
    "        sdf_model = sdf_model.fillna(medians)\n",
    "\n",
    "# Fill categoricals with sentinel\n",
    "if string_cols:\n",
    "    sdf_model = sdf_model.fillna({c: \"missing\" for c in string_cols})\n",
    "\n",
    "# 4) Convert to pandas for scikit-learn\n",
    "pdf = sdf_model.toPandas()\n",
    "row_id = pdf[\"__row_id\"].copy()\n",
    "X_df = pdf.drop(columns=[\"__row_id\"])\n",
    "\n",
    "# 5) Build preprocessing (sklearn >= 1.4)\n",
    "num_cols = X_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    # with_mean=False so it works if final matrix is sparse\n",
    "    numeric_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    transformers.append((\"num\", numeric_pipe, num_cols))\n",
    "\n",
    "if cat_cols:\n",
    "    categorical_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ])\n",
    "    transformers.append((\"cat\", categorical_pipe, cat_cols))\n",
    "\n",
    "# If both lists are empty, raise a friendly error\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable features found after drops. Please keep at least one numeric or categorical column.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers)\n",
    "\n",
    "# 6) Transform and cluster\n",
    "X = preprocess.fit_transform(X_df)\n",
    "db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric=\"euclidean\")\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "# 7) Merge labels back to Spark (non-destructive) and save to a new table\n",
    "labels_pdf = pd.DataFrame({\"__row_id\": row_id, \"dbscan_label\": labels.astype(int)})\n",
    "labels_sdf = spark.createDataFrame(labels_pdf)\n",
    "\n",
    "result_sdf = (\n",
    "    sdf_original.join(labels_sdf, on=\"__row_id\", how=\"left\")\n",
    "                .drop(\"__row_id\")\n",
    ")\n",
    "\n",
    "(result_sdf.write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(RESULT_TABLE)\n",
    ")\n",
    "\n",
    "display(result_sdf)\n",
    "print(f\"✅ Output table: {RESULT_TABLE}\")\n",
    "print(f\"ℹ️ Dropped for modeling: {existing_to_drop}\")\n",
    "print(f\"DBSCAN parameters: eps={EPS}, min_samples={MIN_SAMPLES}\")\n",
    "print(\"🔒 Original table remains unchanged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a42cc5-803b-487b-b5bd-2d0b63e173ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.table(\"sc_gold.features_table_dbscan\")\n",
    "       .groupBy(\"dbscan_label\", \"lead_source\")\n",
    "       .count()\n",
    "       .orderBy(\"dbscan_label\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42bd02be-dea4-413c-a9ce-7beddaec3b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Cluster Profiling for DBSCAN results (PySpark)\n",
    "# -----------------------------------------------\n",
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "RESULT_TABLE = \"sc_gold.features_table_dbscan\"   # change if needed\n",
    "TOP_K = 3                                        # top K categories per cat column\n",
    "\n",
    "sdf = spark.table(RESULT_TABLE)\n",
    "\n",
    "# --- 0) Quick sanity check\n",
    "if \"dbscan_label\" not in sdf.columns:\n",
    "    raise ValueError(\"dbscan_label column not found. Make sure you ran the DBSCAN cell first.\")\n",
    "\n",
    "# --- 1) Cluster sizes + noise rate\n",
    "cluster_sizes = (\n",
    "    sdf.groupBy(\"dbscan_label\")\n",
    "       .count()\n",
    "       .orderBy(\"dbscan_label\")\n",
    ")\n",
    "\n",
    "total_n = sdf.count()\n",
    "noise_n = sdf.filter(F.col(\"dbscan_label\") == -1).count()\n",
    "noise_rate = (noise_n / total_n) if total_n > 0 else 0.0\n",
    "\n",
    "print(f\"Total rows: {total_n:,}\")\n",
    "print(f\"Noise rows (-1): {noise_n:,}  |  Noise rate: {noise_rate:.2%}\")\n",
    "display(cluster_sizes)\n",
    "\n",
    "# --- 2) Identify numeric vs categorical columns\n",
    "# treat booleans as categorical for readability\n",
    "num_types = {\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\",\"decimal\"}\n",
    "cat_types = {\"string\",\"boolean\"}\n",
    "\n",
    "numeric_cols = [c for c, t in sdf.dtypes if (t in num_types or t.startswith(\"decimal\")) and c != \"dbscan_label\"]\n",
    "categorical_cols = [c for c, t in sdf.dtypes if t in cat_types and c != \"dbscan_label\"]\n",
    "\n",
    "# --- 3) Numeric summary per cluster\n",
    "numeric_summary = None\n",
    "if numeric_cols:\n",
    "    agg_exprs = []\n",
    "    for c in numeric_cols:\n",
    "        agg_exprs.extend([\n",
    "            F.count(F.col(c)).alias(f\"{c}__count\"),\n",
    "            F.mean(F.col(c)).alias(f\"{c}__mean\"),\n",
    "            F.stddev_samp(F.col(c)).alias(f\"{c}__std\"),\n",
    "            F.min(F.col(c)).alias(f\"{c}__min\"),\n",
    "            F.expr(f\"percentile_approx({c}, 0.5)\").alias(f\"{c}__median\"),\n",
    "            F.max(F.col(c)).alias(f\"{c}__max\"),\n",
    "        ])\n",
    "    numeric_summary = sdf.groupBy(\"dbscan_label\").agg(*agg_exprs).orderBy(\"dbscan_label\")\n",
    "    display(numeric_summary)\n",
    "else:\n",
    "    print(\"No numeric columns detected for summary.\")\n",
    "\n",
    "# --- 4) Top-K categories per cluster for each categorical column\n",
    "def topk_modes_for_column(df, col_name, k=3):\n",
    "    # counts per (cluster, value)\n",
    "    counts = (df.groupBy(\"dbscan_label\", F.col(col_name).alias(\"value\"))\n",
    "                .count())\n",
    "    # total per cluster for percentages\n",
    "    totals = counts.groupBy(\"dbscan_label\").agg(F.sum(\"count\").alias(\"cluster_total\"))\n",
    "    joined = counts.join(totals, on=\"dbscan_label\", how=\"left\")\\\n",
    "                   .withColumn(\"pct\", F.col(\"count\")/F.col(\"cluster_total\"))\n",
    "    # rank within each cluster by count desc\n",
    "    w = Window.partitionBy(\"dbscan_label\").orderBy(F.col(\"count\").desc(), F.col(\"value\").asc())\n",
    "    ranked = joined.withColumn(\"rank\", F.row_number().over(w))\\\n",
    "                   .filter(F.col(\"rank\") <= k)\\\n",
    "                   .withColumn(\"column\", F.lit(col_name))\\\n",
    "                   .select(\"dbscan_label\", \"column\", \"value\", \"count\", F.round(\"pct\", 4).alias(\"pct\"))\n",
    "    return ranked\n",
    "\n",
    "cat_topk = None\n",
    "if categorical_cols:\n",
    "    frames = [topk_modes_for_column(sdf, c, TOP_K) for c in categorical_cols]\n",
    "    # union all\n",
    "    cat_topk = frames[0]\n",
    "    for f in frames[1:]:\n",
    "        cat_topk = cat_topk.unionByName(f)\n",
    "    cat_topk = cat_topk.orderBy(\"dbscan_label\", \"column\", F.desc(\"count\"))\n",
    "    display(cat_topk)\n",
    "else:\n",
    "    print(\"No categorical columns detected for top-K analysis.\")\n",
    "\n",
    "# --- 5) (Optional) Quick per-cluster breakdown for a few key fields\n",
    "# Replace these with fields you care about most.\n",
    "KEY_CATS = [c for c in [\"lead_source\", \"modelo_contactos\", \"deal_speed_category\"] if c in sdf.columns]\n",
    "for c in KEY_CATS:\n",
    "    print(f\"\\n=== Distribution of {c} by cluster ===\")\n",
    "    dist = (\n",
    "        sdf.groupBy(\"dbscan_label\", F.col(c).alias(c))\n",
    "           .count()\n",
    "           .withColumn(\"pct_in_cluster\",\n",
    "                       F.round(F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"dbscan_label\")), 4))\n",
    "           .orderBy(\"dbscan_label\", F.desc(\"count\"))\n",
    "    )\n",
    "    display(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e76992-4e21-490d-a186-fd5da1da0211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf = spark.read.table(\"sc_gold.features_table\")\n",
    "string_cols = [c for c, t in sdf.dtypes if t == \"string\"]\n",
    "\n",
    "# count unique values per string column\n",
    "cardinality = (\n",
    "    sdf.select([\n",
    "        F.countDistinct(F.col(c)).alias(c) for c in string_cols\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(cardinality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015bd842-48c0-4a1d-b83b-e30c9ac84a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show column names and data types\n",
    "df_info = spark.read.table(\"sc_gold.features_table\")\n",
    "display(df_info.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e71dfc1-aaea-4f1d-a9ca-802ffa4cd6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# DBSCAN on sc_gold.features_table  (updated drops + robust)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# === PARAMETERS ===\n",
    "SOURCE_TABLE = \"sc_gold.features_table\"\n",
    "RESULT_TABLE  = \"sc_gold.features_table_dbscan\"   # output (source remains untouched)\n",
    "EPS           = 0.5   # tune after elbow if needed\n",
    "MIN_SAMPLES   = 5\n",
    "# ===================\n",
    "\n",
    "# 1) Read + helper id\n",
    "sdf = spark.read.table(SOURCE_TABLE).withColumn(\"__row_id\", F.monotonically_increasing_id())\n",
    "sdf_original = sdf\n",
    "\n",
    "# 2) Drop columns ONLY for modeling (non-destructive)\n",
    "cols_to_drop = [\n",
    "    \"concessao_contactos\",                # high-card\n",
    "    \"proposta_realizada_stage_propostas\", # leakage\n",
    "    \"stage_deals\",                        # leakage\n",
    "    \"motivo_da_perda_deals\",              # leakage\n",
    "    \"campaign_name_campaigns\",            # high-card\n",
    "    \"versao_propostas\"                    # very high-card\n",
    "]\n",
    "existing_to_drop = [c for c in cols_to_drop if c in sdf.columns]\n",
    "sdf_model = sdf.drop(*existing_to_drop)\n",
    "\n",
    "# 3) TEMP null handling in Spark (in-memory only)\n",
    "#    - numeric -> median (cast to double to avoid Decimal issues)\n",
    "#    - categorical -> \"missing\"\n",
    "string_cols  = [c for c, t in sdf_model.dtypes if t == \"string\"]\n",
    "numeric_cols = [c for c, t in sdf_model.dtypes\n",
    "                if t in (\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\") or t.startswith(\"decimal\")]\n",
    "\n",
    "# Cast numerics to double so percentile_approx returns double\n",
    "for c in numeric_cols:\n",
    "    sdf_model = sdf_model.withColumn(c, F.col(c).cast(\"double\"))\n",
    "\n",
    "# Fill numeric with median\n",
    "if numeric_cols:\n",
    "    median_exprs = [F.percentile_approx(F.col(c), 0.5).alias(c) for c in numeric_cols]\n",
    "    median_vals  = sdf_model.select(*median_exprs).first().asDict()\n",
    "    medians      = {c: float(v) for c, v in median_vals.items() if v is not None}\n",
    "    if medians:\n",
    "        sdf_model = sdf_model.fillna(medians)\n",
    "\n",
    "# Fill categoricals with sentinel\n",
    "if string_cols:\n",
    "    sdf_model = sdf_model.fillna({c: \"missing\" for c in string_cols})\n",
    "\n",
    "# 4) To pandas for scikit-learn\n",
    "pdf   = sdf_model.toPandas()\n",
    "row_id = pdf[\"__row_id\"].copy()\n",
    "X_df   = pdf.drop(columns=[\"__row_id\"])\n",
    "\n",
    "# 5) Preprocessing (sklearn ≥ 1.4)\n",
    "num_cols = X_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    numeric_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\",  StandardScaler(with_mean=False))  # safe if final is sparse\n",
    "    ])\n",
    "    transformers.append((\"num\", numeric_pipe, num_cols))\n",
    "\n",
    "if cat_cols:\n",
    "    categorical_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ])\n",
    "    transformers.append((\"cat\", categorical_pipe, cat_cols))\n",
    "\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable features found after drops.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers)\n",
    "\n",
    "# 6) Transform + DBSCAN\n",
    "X  = preprocess.fit_transform(X_df)\n",
    "db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric=\"euclidean\")\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "# 7) Merge back + save as NEW table (source untouched)\n",
    "labels_pdf = pd.DataFrame({\"__row_id\": row_id, \"dbscan_label\": labels.astype(int)})\n",
    "labels_sdf = spark.createDataFrame(labels_pdf)\n",
    "\n",
    "result_sdf = (\n",
    "    sdf_original.join(labels_sdf, on=\"__row_id\", how=\"left\")\n",
    "                .drop(\"__row_id\")\n",
    ")\n",
    "\n",
    "(result_sdf.write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(RESULT_TABLE)\n",
    ")\n",
    "\n",
    "# Quick cluster counts\n",
    "display(result_sdf.groupBy(\"dbscan_label\").count().orderBy(\"dbscan_label\"))\n",
    "print(f\"✅ Output table: {RESULT_TABLE}\")\n",
    "print(f\"ℹ️ Dropped for modeling: {existing_to_drop}\")\n",
    "print(f\"DBSCAN params: eps={EPS}, min_samples={MIN_SAMPLES}\")\n",
    "print(\"🔒 Original table remains unchanged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71cec3de-93fc-40bc-b4f5-a1f97536a560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cluster profiling for tuned DBSCAN table\n",
    "# ========================================\n",
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "RESULT_TABLE = \"sc_gold.features_table_dbscan\"\n",
    "TOP_K = 3\n",
    "\n",
    "sdf = spark.table(RESULT_TABLE)\n",
    "\n",
    "# Cluster sizes + noise rate\n",
    "cluster_sizes = sdf.groupBy(\"dbscan_label\").count().orderBy(\"dbscan_label\")\n",
    "total_n = sdf.count()\n",
    "noise_n  = sdf.filter(F.col(\"dbscan_label\") == -1).count()\n",
    "print(f\"Total rows: {total_n:,}\")\n",
    "print(f\"Noise rows (-1): {noise_n:,}  |  Noise rate: {noise_n/total_n:.2%}\")\n",
    "display(cluster_sizes)\n",
    "\n",
    "# Identify types\n",
    "num_types = {\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\",\"decimal\"}\n",
    "cat_types = {\"string\",\"boolean\"}\n",
    "numeric_cols = [c for c, t in sdf.dtypes if (t in num_types or t.startswith(\"decimal\")) and c != \"dbscan_label\"]\n",
    "categorical_cols = [c for c, t in sdf.dtypes if t in cat_types and c != \"dbscan_label\"]\n",
    "\n",
    "# Numeric summary\n",
    "if numeric_cols:\n",
    "    agg_exprs = []\n",
    "    for c in numeric_cols:\n",
    "        agg_exprs += [\n",
    "            F.count(F.col(c)).alias(f\"{c}__count\"),\n",
    "            F.mean(F.col(c)).alias(f\"{c}__mean\"),\n",
    "            F.stddev_samp(F.col(c)).alias(f\"{c}__std\"),\n",
    "            F.min(F.col(c)).alias(f\"{c}__min\"),\n",
    "            F.expr(f\"percentile_approx({c}, 0.5)\").alias(f\"{c}__median\"),\n",
    "            F.max(F.col(c)).alias(f\"{c}__max\"),\n",
    "        ]\n",
    "    display(sdf.groupBy(\"dbscan_label\").agg(*agg_exprs).orderBy(\"dbscan_label\"))\n",
    "\n",
    "# Top-K categories per cluster\n",
    "def topk(df, col, k=3):\n",
    "    counts = df.groupBy(\"dbscan_label\", F.col(col).alias(\"value\")).count()\n",
    "    totals = counts.groupBy(\"dbscan_label\").agg(F.sum(\"count\").alias(\"tot\"))\n",
    "    joined = counts.join(totals, \"dbscan_label\").withColumn(\"pct\", F.col(\"count\")/F.col(\"tot\"))\n",
    "    w = Window.partitionBy(\"dbscan_label\").orderBy(F.desc(\"count\"), F.asc(\"value\"))\n",
    "    return (joined.withColumn(\"rank\", F.row_number().over(w))\n",
    "                  .filter(F.col(\"rank\") <= k)\n",
    "                  .withColumn(\"column\", F.lit(col))\n",
    "                  .select(\"dbscan_label\",\"column\",\"value\",\"count\",F.round(\"pct\",4).alias(\"pct\")))\n",
    "\n",
    "if categorical_cols:\n",
    "    frames = [topk(sdf, c, TOP_K) for c in categorical_cols]\n",
    "    out = frames[0]\n",
    "    for f in frames[1:]:\n",
    "        out = out.unionByName(f)\n",
    "    display(out.orderBy(\"dbscan_label\",\"column\",F.desc(\"count\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6a8e6c-afd0-4777-b937-b1e8eda012de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(\"sc_gold.features_table_dbscan\")\n",
    "\n",
    "# Noise rate\n",
    "total = df.count()\n",
    "noise = df.filter(F.col(\"dbscan_label\") == -1).count()\n",
    "print(f\"Noise rate: {noise/total:.2%} ({noise:,}/{total:,})\")\n",
    "\n",
    "# Cluster sizes\n",
    "display(df.groupBy(\"dbscan_label\").count().orderBy(F.desc(\"count\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96acd078-18d0-4f41-be89-e329897b663c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  df.groupBy(\"dbscan_label\", \"lead_source\")\n",
    "    .count()\n",
    "    .orderBy(\"dbscan_label\", F.desc(\"count\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41872ce7-0a90-4ac1-8193-1c21056e5815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FAST Grid search for DBSCAN eps — overwrites sc_gold.features_table_dbscan\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SOURCE_TABLE = \"sc_gold.features_table\"\n",
    "RESULT_TABLE = \"sc_gold.features_table_dbscan\"\n",
    "MIN_SAMPLES = 5\n",
    "EPS_GRID = [0.8, 1.0, 1.2, 1.5, 2.0]\n",
    "SAMPLE_ROWS = 15000          # use subset for eps testing\n",
    "COLS_TO_DROP = [\n",
    "    \"concessao_contactos\",\n",
    "    \"proposta_realizada_stage_propostas\",\n",
    "    \"stage_deals\",\n",
    "    \"motivo_da_perda_deals\",\n",
    "    \"campaign_name_campaigns\",\n",
    "    \"versao_propostas\",\n",
    "    \"modelo_contactos\",\n",
    "    \"origem_do_negocio_deals\",\n",
    "    \"combustivel_propostas\",\n",
    "    \"deal_speed_category\",\n",
    "    \"Season\",\n",
    "    \"is_month_end_deal\",\n",
    "    \"lead_year\",\n",
    "    \"lead_month\"\n",
    "]\n",
    "\n",
    "\n",
    "#| Column                  | Reason for drop                                                                                                                                                 |\n",
    "#| ----------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "#| **deal_speed_category** | It’s just a binned version of `lead_to_deal_days`. Keeping both adds noise and unnecessary dimensionality — DBSCAN “double counts” time-to-close information.   |\n",
    "#| **Season**              | Has only 4 unique values (Winter, Spring, etc.) and is *highly correlated* with `lead_month` and `lead_year`. Removing it avoids redundant distance dimensions. |\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "\n",
    "# === 1. Load and prepare sample ===\n",
    "sdf = spark.read.table(SOURCE_TABLE).withColumn(\"__row_id\", F.monotonically_increasing_id())\n",
    "sdf = sdf.drop(*[c for c in COLS_TO_DROP if c in sdf.columns])\n",
    "\n",
    "# quick fill\n",
    "string_cols  = [c for c, t in sdf.dtypes if t == \"string\"]\n",
    "numeric_cols = [c for c, t in sdf.dtypes if t in (\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\") or t.startswith(\"decimal\")]\n",
    "for c in numeric_cols:\n",
    "    sdf = sdf.withColumn(c, F.col(c).cast(\"double\"))\n",
    "\n",
    "if numeric_cols:\n",
    "    med_exprs = [F.percentile_approx(F.col(c), 0.5).alias(c) for c in numeric_cols]\n",
    "    med_vals  = sdf.select(*med_exprs).first().asDict()\n",
    "    medians   = {c: float(v) for c, v in med_vals.items() if v is not None}\n",
    "    sdf = sdf.fillna(medians)\n",
    "if string_cols:\n",
    "    sdf = sdf.fillna({c: \"missing\" for c in string_cols})\n",
    "\n",
    "# take a sample\n",
    "sdf_sample = sdf.sample(withReplacement=False, fraction=min(SAMPLE_ROWS / sdf.count(), 1.0), seed=42)\n",
    "pdf = sdf_sample.toPandas()\n",
    "row_id = pdf[\"__row_id\"].copy()\n",
    "X_df = pdf.drop(columns=[\"__row_id\"])\n",
    "\n",
    "# === 2. Preprocess ===\n",
    "num_cols = X_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\",  StandardScaler(with_mean=False))\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ]), cat_cols))\n",
    "preprocess = ColumnTransformer(transformers)\n",
    "X = preprocess.fit_transform(X_df)\n",
    "n_rows = X.shape[0]\n",
    "\n",
    "# === 3. Try several eps ===\n",
    "summary = []\n",
    "labels_cache = {}\n",
    "\n",
    "for eps in EPS_GRID:\n",
    "    t0 = perf_counter()\n",
    "    db = DBSCAN(eps=float(eps), min_samples=MIN_SAMPLES, metric=\"euclidean\")\n",
    "    labels = db.fit_predict(X)\n",
    "    dt = perf_counter() - t0\n",
    "\n",
    "    labels_np = np.asarray(labels)\n",
    "    noise_n = int((labels_np == -1).sum())\n",
    "    noise_rate = noise_n / n_rows\n",
    "    n_clusters = len(set(labels_np)) - (1 if -1 in labels_np else 0)\n",
    "    summary.append({\n",
    "        \"eps\": float(eps),\n",
    "        \"noise_rate\": round(noise_rate, 4),\n",
    "        \"n_clusters\": int(n_clusters),\n",
    "        \"seconds\": round(dt, 2)\n",
    "    })\n",
    "    labels_cache[float(eps)] = labels_np\n",
    "\n",
    "summary_pdf = pd.DataFrame(summary).sort_values(\"eps\")\n",
    "display(summary_pdf)\n",
    "\n",
    "# pick best eps by lowest noise deviation (~0.3 target)\n",
    "summary_pdf[\"score\"] = abs(summary_pdf[\"noise_rate\"] - 0.3)\n",
    "best_eps = summary_pdf.loc[summary_pdf[\"score\"].idxmin(), \"eps\"]\n",
    "print(f\"✅ Suggested eps = {best_eps} (based on sample results)\")\n",
    "\n",
    "# === 4. Run DBSCAN once on full dataset with best eps ===\n",
    "full_pdf = sdf.toPandas()\n",
    "row_id_full = full_pdf[\"__row_id\"].copy()\n",
    "X_df_full = full_pdf.drop(columns=[\"__row_id\"])\n",
    "X_full = preprocess.transform(X_df_full)\n",
    "\n",
    "db_best = DBSCAN(eps=float(best_eps), min_samples=MIN_SAMPLES, metric=\"euclidean\")\n",
    "labels_full = db_best.fit_predict(X_full)\n",
    "\n",
    "# === 5. Save back to table ===\n",
    "labels_pdf = pd.DataFrame({\"__row_id\": row_id_full, \"dbscan_label\": labels_full.astype(int)})\n",
    "labels_sdf = spark.createDataFrame(labels_pdf)\n",
    "result_sdf = sdf.join(labels_sdf, on=\"__row_id\", how=\"left\").drop(\"__row_id\")\n",
    "\n",
    "(result_sdf.write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(RESULT_TABLE))\n",
    "\n",
    "print(f\"📁 Overwritten table: {RESULT_TABLE}\")\n",
    "print(f\"DBSCAN params: eps={best_eps}, min_samples={MIN_SAMPLES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0225b2d-b941-4b47-a462-2f899491ad45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# DBSCAN (eps=2.0) with memory-safe pipeline (SVD + float32)\n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# ===== Params =====\n",
    "SOURCE_TABLE = \"sc_gold.features_table\"\n",
    "RESULT_TABLE = \"sc_gold.features_table_dbscan\"   # overwritten\n",
    "EPS = 2.0\n",
    "MIN_SAMPLES = 5\n",
    "SVD_COMPONENTS = 50          # dimensionality reducer (strongly recommended)\n",
    "MAX_ROWS = None              # e.g., 50000 to sample for testing; None = use all\n",
    "# ==================\n",
    "\n",
    "# Columns to drop only for modeling\n",
    "COLS_TO_DROP = [\n",
    "    \"concessao_contactos\",\"proposta_realizada_stage_propostas\",\"stage_deals\",\n",
    "    \"motivo_da_perda_deals\",\"campaign_name_campaigns\",\"versao_propostas\",\n",
    "    \"modelo_contactos\",\"origem_do_negocio_deals\",\"combustivel_propostas\",\n",
    "    \"deal_speed_category\",\"Season\",\"model_in_campaign\",\"is_month_end_deal\",\n",
    "    \"lead_year\",\"lead_month\",\n",
    "]\n",
    "\n",
    "# 1) Read + helper id\n",
    "sdf = spark.read.table(SOURCE_TABLE).withColumn(\"__row_id\", F.monotonically_increasing_id())\n",
    "sdf_original = sdf\n",
    "\n",
    "# 2) Drop modeling-only columns\n",
    "sdf_model = sdf.drop(*[c for c in COLS_TO_DROP if c in sdf.columns])\n",
    "\n",
    "# 3) TEMP null handling (Spark-side; not persisted)\n",
    "string_cols  = [c for c, t in sdf_model.dtypes if t == \"string\"]\n",
    "numeric_cols = [c for c, t in sdf_model.dtypes if t in (\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\") or t.startswith(\"decimal\")]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    sdf_model = sdf_model.withColumn(c, F.col(c).cast(\"double\"))\n",
    "\n",
    "if numeric_cols:\n",
    "    med_exprs = [F.percentile_approx(F.col(c), 0.5).alias(c) for c in numeric_cols]\n",
    "    med_vals  = sdf_model.select(*med_exprs).first().asDict()\n",
    "    medians   = {c: float(v) for c, v in med_vals.items() if v is not None}\n",
    "    if medians:\n",
    "        sdf_model = sdf_model.fillna(medians)\n",
    "\n",
    "if string_cols:\n",
    "    sdf_model = sdf_model.fillna({c: \"missing\" for c in string_cols})\n",
    "\n",
    "# 4) Optional: sample rows to keep memory in check during development\n",
    "if MAX_ROWS:\n",
    "    frac = min(MAX_ROWS / sdf_model.count(), 1.0)\n",
    "    sdf_model = sdf_model.sample(False, frac, seed=42)\n",
    "\n",
    "# 5) To pandas for scikit-learn\n",
    "pdf = sdf_model.toPandas()\n",
    "row_id = pdf[\"__row_id\"].to_numpy()\n",
    "X_df  = pdf.drop(columns=[\"__row_id\"])\n",
    "del pdf; gc.collect()\n",
    "\n",
    "# 6) Preprocess -> OneHot (sparse) + SVD (dense low-dim) + float32\n",
    "num_cols = X_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\",  StandardScaler(with_mean=False))\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ]), cat_cols))\n",
    "\n",
    "preprocess = ColumnTransformer(transformers)\n",
    "X_sparse = preprocess.fit_transform(X_df)\n",
    "del X_df; gc.collect()\n",
    "\n",
    "# Dimensionality reduction (recommended for memory + speed)\n",
    "svd = TruncatedSVD(n_components=min(SVD_COMPONENTS, X_sparse.shape[1]-1), random_state=42)\n",
    "X = svd.fit_transform(X_sparse).astype(np.float32, copy=False)\n",
    "del X_sparse; gc.collect()\n",
    "\n",
    "# 7) DBSCAN\n",
    "db = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric=\"euclidean\", n_jobs=-1)\n",
    "labels = db.fit_predict(X).astype(int)\n",
    "del X; gc.collect()\n",
    "\n",
    "# 8) Merge back + save\n",
    "labels_pdf = pd.DataFrame({\"__row_id\": row_id, \"dbscan_label\": labels})\n",
    "labels_sdf = spark.createDataFrame(labels_pdf)\n",
    "\n",
    "result_sdf = (\n",
    "    sdf_original.join(labels_sdf, on=\"__row_id\", how=\"left\")\n",
    "                .drop(\"__row_id\")\n",
    ")\n",
    "\n",
    "(result_sdf.write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(RESULT_TABLE)\n",
    ")\n",
    "\n",
    "display(result_sdf.groupBy(\"dbscan_label\").count().orderBy(\"dbscan_label\"))\n",
    "print(f\"📁 Overwritten table: {RESULT_TABLE}\")\n",
    "print(f\"DBSCAN params: eps={EPS}, min_samples={MIN_SAMPLES}, svd_components={SVD_COMPONENTS}, dtype=float32\")\n",
    "print(\"🔒 Original table remains unchanged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09da6e9a-31b9-4c28-a24d-736b5e221713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TABLE = \"sc_gold.features_table_dbscan\"\n",
    "\n",
    "# load\n",
    "sdf = spark.table(TABLE)\n",
    "\n",
    "# handle null labels as noise\n",
    "sdf = sdf.withColumn(\"dbscan_label\",\n",
    "                     F.when(F.col(\"dbscan_label\").isNull(), F.lit(-1)).otherwise(F.col(\"dbscan_label\")))\n",
    "\n",
    "# basic info\n",
    "n_total = sdf.count()\n",
    "n_noise = sdf.filter(F.col(\"dbscan_label\") == -1).count()\n",
    "n_clusters = sdf.select(\"dbscan_label\").distinct().count() - (1 if n_noise > 0 else 0)\n",
    "\n",
    "print(f\"Total rows: {n_total:,}\")\n",
    "print(f\"Noise rows (-1): {n_noise:,}  ({n_noise/n_total:.2%})\")\n",
    "print(f\"Clusters found: {n_clusters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a2dd82-ba23-471b-b041-65a83200715d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [c for c, t in sdf.dtypes if t in (\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\") or t.startswith(\"decimal\")]\n",
    "\n",
    "agg_exprs = [F.count(\"*\").alias(\"n\")]\n",
    "for c in numeric_cols:\n",
    "    agg_exprs += [\n",
    "        F.mean(F.col(c)).alias(f\"{c}_mean\"),\n",
    "        F.stddev(F.col(c)).alias(f\"{c}_std\"),\n",
    "        F.min(F.col(c)).alias(f\"{c}_min\"),\n",
    "        F.max(F.col(c)).alias(f\"{c}_max\"),\n",
    "    ]\n",
    "\n",
    "num_profile = (\n",
    "    sdf.groupBy(\"dbscan_label\")\n",
    "       .agg(*agg_exprs)\n",
    "       .orderBy(\"dbscan_label\")\n",
    ")\n",
    "display(num_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b507057f-9aa8-4e9b-8f4d-c3f6dcf9c030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "cluster_sizes = (\n",
    "    sdf.groupBy(\"dbscan_label\")\n",
    "       .count()\n",
    "       .orderBy(\"dbscan_label\")\n",
    "       .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(cluster_sizes[\"dbscan_label\"], cluster_sizes[\"count\"], color=\"skyblue\")\n",
    "plt.xlabel(\"DBSCAN Label\")\n",
    "plt.ylabel(\"Row Count\")\n",
    "plt.title(\"Cluster Sizes (Including Noise)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1deea3-0bb6-4221-95df-596483e5050b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = sdf.toPandas()\n",
    "num_cols = [c for c in pdf.columns if pdf[c].dtype in (\"float64\", \"int64\") and c not in [\"dbscan_label\"]]\n",
    "cluster_summary = pdf.groupby(\"dbscan_label\")[num_cols].mean()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(cluster_summary, cmap=\"viridis\", annot=False)\n",
    "plt.title(\"Cluster Mean Feature Heatmap\")\n",
    "plt.xlabel(\"Numeric Features\")\n",
    "plt.ylabel(\"DBSCAN Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f400de55-2f14-45d1-9c2c-0c1cbdbb91eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pdf_sample = pdf.sample(n=min(30000, len(pdf)), random_state=42)\n",
    "num_cols = [c for c in pdf_sample.columns if np.issubdtype(pdf_sample[c].dtype, np.number) and c != \"dbscan_label\"]\n",
    "\n",
    "X = pdf_sample[num_cols].fillna(0)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "coords = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(coords[:,0], coords[:,1],\n",
    "            c=pdf_sample[\"dbscan_label\"],\n",
    "            cmap=\"tab10\",\n",
    "            s=10, alpha=0.6)\n",
    "plt.title(\"2D PCA Cluster Visualization (sampled)\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124c46e5-a305-4374-bb11-9363dcb16ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    sdf.filter(F.col(\"dbscan_label\") == -1)\n",
    "       .groupBy(\"lead_source\")\n",
    "       .count()\n",
    "       .orderBy(F.desc(\"count\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526e072a-c584-47e6-8a97-96464fa7dcc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all columns that were dropped for modeling\n",
    "COLS_TO_DROP = [\n",
    "    \"concessao_contactos\", \"proposta_realizada_stage_propostas\", \"stage_deals\",\n",
    "    \"motivo_da_perda_deals\", \"campaign_name_campaigns\", \"versao_propostas\",\n",
    "    \"modelo_contactos\", \"origem_do_negocio_deals\", \"combustivel_propostas\",\n",
    "    \"deal_speed_category\", \"Season\", \"model_in_campaign\", \"is_month_end_deal\",\n",
    "    \"lead_year\", \"lead_month\"\n",
    "]\n",
    "\n",
    "# read the DBSCAN table\n",
    "sdf = spark.table(\"sc_gold.features_table_dbscan\")\n",
    "\n",
    "# keep only used features + dbscan label\n",
    "used_features = [c for c in sdf.columns if c not in COLS_TO_DROP + [\"__row_id\"]]\n",
    "sdf = sdf.select(used_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cdfa20-dc2d-4f8a-8bd3-c0351e8d8663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [c for c, t in sdf.dtypes if t in (\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\") or t.startswith(\"decimal\")]\n",
    "string_cols  = [c for c, t in sdf.dtypes if t == \"string\"]\n",
    "\n",
    "print(\"Numeric used:\", numeric_cols)\n",
    "print(\"Categorical used:\", string_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900aea43-4cd7-45da-b40c-d2455f0157a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Basic metrics\n",
    "total = sdf.count()\n",
    "noise = sdf.filter(F.col(\"dbscan_label\") == -1).count()\n",
    "clusters = sdf.select(\"dbscan_label\").distinct().count() - (1 if noise > 0 else 0)\n",
    "\n",
    "print(f\"📊 Total rows: {total:,}\")\n",
    "print(f\"🌀 Clusters found: {clusters}\")\n",
    "print(f\"⚠️ Noise rows (-1): {noise:,} ({noise/total:.2%})\")\n",
    "\n",
    "# Cluster size table\n",
    "display(\n",
    "    sdf.groupBy(\"dbscan_label\")\n",
    "       .count()\n",
    "       .orderBy(\"dbscan_label\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d573eeaa-560d-4d8c-a7a3-6e84a9198352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [c for c, t in sdf.dtypes if t in (\"double\",\"float\",\"int\",\"bigint\",\"smallint\",\"tinyint\") or t.startswith(\"decimal\")]\n",
    "\n",
    "agg_exprs = [F.count(\"*\").alias(\"n\")]\n",
    "for c in numeric_cols:\n",
    "    agg_exprs += [\n",
    "        F.mean(F.col(c)).alias(f\"{c}_mean\"),\n",
    "        F.stddev(F.col(c)).alias(f\"{c}_std\"),\n",
    "        F.min(F.col(c)).alias(f\"{c}_min\"),\n",
    "        F.max(F.col(c)).alias(f\"{c}_max\"),\n",
    "    ]\n",
    "\n",
    "num_profile = (\n",
    "    sdf.groupBy(\"dbscan_label\")\n",
    "       .agg(*agg_exprs)\n",
    "       .orderBy(\"dbscan_label\")\n",
    ")\n",
    "display(num_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa9e0df-4d5c-4987-9a25-f6339e5639e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "string_cols = [c for c, t in sdf.dtypes if t == \"string\"]\n",
    "\n",
    "for c in string_cols:\n",
    "    print(f\"\\n📊 Top categories for '{c}' by cluster:\")\n",
    "    top = (\n",
    "        sdf.groupBy(\"dbscan_label\", c)\n",
    "           .count()\n",
    "           .withColumn(\"pct\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"dbscan_label\")))\n",
    "           .orderBy(\"dbscan_label\", F.desc(\"count\"))\n",
    "    )\n",
    "    display(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9922c8dd-54ff-43e2-ace2-cfe9dcf34f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cluster_sizes = (\n",
    "    sdf.groupBy(\"dbscan_label\")\n",
    "       .count()\n",
    "       .orderBy(\"dbscan_label\")\n",
    "       .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(cluster_sizes[\"dbscan_label\"], cluster_sizes[\"count\"], color=\"cornflowerblue\")\n",
    "plt.xlabel(\"DBSCAN Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Cluster Sizes (Including Noise)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40abfcd2-da89-4817-9725-cd2cf2c04db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = sdf.toPandas()\n",
    "num_cols = [c for c in pdf.columns if pdf[c].dtype in (\"float64\",\"int64\") and c not in [\"dbscan_label\"]]\n",
    "\n",
    "cluster_summary = pdf.groupby(\"dbscan_label\")[num_cols].mean()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(cluster_summary, cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Cluster Mean Feature Heatmap\")\n",
    "plt.xlabel(\"Numeric Features\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ded521-1f03-43e0-8695-53918d160b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pdf_sample = pdf.sample(n=min(30000, len(pdf)), random_state=42)\n",
    "num_cols = [c for c in pdf_sample.columns if np.issubdtype(pdf_sample[c].dtype, np.number) and c != \"dbscan_label\"]\n",
    "\n",
    "X = pdf_sample[num_cols].fillna(0)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "coords = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(coords[:,0], coords[:,1],\n",
    "            c=pdf_sample[\"dbscan_label\"],\n",
    "            cmap=\"tab10\",\n",
    "            s=10, alpha=0.6)\n",
    "plt.title(\"2D PCA Projection of DBSCAN Clusters\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8270333655790145,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Gold_Models_Pedro_Base",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
