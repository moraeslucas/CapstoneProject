{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78526232-033b-489e-957c-ba5ec3a4255d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn \n",
    "%pip install pandas \n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3b4d03-505c-49c1-bfd9-795210198da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, StructType, StructField, LongType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from databricks.vector_search.client import VectorSearchClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f257c03-9109-4758-9af6-e7e982757490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#'calculate_dunn_index_optimized' measures how compact, well-separated clusters are\n",
    "def calculate_dunn_index(embeddings, cluster_labels):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    clusters = [embeddings[cluster_labels == k] for k in unique_clusters]\n",
    "    # Calculate inter-cluster distances (minimum between clusters)\n",
    "    inter_cluster = np.min([\n",
    "        np.min(cdist(clusters[i], clusters[j]))\n",
    "        for i in range(len(clusters)) for j in range(len(clusters)) if i != j\n",
    "    ])\n",
    "    # Calculate intra-cluster distances (maximum within clusters)\n",
    "    intra_cluster = np.max([\n",
    "        np.max(cdist(cluster, cluster)) if len(cluster) > 1 else 0\n",
    "        for cluster in clusters\n",
    "    ])\n",
    "    # Avoid division by zero\n",
    "    if intra_cluster == 0:\n",
    "        return 0\n",
    "    return inter_cluster / intra_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8772b94b-7873-40d7-bb2b-b1abb45b1195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\"\"\"Perform K-means clustering with optimal cluster selection\"\"\"\n",
    "def perform_clustering(embeddings, n_clusters_range=range(2, 10)):\n",
    "    best_davies_bouldin = -1\n",
    "    davies_bouldin_scores = []\n",
    "    best_k = 1\n",
    "    \n",
    "    # Find optimal nº of clusters using multiple metrics\n",
    "    for k in n_clusters_range:\n",
    "        # 'n_init=10' runs 10 times with different centroids¹ and picks the best result\n",
    "        # ¹They're the algorithm’s guess at where clusters might be\n",
    "        kmeans = KMeans(n_clusters=k, \n",
    "                        random_state=42, \n",
    "                        n_init=10)\n",
    "        #Assigns each embeddings data point to a cluster\n",
    "        cluster_labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        #'davies_bouldin_score' measures cluster separation (with lower numbers being better)\n",
    "        davies_bouldin = davies_bouldin_score(embeddings, cluster_labels)\n",
    "        davies_bouldin_scores.append(davies_bouldin)\n",
    "        \n",
    "        # The Davies–Bouldin Index is generally considered more comprehensive than the Dunn Index (calculated in another cell)\n",
    "        if davies_bouldin > best_davies_bouldin:\n",
    "            best_davies_bouldin = davies_bouldin\n",
    "            best_k = k\n",
    "\n",
    "    if best_k != 9:\n",
    "        # Final clustering with best k\n",
    "        final_kmeans = KMeans(n_clusters=best_k, \n",
    "                              random_state=42, \n",
    "                              n_init=10)\n",
    "        final_labels = final_kmeans.fit_predict(embeddings)\n",
    "    else:\n",
    "        final_kmeans = kmeans\n",
    "        final_labels = cluster_labels\n",
    "    \n",
    "    return best_k, final_labels, best_davies_bouldin, davies_bouldin_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ada40c-be75-4afd-b9cb-b7a33ad5ba9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Analyze cluster characteristics\"\"\"\n",
    "def analyze_clusters(df, cluster_labels, descriptions):\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered['labels'] = cluster_labels\n",
    "    df_clustered['descriptions'] = descriptions\n",
    "    \n",
    "    cluster_analysis = {}\n",
    "    \n",
    "    # Analyze cluster distributions\n",
    "    #'set' gets all distinct cluster labels, while 'range' lets you iterate over them\n",
    "    for cluster_label in range(len(set(cluster_labels))):\n",
    "        cluster_data = df_clustered[df_clustered['labels'] == cluster_label]\n",
    "        \n",
    "        analysis = {\n",
    "            'size': len(cluster_data),                                  # Nº of rows in the cluster\n",
    "            'percentage': len(cluster_data) / len(df_clustered) * 100,  # Share of this cluster relative to the whole dataset\n",
    "            'top_origem': cluster_data['origem'].value_counts()\n",
    "                                                .head(3) # Top 3 most frequent values in the origem column \n",
    "                                                .to_dict(),\n",
    "            'top_formulario': cluster_data['formulario'].value_counts()\n",
    "                                                        .head(3)\n",
    "                                                         .to_dict(),\n",
    "            'top_tipo_de_pedido': cluster_data['tipo_de_pedido'].value_counts()\n",
    "                                                                .head(3)\n",
    "                                                                .to_dict(),\n",
    "            'top_modelo': cluster_data['modelo'].value_counts().head(3) # First 3 most frequent values in the modelo column\n",
    "                                                               .to_dict(),\n",
    "            'top_agrupamento': cluster_data['agrupamento_cliente'].value_counts()\n",
    "                                                                  .head(3)\n",
    "                                                                  .to_dict(),\n",
    "            'top_caracterizacao': cluster_data['caracterizacao'].value_counts()\n",
    "                                                                .head(3)\n",
    "                                                                .to_dict(),\n",
    "            'sample_descriptions': cluster_data['descriptions'].head(3) #First 3 Contacto descriptions from this cluster\n",
    "                                                               .tolist()  \n",
    "        }\n",
    "\n",
    "        cluster_analysis[f'Cluster_{cluster_label}'] = analysis\n",
    "    \n",
    "    return df_clustered, cluster_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821e3cdd-05fe-4eff-90a2-81a0e1ec8e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Create visualizations for cluster analysis\"\"\"\n",
    "def visualize_clusters(embeddings, cluster_labels, best_k):    \n",
    "    # Create mapping for cluster names\n",
    "    cluster_name_map = {0: 'Site Hyundai Portugal', 0.0: 'Site Hyundai Portugal', \n",
    "                        1: 'Showroom', 1.0: 'Showroom',\n",
    "                        2: 'Facebook',  2.0: 'Facebook'}\n",
    "    \n",
    "    #Principal Component Analysis (PCA) reduces high-dimensional data down to 2D, so it can be easier to explore it visually.\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Define discrete colors for each cluster\n",
    "    colors = ['#1f77b4', '#8c564b', '#17becf']  # blue, brown, cyan\n",
    "\n",
    "    # PCA plot\n",
    "    scatter = axes[0].scatter(embeddings_2d[:, 0], \n",
    "                              embeddings_2d[:, 1], \n",
    "                              c=cluster_labels, \n",
    "                              cmap='tab10', \n",
    "                              alpha=0.6)\n",
    "    axes[0].set_title(f'Contacto Clusters - PCA Visualization')\n",
    "    \n",
    "    # Create colorbar with discrete labels\n",
    "    cbar = plt.colorbar(scatter, ax=axes[0], ticks=[0, 1, 2])\n",
    "    cbar.set_ticklabels(['Site Hyundai', 'Showroom', 'Facebook'])\n",
    "\n",
    "    # Cluster distribution\n",
    "    unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    # Map labels to names for x-axis\n",
    "    label_names = [cluster_name_map.get(label, label) for label in unique_labels]\n",
    "\n",
    "    # Use label_names for x-axis and range for x positions\n",
    "    axes[1].bar(range(len(label_names)), counts, color=colors[:len(label_names)])\n",
    "    axes[1].set_xticks(range(len(label_names)))\n",
    "    axes[1].set_xticklabels(label_names)\n",
    "\n",
    "    axes[1].set_title('Cluster Distribution')\n",
    "    axes[1].set_xlabel('Cluster ID')\n",
    "    axes[1].set_ylabel('Number of Contactos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be4d210-7ebb-40bb-b6d7-cee73d753fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Create visualizations for cluster analysis, excluding the noise (i.e., outlier cluster) if needed\"\"\"\n",
    "def visualize_clusters_optimized(embeddings, cluster_labels, best_k):\n",
    "    unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    # Identify the outliers\n",
    "    smallest_indices = np.argsort(counts)[:2]  # Get indices of 2 smallest clusters\n",
    "    smallest_cluster_ids = unique_labels[smallest_indices]\n",
    "    # smallest_cluster_id = unique_labels[np.argmin(counts)]\n",
    "    \n",
    "    # Filter out this outlier\n",
    "    mask = ~np.isin(cluster_labels, smallest_cluster_ids)\n",
    "    # mask = cluster_labels != smallest_cluster_id\n",
    "    # Applies the mask to the embeddings array, meaning it keeps the embeddings that don’t belong to the outlier cluster.\n",
    "    filtered_embeddings = embeddings[mask]\n",
    "    filtered_labels = cluster_labels[mask]\n",
    "    \n",
    "    # Update remaining cluster labels to be contiguous (0, 1, 2, ...)\n",
    "    label_mapping = {old_label: new_label for new_label, old_label in \n",
    "                    enumerate(np.unique(filtered_labels))}\n",
    "    mapped_labels = np.array([label_mapping[label] for label in filtered_labels])\n",
    "    \n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca.fit_transform(filtered_embeddings)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # PCA plot\n",
    "    scatter = axes[0].scatter(embeddings_2d[:, 0], \n",
    "                              embeddings_2d[:, 1], \n",
    "                              c=mapped_labels, \n",
    "                              cmap='tab10', \n",
    "                              alpha=0.6)\n",
    "    axes[0].set_title(f'Contacto - Clusters (PCA) - {best_k-3} clusters')\n",
    "    plt.colorbar(scatter, ax=axes[0])\n",
    "    \n",
    "    # Cluster distribution\n",
    "    filtered_unique_labels, filtered_counts = np.unique(mapped_labels, return_counts=True)\n",
    "    axes[1].bar(filtered_unique_labels, filtered_counts)\n",
    "    axes[1].set_title('Cluster Distribution')\n",
    "    axes[1].set_xlabel('Cluster ID')\n",
    "    axes[1].set_ylabel('Number of Contactos')\n",
    "       \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1430d653-06de-44c3-bd9f-f9e223e4439e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Load contactos data to create behavioral descriptions later on\"\"\"\n",
    "def load_and_preprocess_gold_contactos():\n",
    "    df = spark.table(\"workspace.sc_gold.contactos_pbs\")\n",
    "    # Convert to Pandas for easier text processing\n",
    "    df_pd = df.toPandas()\n",
    "    \n",
    "    return df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b77775-fb67-4412-a018-14f7c3dc7c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Transform contactos data into natural language descriptions\"\"\"\n",
    "def create_contactos_descriptions(df):\n",
    "    descriptions = []\n",
    "    \n",
    "    # 'df.iterrows()' to iterate over DataFrame rows as (index, row) pairs, with this index being ignored as '_'\n",
    "    for _, row in df.iterrows():\n",
    "        # Our key behavioral features\n",
    "        origem        = row['origem']         if pd.notna(row['origem']) \\\n",
    "                                              else 'não especificado'\n",
    "        formulario    = row['formulario']     if pd.notna(row['formulario']) \\\n",
    "                                              else 'não especificado'\n",
    "        tipo_de_pedido= row['tipo_de_pedido'] if pd.notna(row['tipo_de_pedido']) \\\n",
    "                                              else 'não especificado'\n",
    "        modelo        = row['modelo']         if pd.notna(row['modelo']) \\\n",
    "                                              else 'não especificado'\n",
    "        consentimento = row['consentimento']  if pd.notna(row['consentimento']) \\\n",
    "                                              else 'não especificado'\n",
    "        email_opt_out = row['email_opt_out']  if pd.notna(row['email_opt_out']) \\\n",
    "                                              else 'não especificado'\n",
    "        agrupamento   = row['agrupamento_cliente'] if pd.notna(row['agrupamento_cliente'])\\\n",
    "                                                   else 'não especificado'\n",
    "        caracterizacao= row['caracterizacao'] if pd.notna(row['caracterizacao']) \\\n",
    "                                              else 'não especificado'\n",
    "        \n",
    "        # And then we create a natural language description in Portuguese\n",
    "        description = f\"Origem do contacto: {origem}, através do formulário {formulario}. \" \\\n",
    "                      f\"Tipo de Pedido: {tipo_de_pedido}, solicitado para um modelo {modelo}. \" \\\n",
    "                      f\"Agrupado em: {agrupamento}, e caracterizado como {caracterizacao}. \" \\\n",
    "                      f\"Status do consentimento: {consentimento}, e a opção para receber email está em {email_opt_out}.\"\n",
    "\n",
    "        descriptions.append(description)\n",
    "    \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c6804f-3c6e-48c5-ae98-7b7f347f3c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Start by loading the table\n",
    "df = spark.table(\"workspace.sc_gold.contactos_pbs_embeddings_new\")\n",
    "embeddings_df = df.select(\"embedding\")\n",
    "# Then, convert to a list of lists (i.e., each row is one embedding vector)\n",
    "embeddings_list = embeddings_df.toPandas()[\"embedding\"].tolist()\n",
    "# Lastly, convert to NumPy array\n",
    "embeddings = np.array(embeddings_list, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55d279c7-a1b2-4c68-8a67-795f4e101bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Davies-Bouldin index score reference:\n",
    "≈ 0.0 - 0.5  = Excellent clustering\n",
    "≈ 0.5 - 1.0\t = Good\n",
    "≈ 1.0 - 2.0\t = Moderate\n",
    "> 2.0 - Poor = clustering\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793bd226-ddb1-487f-bdcd-76751ace67b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Performing clustering...\")\n",
    "best_k, cluster_labels, best_davies_bouldin, davies_bouldin_scores = perform_clustering(embeddings)\n",
    "print(f\"Optimal number of clusters: {best_k}\")\n",
    "\n",
    "print(f\"\\nBest Davies-Bouldin: {best_davies_bouldin:.1f}\")\n",
    "print(f\"Best Davies-Bouldin scores for different clusters/Ks: {davies_bouldin_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe2dee6-2577-4363-b1c3-d3f80a2c2542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dunn Index score reference:\n",
    "0.5 - 1.0 = Good Clustering\n",
    "\n",
    "Observations:\n",
    "•Distinct cluster separation with reasonable compactness.\n",
    "•This is often the practical \"good\" range for real applications.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9adfe769-52e6-4cc0-ad87-4f3bc0d8c827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#'calculate_dunn_index' measures how compact, well-separated clusters are\n",
    "dunn = calculate_dunn_index(embeddings, cluster_labels)\n",
    "print(f\"\\nBest Dunn Index: {dunn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a028aa92-fe1a-447d-acf5-d8e2c1526019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading contactos data...\")\n",
    "contacto_df = load_and_preprocess_gold_contactos()\n",
    "\n",
    "print(\"Creating contactos behavioral descriptions...\")\n",
    "contacto_descriptions = create_contactos_descriptions(contacto_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6025248-60eb-46b6-a8a8-4333a65c7f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Analyzing 3 clusters...\")\n",
    "clustered_df, cluster_analysis = analyze_clusters(contacto_df, cluster_labels, contacto_descriptions)\n",
    "\n",
    "# Print cluster analysis\n",
    "for cluster_name, analysis in cluster_analysis.items():\n",
    "    print(f\"\\n{cluster_name}:\")\n",
    "    print(f\"  Size: {analysis['size']} ({analysis['percentage']:.1f}%)\")\n",
    "    print(f\"  Top Origins: {analysis['top_origem']}\")\n",
    "    print(f\"  Top Formulario: {analysis['top_formulario']}\")\n",
    "    print(f\"  Tipo Pedido: {analysis['top_tipo_de_pedido']}\")\n",
    "    print(f\"  Top Models: {analysis['top_modelo']}\")\n",
    "    print(f\"  Top Agrupamento: {analysis['top_agrupamento']}\")\n",
    "    print(f\"  Top Caracterização: {analysis['top_caracterizacao']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407250bb-a601-4d30-8330-f1d8ec66518c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "visualize_clusters(embeddings, cluster_labels, best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c955b0-f887-4b13-8773-de729fedfd1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualization with meaningful labels\n",
    "visualize_clusters(embeddings, cluster_labels, best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807456eb-be3b-41d9-b698-c8e895428108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(clustered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81456f0c-c6d5-43d0-8528-5b6684198845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "target_schema = spark.table(\"workspace.sc_gold.contactos_pbs_clusters\").schema\n",
    "\n",
    "# Converts clustered_df, which is a pandas DataFrame\n",
    "clustered_spark_df = spark.createDataFrame(clustered_df)\n",
    "# Reorder and cast DataFrame columns to match the target schema\n",
    "clustered_spark_df = clustered_spark_df.select(\n",
    "    [col(field.name).cast(field.dataType) for field in target_schema]\n",
    ")\n",
    "\n",
    "clustered_spark_df.write \\\n",
    "                  .format(\"delta\") \\\n",
    "                  .mode(\"append\") \\\n",
    "                  .saveAsTable(\"workspace.sc_gold.contactos_pbs_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c65eef-3ef6-4f1d-9fa1-0b1bdbecc523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Additional analysis, confirming that clusters with lower scores are not suited\n",
    "'''\n",
    "print(\"Analyzing 4 clusters...\")\n",
    "clustered_df, cluster_analysis = analyze_clusters(contacto_df, cluster_labels, contacto_descriptions)\n",
    "\n",
    "# Print cluster analysis for 4 groups\n",
    "for cluster_name, analysis in cluster_analysis.items():\n",
    "    print(f\"\\n{cluster_name}:\")\n",
    "    print(f\"  Size: {analysis['size']} ({analysis['percentage']:.1f}%)\")\n",
    "    print(f\"  Top Origins: {analysis['top_origem']}\")\n",
    "    print(f\"  Top Formulario: {analysis['top_formulario']}\")\n",
    "    print(f\"  Tipo Pedido: {analysis['top_tipo_de_pedido']}\")\n",
    "    print(f\"  Top Models: {analysis['top_modelo']}\")\n",
    "    print(f\"  Top Agrupamento: {analysis['top_agrupamento']}\")\n",
    "    print(f\"  Top Caracterização: {analysis['top_caracterizacao']}\")\n",
    "\n",
    "# Visualize results\n",
    "visualize_clusters(embeddings, cluster_labels, best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbda0988-d011-43ee-a98b-84a669eaac46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Results for 5 clusters (with noise removed)\n",
    "visualize_clusters_optimized(embeddings, cluster_labels, best_k)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8312250017423022,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook_contactos_clustering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
