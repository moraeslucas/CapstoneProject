{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58351989-060c-4a08-a807-b237050195a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#'-U' upgrades the package to the latest available version\n",
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78526232-033b-489e-957c-ba5ec3a4255d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn \n",
    "%pip install pandas \n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3b4d03-505c-49c1-bfd9-795210198da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e4d1031-d1ed-4aab-9ecc-4fb3db564bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Load contactos data to create behavioral descriptions later on\"\"\"\n",
    "def load_and_preprocess_gold_contactos():\n",
    "    df = spark.table(\"workspace.sc_gold.contactos_pbs\").limit(10000)\n",
    "    # Convert to Pandas for easier text processing\n",
    "    df_pd = df.toPandas()\n",
    "    \n",
    "    return df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c7dcbf-9d67-4c41-9dc5-963387b2d030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Transform contactos data into natural language descriptions\"\"\"\n",
    "def create_contactos_descriptions(df):\n",
    "    descriptions = []\n",
    "    \n",
    "    # 'df.iterrows()' to iterate over DataFrame rows as (index, Series) pairs, with this index being ignored due to the '_'\n",
    "    for _, row in df.iterrows():\n",
    "        # Our key behavioral features\n",
    "        origem        = row['origem']         if pd.notna(row['origem']) \\\n",
    "                                              else 'não especificado'\n",
    "        formulario    = row['formulario']     if pd.notna(row['formulario']) \\\n",
    "                                              else 'não especificado'\n",
    "        tipo_de_pedido= row['tipo_de_pedido'] if pd.notna(row['tipo_de_pedido']) \\\n",
    "                                              else 'não especificado'\n",
    "        modelo        = row['modelo']         if pd.notna(row['modelo']) \\\n",
    "                                              else 'não especificado'\n",
    "        consentimento = row['consentimento']  if pd.notna(row['consentimento']) \\\n",
    "                                              else 'não especificado'\n",
    "        email_opt_out = row['email_opt_out']  if pd.notna(row['email_opt_out']) \\\n",
    "                                              else 'não especificado'\n",
    "        agrupamento   = row['agrupamento_cliente'] if pd.notna(row['agrupamento_cliente'])\\\n",
    "                                                   else 'não especificado'\n",
    "        caracterizacao= row['caracterizacao'] if pd.notna(row['caracterizacao']) \\\n",
    "                                              else 'não especificado'\n",
    "        \n",
    "        # And then we create a natural language description in Portuguese\n",
    "        description = f\"Origem do contacto: {origem}, através do formulário {formulario}, \" \\\n",
    "                      f\"solicitando {tipo_de_pedido} para o modelo {modelo}. \" \\\n",
    "                      f\"Status do consentimento: {consentimento}, \" \\\n",
    "                      f\"prefere receber email: {email_opt_out}, agrupado em: {agrupamento}, \" \\\n",
    "                      f\"caracterizado como: {caracterizacao}\"\n",
    "\n",
    "        descriptions.append(description)\n",
    "    \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3896fbc1-8528-4b63-aca3-9c7087fb9f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Uses SentenceTransformer model from Hugging Face, a Python framework which performs comparably to OpenAI embeddings\"\"\"\n",
    "def generate_embeddings(descriptions, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate the embeddings, meaning the numerical vector representations of contactos descriptions\n",
    "    print(f\"Generating embeddings for {len(descriptions)} Contactos...\")\n",
    "    embeddings = model.encode(descriptions, show_progress_bar=True)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8772b94b-7873-40d7-bb2b-b1abb45b1195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Perform K-means clustering with optimal cluster selection\"\"\"\n",
    "def perform_clustering(embeddings, n_clusters_range=range(2, 10)):\n",
    "    best_score = -1\n",
    "    best_k = 2\n",
    "    scores = []\n",
    "    \n",
    "    # Find optimal nº of clusters using the silhouette score\n",
    "    for k in n_clusters_range:\n",
    "        #'n_init=10' runs 10 times with different centroids¹ and picks the best result\n",
    "        #¹They're the algorithm’s guess at where clusters might be\n",
    "        kmeans = KMeans(n_clusters=k, \n",
    "                        random_state=42, \n",
    "                        n_init=10)\n",
    "        #Assigns each embeddings data point to a cluster\n",
    "        cluster_labels = kmeans.fit_predict(embeddings)\n",
    "        #'silhouette_score' measures how well each point fits within its cluster.\n",
    "        score = silhouette_score(embeddings, cluster_labels)\n",
    "        scores.append(score)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    \n",
    "    # Final clustering with best k\n",
    "    final_kmeans = KMeans(n_clusters=best_k, \n",
    "                          random_state=42, \n",
    "                          n_init=10)\n",
    "    final_labels = final_kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    return final_labels, best_k, best_score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ada40c-be75-4afd-b9cb-b7a33ad5ba9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Analyze cluster characteristics\"\"\"\n",
    "def analyze_clusters(df, cluster_labels, descriptions):\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered['labels'] = cluster_labels\n",
    "    df_clustered['descriptions'] = descriptions\n",
    "    \n",
    "    cluster_analysis = {}\n",
    "    # Analyze cluster distributions\n",
    "    #'set' gets all distinct cluster labels, while 'range' lets you iterate over them\n",
    "    for cluster_label in range(len(set(cluster_labels))):\n",
    "        cluster_data = df_clustered[df_clustered['labels'] == cluster_label]\n",
    "        \n",
    "        analysis = {\n",
    "            'size': len(cluster_data),                                  # Nº of rows in the cluster\n",
    "            'percentage': len(cluster_data) / len(df_clustered) * 100,  # Share of this cluster relative to the whole dataset\n",
    "            'top_origem': cluster_data['origem'].value_counts().head(3) # Top 3 most frequent values in the origem column \n",
    "                                                               .to_dict(),\n",
    "            'top_modelo': cluster_data['modelo'].value_counts().head(3) # First 3 most frequent values in the modelo column\n",
    "                                                               .to_dict(),\n",
    "            'top_tipo_cliente': cluster_data['tipo_cliente'].value_counts()\n",
    "                                                            .head(3)    # Top 3 most frequent Contactos types\n",
    "                                                            .to_dict(),   \n",
    "            'sample_descriptions': cluster_data['descriptions'].head(3) #First 3 Contacto descriptions from this cluster\n",
    "                                                               .tolist()  \n",
    "        }\n",
    "\n",
    "        cluster_analysis[f'Cluster_{cluster_label}'] = analysis\n",
    "    \n",
    "    return df_clustered, cluster_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821e3cdd-05fe-4eff-90a2-81a0e1ec8e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Create visualizations for cluster analysis\"\"\"\n",
    "def visualize_clusters(embeddings, cluster_labels, best_k):    \n",
    "    # PCA for 2D visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # PCA plot\n",
    "    scatter = axes[0].scatter(embeddings_2d[:, 0], \n",
    "                              embeddings_2d[:, 1], \n",
    "                              c=cluster_labels, \n",
    "                              cmap='tab10', \n",
    "                              alpha=0.6)\n",
    "    axes[0].set_title(f'Contacto Clusters (PCA) - {best_k} clusters')\n",
    "    axes[0].set_xlabel('First Principal Component')\n",
    "    axes[0].set_ylabel('Second Principal Component')\n",
    "    plt.colorbar(scatter, ax=axes[0])\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    axes[1].bar(unique_labels, counts)\n",
    "    axes[1].set_title('Cluster Size Distribution')\n",
    "    axes[1].set_xlabel('Cluster ID')\n",
    "    axes[1].set_ylabel('Number of Contactos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4aaf8aa-61c7-423d-94c8-83e1b568e840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Main function to execute the clustering pipeline\"\"\"\n",
    "\n",
    "print(\"Loading contactos data...\")\n",
    "contacto_df = load_and_preprocess_gold_contactos()\n",
    "\n",
    "print(\"Creating contactos behavioral descriptions...\")\n",
    "contacto_descriptions = create_contactos_descriptions(contacto_df)\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = generate_embeddings(contacto_descriptions)\n",
    "\n",
    "print(\"Performing clustering...\")\n",
    "cluster_labels, best_k, best_score, scores = perform_clustering(embeddings)\n",
    "\n",
    "print(f\"Optimal number of clusters: {best_k}\")\n",
    "print(f\"Best silhouette score: {best_score:.3f}\")\n",
    "print(f\"Silhouette scores for different Ks: {scores}\")\n",
    "\n",
    "print(\"Analyzing clusters...\")\n",
    "clustered_df, cluster_analysis = analyze_clusters(contacto_df, cluster_labels, contacto_descriptions)\n",
    "\n",
    "# Print cluster analysis\n",
    "for cluster_name, analysis in cluster_analysis.items():\n",
    "    print(f\"\\n{cluster_name}:\")\n",
    "    print(f\"  Size: {analysis['size']} ({analysis['percentage']:.1f}%)\")\n",
    "    print(f\"  Top Origins: {analysis['top_origem']}\")\n",
    "    print(f\"  Top Models: {analysis['top_modelo']}\")\n",
    "    print(f\"  Contacto Types: {analysis['top_tipo_cliente']}\")\n",
    "\n",
    "# Visualize results\n",
    "visualize_clusters(embeddings, cluster_labels, best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aabb550-c717-4509-92c7-63f065b47128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Slides\"\"\"\n",
    "#SentenceTransformers model from Hugging Face is a Python framework for state-of-the-art embeddings\n",
    "#Include slides for -Embeddings too\n",
    "#the silhouette score description as well"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook_contactos_clustering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
