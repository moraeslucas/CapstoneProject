{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df86d39f-a797-42c0-8263-4c3e5454cdb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--To check if there are duplicates in table contactos before the join , just for security\n",
    "\n",
    "SELECT\n",
    "  id,\n",
    "  data_criacao_da_lead,\n",
    "  COUNT(*) AS cnt\n",
    "FROM sc_gold.contactos_pbs\n",
    "GROUP BY id, data_criacao_da_lead\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY cnt DESC, id, data_criacao_da_lead;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d42b5d-8fb0-486f-9df7-8375592abeaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE sc_gold.`leads_pbs`;\n",
    "-- or\n",
    "SHOW COLUMNS IN sc_gold.`leads_pbs`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f35e18-59b4-4588-a1f4-00e4e75c0e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  created_time AS lead_created_time\n",
    "FROM sc_silver.leads_pbs\n",
    "LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e7d5ec-1780-42bf-8857-907dee087687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  data_criacao_da_lead\n",
    "FROM sc_silver.contactos_pbs\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722797fb-e955-4e72-a980-49e90d182f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "\n",
    "SELECT * FROM sc_gold.campaigns             LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a83bf42-5883-4081-bd46-59801fb7e35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.leads_pbs             LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92a373b-c31d-4fb5-a69b-eb1ec4835b61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sc_gold.contactos_pbs         LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eabf9ef-a305-4a84-869b-bec6ea220f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.propostas_realizadas  LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9a2714-e79b-4a59-9d70-fcd4575cc79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.deals                 LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfa7f30-b7b1-42b5-afff-e6b1edf03d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.contactos_pbs         LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7404c247-7710-45f4-8af6-8a32adb70f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build and run: CREATE OR REPLACE TABLE sc_gold.contactos_pbs_2 AS SELECT ... FROM sc_gold.contactos_pbs\n",
    "\n",
    "# 1) Read source columns in order\n",
    "cols = spark.sql(\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM information_schema.columns\n",
    "  WHERE table_schema = 'sc_gold'\n",
    "    AND table_name   = 'contactos_pbs'\n",
    "  ORDER BY ordinal_position\n",
    "\"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "# Optional: columns you do NOT want renamed (e.g., keep id as-is)\n",
    "skip = set()   # e.g., {'id'}\n",
    "\n",
    "# 2) Build the select list with aliases\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    new = c if (c.endswith('_contactos') or c in skip) else f\"{c}_contactos\"\n",
    "    select_exprs.append(f\"`{c}` AS `{new}`\")\n",
    "\n",
    "select_sql = \", \".join(select_exprs)\n",
    "\n",
    "# 3) Build the full CREATE TABLE AS SELECT statement\n",
    "create_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE sc_gold.contactos_pbs_2 AS\n",
    "SELECT {select_sql}\n",
    "FROM sc_gold.contactos_pbs\n",
    "\"\"\"\n",
    "\n",
    "print(create_sql)  # shows exactly what will run\n",
    "spark.sql(create_sql)\n",
    "\n",
    "# 4) Quick verification\n",
    "spark.sql(\"DESCRIBE TABLE sc_gold.contactos_pbs_2\").show(truncate=False)\n",
    "spark.sql(\"SELECT * FROM sc_gold.contactos_pbs_2 LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432f8a4a-597b-4463-b7a9-53471c5de947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.contactos_pbs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b1cd2b-2e6a-4cef-9f08-ae524008923c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source and target\n",
    "source_table = \"sc_gold.propostas_realizadas\"\n",
    "target_table = \"sc_gold.propostas_realizadas2\"\n",
    "\n",
    "# 1) Get columns\n",
    "cols = spark.sql(f\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM information_schema.columns\n",
    "  WHERE table_schema = 'sc_gold'\n",
    "    AND table_name   = 'propostas_realizadas'\n",
    "  ORDER BY ordinal_position\n",
    "\"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "# Optional: columns to keep unchanged\n",
    "skip = set()   # e.g., {'id'}\n",
    "\n",
    "# 2) Build select list with renamed aliases\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    new = c if (c.endswith('_propostas') or c in skip) else f\"{c}_propostas\"\n",
    "    select_exprs.append(f\"`{c}` AS `{new}`\")\n",
    "\n",
    "select_sql = \", \".join(select_exprs)\n",
    "\n",
    "# 3) Build CREATE TABLE AS SELECT\n",
    "create_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {target_table} AS\n",
    "SELECT {select_sql}\n",
    "FROM {source_table}\n",
    "\"\"\"\n",
    "\n",
    "print(create_sql)   # Show what will run\n",
    "spark.sql(create_sql)\n",
    "\n",
    "# 4) Verify\n",
    "spark.sql(f\"DESCRIBE TABLE {target_table}\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {target_table} LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f58f63-0892-4b59-8c1c-b6829bc0c796",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"created_time_propostas\":261,\"id_propostas\":218},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757282795618}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.propostas_realizadas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae47343-02f0-41d9-8cd3-2cab918d5090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source and target\n",
    "source_table = \"sc_gold.deals\"\n",
    "target_table = \"sc_gold.deals2\"\n",
    "\n",
    "# 1) Get columns\n",
    "cols = spark.sql(f\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM information_schema.columns\n",
    "  WHERE table_schema = 'sc_gold'\n",
    "    AND table_name   = 'deals'\n",
    "  ORDER BY ordinal_position\n",
    "\"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "# Optional: columns to keep unchanged (e.g., primary keys)\n",
    "skip = set()   # Example: {'id'}\n",
    "\n",
    "# 2) Build select list with renamed aliases\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    new = c if (c.endswith('_deals') or c in skip) else f\"{c}_deals\"\n",
    "    select_exprs.append(f\"`{c}` AS `{new}`\")\n",
    "\n",
    "select_sql = \", \".join(select_exprs)\n",
    "\n",
    "# 3) Build CREATE TABLE AS SELECT\n",
    "create_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {target_table} AS\n",
    "SELECT {select_sql}\n",
    "FROM {source_table}\n",
    "\"\"\"\n",
    "\n",
    "print(create_sql)   # Show generated SQL\n",
    "spark.sql(create_sql)\n",
    "\n",
    "# 4) Verify structure and sample data\n",
    "spark.sql(f\"DESCRIBE TABLE {target_table}\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {target_table} LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9770184b-84d8-4485-a093-1662d51c64c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.deals2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d83ff6a-2dac-4227-a4cf-9b811b1142f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source and target\n",
    "source_table = \"sc_gold.campaigns\"\n",
    "target_table = \"sc_gold.campaigns2\"\n",
    "\n",
    "# 1) Get columns\n",
    "cols = spark.sql(f\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM information_schema.columns\n",
    "  WHERE table_schema = 'sc_gold'\n",
    "    AND table_name   = 'campaigns'\n",
    "  ORDER BY ordinal_position\n",
    "\"\"\").toPandas()['column_name'].tolist()\n",
    "\n",
    "# Optional: columns to keep unchanged (like primary keys)\n",
    "skip = set()   # Example: {'id'}\n",
    "\n",
    "# 2) Build select list with renamed aliases\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    new = c if (c.endswith('_campaigns') or c in skip) else f\"{c}_campaigns\"\n",
    "    select_exprs.append(f\"`{c}` AS `{new}`\")\n",
    "\n",
    "select_sql = \", \".join(select_exprs)\n",
    "\n",
    "# 3) Build CREATE TABLE AS SELECT\n",
    "create_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {target_table} AS\n",
    "SELECT {select_sql}\n",
    "FROM {source_table}\n",
    "\"\"\"\n",
    "\n",
    "print(create_sql)   # Preview generated SQL\n",
    "spark.sql(create_sql)\n",
    "\n",
    "# 4) Verify new table\n",
    "spark.sql(f\"DESCRIBE TABLE {target_table}\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {target_table} LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a55e47c4-a7f3-4c05-ad1f-dfe71a84c240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.campaigns2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a30767-95aa-4496-aac3-c4c5e6be7ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS strict_matches\n",
    "FROM sc_gold.leads_pbs l\n",
    "JOIN sc_gold.contactos_pbs_2 c\n",
    "  ON  l.converted_contact = c.id_contactos\n",
    "  AND l.created_time      = c.data_criacao_da_lead_contactos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0865170-1842-497b-9696-551f854c5da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH joined AS (\n",
    "  SELECT\n",
    "    l.*,\n",
    "    c.*\n",
    "  FROM sc_gold.leads_pbs       AS l\n",
    "  JOIN sc_gold.contactos_pbs_2 AS c\n",
    "    ON  l.converted_contact = c.id_contactos\n",
    "   AND l.created_time      = c.data_criacao_da_lead_contactos\n",
    ")\n",
    "SELECT COUNT(*) AS check_count FROM joined;   -- should be ~118329\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62ef095e-3704-4379-b4d6-0cdb1792d368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- (optional) ensure you're in the right place\n",
    "-- USE CATALOG workspace;\n",
    "-- USE SCHEMA sc_gold;\n",
    "\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT\n",
    "  l.*,\n",
    "  c.*\n",
    "FROM sc_gold.leads_pbs       AS l\n",
    "JOIN sc_gold.contactos_pbs_2 AS c\n",
    "  ON  l.converted_contact = c.id_contactos\n",
    "  AND l.created_time      = c.data_criacao_da_lead_contactos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d674cd-2ce7-4a87-b60e-abe1b3f6833d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  f.*,\n",
    "  p.*   -- assumes Propostas columns are already suffixed (e.g., *_propostas)\n",
    "FROM sc_gold.Features_Table AS f\n",
    "LEFT JOIN sc_gold.propostas_realizadas2 AS p\n",
    "  ON f.id_contactos = p.id_contacto_propostas;   -- keep all Feature rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a780d349-2705-42de-938f-1b41e3b5e5ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  f.*,\n",
    "  d.*   -- assumes Deals columns are already suffixed (e.g., *_deals)\n",
    "FROM sc_gold.Features_Table AS f\n",
    "LEFT JOIN sc_gold.deals2 AS d\n",
    "  ON f.id_contacto_propostas = d.id_contacto_deals;   -- keep all Feature rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c51127-a90a-44b0-9f0e-cd9458ef8d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  f.*,\n",
    "  cp.*   -- assumes campaigns columns are already suffixed (e.g., *_campaigns)\n",
    "FROM sc_gold.Features_Table AS f\n",
    "LEFT JOIN sc_gold.campaigns2 AS cp\n",
    "  ON f.campanha_deals = cp.id_campaigns;   -- join via Features_Table.campanha_deals -> campaigns.id_campaigns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9870b82-3a74-412b-8165-0c3fd24a36a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.fact_table AS\n",
    "SELECT\n",
    "    l.id,\n",
    "  l.converted_contact,\n",
    "  l.created_time,\n",
    "  c.id_contactos,\n",
    "  c.data_criacao_da_lead_contactos\n",
    "FROM sc_gold.leads_pbs AS l\n",
    "JOIN sc_gold.contactos_pbs_2 AS c\n",
    "  ON l.converted_contact = c.id_contactos\n",
    " AND l.created_time = c.data_criacao_da_lead_contactos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363d9117-13c3-492b-a5a4-0aecac0aae2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.fact_table AS\n",
    "SELECT\n",
    "  f.*,\n",
    " \n",
    "    p.id_contacto_propostas,\n",
    "  p.id_proposta_realizada_propostas\n",
    "\n",
    "FROM sc_gold.fact_table AS f\n",
    "LEFT JOIN sc_gold.propostas_realizadas2 AS p\n",
    "  ON p.id_contacto_propostas = f.id_contactos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57fc58b1-56bb-4230-a034-35a3e8e45345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Fact_table AS\n",
    "SELECT\n",
    "  f.*,\n",
    "  d.id_deals,\n",
    "  d.id_contacto_deals,\n",
    "  d.campanha_deals\n",
    "FROM sc_gold.Fact_table AS f\n",
    "LEFT JOIN sc_gold.deals2 AS d\n",
    "  ON f.id_contacto_propostas = d.id_contacto_deals;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d63b20-c30f-429d-8a99-f0100c51c67b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# snapshot the current table to a temp view\n",
    "spark.sql(\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM sc_gold.fact_table\")\n",
    "\n",
    "to_drop = {'converted_contact','data_criacao_da_lead_contactos','id_contacto_propostas','id_contacto_deals'}\n",
    "cols = [f\"`{c}`\" for c in spark.table(\"sc_gold.fact_table\").columns if c not in to_drop]\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE sc_gold.fact_table\n",
    "USING DELTA AS\n",
    "SELECT {', '.join(cols)} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c24d33-8dfa-4150-8e7e-5b01aa17d823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1) Summary counts of duplicates on (id, created_time)\n",
    "WITH d AS (\n",
    "  SELECT id, created_time, COUNT(*) AS cnt\n",
    "  FROM sc_gold.leads_pbs\n",
    "  GROUP BY id, created_time\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)                    AS duplicated_keys,          -- number of (id,created_time) combos with >1 row\n",
    "  SUM(cnt)                    AS rows_in_duplicated_keys,  -- total rows participating in duplicates\n",
    "  SUM(cnt) - COUNT(*)         AS extra_rows_to_dedup       -- rows to drop if you keep 1 per key\n",
    "FROM d;\n",
    "\n",
    "-- 2) See the duplicated keys and their sizes (top 100)\n",
    "SELECT id, created_time, COUNT(*) AS cnt\n",
    "FROM sc_gold.leads_pbs\n",
    "GROUP BY id, created_time\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY cnt DESC, id\n",
    "LIMIT 100;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5feabc3e-6365-4c20-bd73-9825cc33590e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"_fivetran_id\":153},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"link_centro_consentimento_contactos\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1757282068950}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SUMMARY: how many duplicate (id, created_time) keys exist in sc_gold.features ( this is normal , can have multiple proposals)\n",
    "WITH d AS (\n",
    "  SELECT id, created_time, COUNT(*) AS cnt\n",
    "  FROM sc_gold.features_table\n",
    "  GROUP BY id, created_time\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)            AS duplicated_keys,\n",
    "  SUM(cnt)            AS rows_in_duplicated_keys,\n",
    "  SUM(cnt) - COUNT(*) AS extra_rows_to_dedup\n",
    "FROM d;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df364f7-ff68-4bd4-b302-064f4f1980ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SUMMARY: how many duplicate (id_propostas, created_time) keys exist\n",
    "WITH d AS (\n",
    "  SELECT id, created_time, COUNT(*) AS cnt\n",
    "  FROM sc_gold.propostas_realizadas\n",
    "  GROUP BY id, created_time\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)            AS duplicated_keys,          -- number of key pairs with >1 row\n",
    "  SUM(cnt)            AS rows_in_duplicated_keys,  -- total rows across those keys\n",
    "  SUM(cnt) - COUNT(*) AS extra_rows_to_dedup       -- rows you’d remove if keeping 1 per key\n",
    "FROM d;\n",
    "\n",
    "-- DETAILS: which keys are duplicated (top 100 by size)\n",
    "SELECT id, created_time, COUNT(*) AS cnt\n",
    "FROM sc_gold.propostas_realizadas\n",
    "GROUP BY id, created_time\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY cnt DESC, id\n",
    "LIMIT 100;\n",
    "\n",
    "-- OPTIONAL: list the actual duplicate rows\n",
    "SELECT t.*\n",
    "FROM sc_gold.propostas_realizadas t\n",
    "JOIN (\n",
    "  SELECT id, created_time\n",
    "  FROM sc_gold.propostas_realizadas\n",
    "  GROUP BY id, created_time\n",
    "  HAVING COUNT(*) > 1\n",
    ") d\n",
    "ON t.id = d.id AND t.created_time = d.created_time\n",
    "ORDER BY t.id, t.created_time;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ec881c-6915-4158-9219-3e9ce2f54a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH d AS (\n",
    "  SELECT\n",
    "    `id`,\n",
    "    `id_deals`,\n",
    "    `id_proposta_realizada_propostas`,\n",
    "    COUNT(*) AS cnt\n",
    "  FROM sc_gold.features_table\n",
    "  GROUP BY `id`, `id_deals`, `id_proposta_realizada_propostas`\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)              AS duplicated_keys,\n",
    "  SUM(d.cnt)            AS rows_in_duplicated_keys,\n",
    "  SUM(d.cnt) - COUNT(*) AS extra_rows_to_dedup\n",
    "FROM d AS d;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6c2d6d-4d0c-4752-be9d-3a6dc59b7811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH d AS (\n",
    "  SELECT\n",
    "    `id`,\n",
    "    `id_deals`,\n",
    "    `id_proposta_realizada_propostas`,\n",
    "    COUNT(*) AS cnt\n",
    "  FROM sc_gold.fact_table\n",
    "  GROUP BY `id`, `id_deals`, `id_proposta_realizada_propostas`\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*)              AS duplicated_keys,\n",
    "  SUM(d.cnt)            AS rows_in_duplicated_keys,\n",
    "  SUM(d.cnt) - COUNT(*) AS extra_rows_to_dedup\n",
    "FROM d AS d;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41e3eb5-8f18-463f-8493-cae7ef8c27d8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"created_time\":206},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757287190480}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Fact_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5312eeb-6ba6-45c3-a9c2-46ccd98f3d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f46215a-226d-4aca-80df-1de1d1be643b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Build keep list to Ml Models\n",
    "cols_to_delete = [  # the  names we listed\n",
    "    \"_fivetran_id\",\"id\",\"lead_owner\",\"lead_owner_name\",\"consentimento\",\n",
    "    \"id_lead\",\"chave_instalacao_bd_rede\",\"chave_concessao_bd_rede\",\"chave_contrato_concessionario_bd_rede\",\n",
    "    \"chave_agrupamento_performance_vendas\",\"chave_agrupamento_cliente\",\n",
    "    \"data_de_validade_de_consentimento_hyundai\",\"lead_response_time\",\"converted_date_time\",\n",
    "    \"_fivetran_deleted\",\"_fivetran_synced\",\"_fivetran_id_contactos\",\"id_contactos\",\n",
    "    \"contacto_owner_contactos\",\"contacto_owner_name_contactos\",\"created_time_contactos\",\n",
    "    \"id_contacto_contactos\",\"consentimento_contactos\",\"id_lead_conversao_contactos\",\n",
    "    \"data_de_recolha_de_consentimento_hyundai_contactos\",\"chave_concessao_bd_rede_contactos\",\n",
    "    \"chave_instalacao_bd_rede_contactos\",\"chave_contrato_concessionario_bd_rede_contactos\",\n",
    "    \"chave_agrupamento_performance_vendas_contactos\",\"chave_agrupamento_cliente_contactos\",\n",
    "    \"link_centro_consentimento_contactos\",\"_fivetran_deleted_contactos\",\"_fivetran_synced_contactos\",\n",
    "    \"_fivetran_id_propostas\",\"id_propostas\",\"created_time_propostas\",\n",
    "    \"proposta_realizada_owner_propostas\",\"proposta_realizada_owner_name_propostas\",\n",
    "    \"modified_time_propostas\",\"conta_name_propostas\",\"id_contacto_propostas\",\"contacto_name_propostas\",\n",
    "    \"deal_name_propostas\",\"created_by_propostas\",\"modified_by_propostas\",\"descricao_apoio_propostas\",\n",
    "    \"codigo_da_proposta_propostas\",\"descricao_pintura_propostas\",\"data_de_criacao_da_proposta_propostas\",\n",
    "    \"data_de_entrega_da_proposta_propostas\",\"id_conta_propostas\",\"id_negocio_propostas\",\n",
    "    \"id_proposta_realizada_propostas\",\"data_de_validade_da_proposta_propostas\",\"codigo_modelo_propostas\",\n",
    "    \"sufixo_modelo_propostas\",\"codigo_cor_exterior_propostas\",\"codigo_cor_interior_propostas\",\n",
    "    \"concessionario_owner_propostas\",\"data_prevista_matricula_propostas\",\"data_prevista_de_entrega_propostas\",\n",
    "    \"data_da_conclusao_propostas\",\"instalacao_propostas\",\"chave_instalacao_bd_rede_propostas\",\n",
    "    \"chave_concessao_bd_rede_propostas\",\"cod_proposta_propostas\",\"codigo_fabricante_cor_interior_propostas\",\n",
    "    \"codigo_fabricante_cor_exterior_propostas\",\"_fivetran_deleted_propostas\",\"_fivetran_synced_propostas\",\n",
    "    \"_fivetran_id_deals\",\"deal_owner_name_deals\",\"id_contacto_deals\",\n",
    "    \"closing_date_deals\",\"deal_owner_deals\",\"conta_name_deals\",\"contacto_name_deals\",\"id_negocio_deals\",\n",
    "    \"data_prevista_de_reentrada_em_negocio_deals\",\"id_conta_deals\",\"codigo_negocio_deals\",\"data_venda_deals\",\n",
    "    \"nome_contacto_deals\",\"id_deals\",\"data_decisao_negocio_deals\",\"_fivetran_deleted_deals\",\n",
    "    \"_fivetran_synced_deals\",\"_fivetran_id_campaigns\",\"id_campaigns\",\n",
    "    \"campaign_owner_name_campaigns\",\"created_time_campaigns\",\"codigo_campanha_campaigns\",\n",
    "    \"_fivetran_deleted_campaigns\",\"_fivetran_synced_campaigns\"\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # quote names safely\n",
    "\n",
    "# 2) Snapshot and recreate from projection\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "spark.sql(\"DROP VIEW _src\")\n",
    "\n",
    "#You should generally remove columns that:\n",
    "#Are unique identifiers (e.g., _fivetran_id, id, id_lead, id_contactos, id_propostas, id_deals, id_campaigns).\n",
    "#Are timestamps or sync markers from ETL (_fivetran_synced, _fivetran_deleted, etc.).\n",
    "#Contain names of people or owners (lead_owner_name, campaign_owner_name_campaigns, etc.).\n",
    "#Are free-text codes or descriptive identifiers (codigo_campanha_campaigns, codigo_da_proposta_propostas, etc.).\n",
    "#Are internal foreign keys / lookup fields (chave_instalacao_bd_rede, chave_agrupamento_cliente, etc.).\n",
    "#Are emails / links (link_centro_consentimento_contactos).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1b6949-42ec-4c6a-9daa-809e5fdb04d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Built the second delete list after a better analysis \n",
    "cols_to_delete = [\n",
    "    # --- timestamps / dates ---\n",
    "  \n",
    "    \"data_criacao_da_lead_contactos\",\n",
    "    \"data_prevista_de_entrega_deals\",\n",
    "    \"data_prevista_matricula_deals\",\n",
    "    \"data_criacao_da_lead_deals\",\n",
    "\n",
    "    # --- IDs / keys ---\n",
    "    \"created_by\", \"modified_by\",\n",
    "    \"id_classe_propostas\", \"id_model_group_propostas\",\n",
    "    \"id_combustivel_propostas\", \"id_modelo_propostas\",\n",
    "    \"chave_agrupamento_cliente_propostas\",\n",
    "    \"tylacode_propostas\",\n",
    "    \"chave_concessao_bd_rede_deals\", \"chave_instalacao_bd_rede_deals\",\n",
    "    \"chave_contrato_concessionario_bd_rede_deals\",\n",
    "    \"chave_agrupamento_performance_vendas_deals\", \"chave_agrupamento_cliente_deals\",\"campanha_deals\",\n",
    "\n",
    "    # --- descriptive text ---\n",
    "    \"descricao_model_group_propostas\",\n",
    "    \n",
    "\n",
    "    # --- administrative fields ---\n",
    "     \"gestor_area_propostas\",\n",
    "\n",
    "    # --- duplicated / rarely useful flags ---\n",
    "    \"hubleads__nao_comunica_com_cc_\",\n",
    "    \"converted_contact\", \"converted_deal\", \"converted_from_lead_deals\",\n",
    "    \"ccupdated_contactos\", \"myhyundai_contactos\",\n",
    "    \"f2_deals\", \"modelovendaupdate_deals\"\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# 2) Snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df06f4c0-5120-47ed-8b15-8039e685a0fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN month(created_time) IN (12, 1, 2) THEN 'Winter'\n",
    "    WHEN month(created_time) IN (3, 4, 5) THEN 'Spring'\n",
    "    WHEN month(created_time) IN (6, 7, 8) THEN 'Summer'\n",
    "    ELSE 'Autumn'\n",
    "  END AS Season\n",
    "FROM sc_gold.Features_Table;\n",
    "\n",
    "--I created a Season for ML Models ( thinking in the prediction already)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff194c0e-c764-4706-a4ea-3c5e9d0455fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  datediff(created_time_deals, created_time) AS lead_to_deal_days\n",
    "FROM sc_gold.Features_Table;\n",
    "\n",
    "-- this is deals date- leads date to give the number of days that take , for my surprise there are negatives but can be typos or retroactive data entry → A deal was created manually before the lead was officially entered in the CRM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9728670-19b5-4403-8aca-93b0f1a50999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN datediff(created_time_deals, created_time) <= 7 THEN 'Fast'\n",
    "    WHEN datediff(created_time_deals, created_time) <= 30 THEN 'Medium'\n",
    "    ELSE 'Slow'\n",
    "  END AS deal_speed_category\n",
    "FROM sc_gold.Features_Table;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352a5a3f-0cb8-4345-bb8f-9fc00111959b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN created_time_deals < created_time THEN 1\n",
    "    ELSE 0\n",
    "  END AS deal_before_lead\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090334ba-e4a9-4955-8a5c-5a9a59b0bb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN dayofweek(created_time) IN (1,7) THEN 1\n",
    "    ELSE 0\n",
    "  END AS is_weekend_lead\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2063a1bd-e3cd-4732-805c-548510bf3251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN day(created_time_deals) >= 25 THEN 1\n",
    "    ELSE 0\n",
    "  END AS is_month_end_deal\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1562696-b36d-4a96-98af-2a7c2b70a9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN qtd_viaturas_deals = 0 OR qtd_viaturas_deals IS NULL \n",
    "        THEN 0\n",
    "        ELSE num_de_propostas_deals / qtd_viaturas_deals\n",
    "    END AS proposals_per_vehicle\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27b647e-2347-410a-8f97-ef06aca63641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE sc_gold.Features_Table AS\n",
    "SELECT\n",
    "  *,\n",
    "  year(created_time)  AS lead_year,\n",
    "  month(created_time) AS lead_month\n",
    "FROM sc_gold.Features_Table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a6f101-4b85-4b98-a910-55d3d117c40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_delete = [\n",
    "    \"created_time\",\n",
    "    \"activities_involved\",\n",
    "    \"created_time_deals\"\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6075ba41-e551-469c-b104-b9db9b8c4982",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757799581949}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef8019f-41b0-46bf-9b3f-b042ccc74a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  is_converted,\n",
    "  COUNT(*) AS total_leads\n",
    "FROM sc_gold.leads_pbs\n",
    "GROUP BY is_converted\n",
    "ORDER BY is_converted;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8997403-1fd1-4dfc-8528-76e34b57d80a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- K-Means via pandas + scikit-learn (version-safe for OneHotEncoder) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from packaging.version import parse as V\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a691f4-9006-41e3-ae49-e5a05e3d5164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pull data (sample if your table is very large)\n",
    "pdf = spark.sql(\"SELECT * FROM sc_gold.Features_Table\").toPandas()\n",
    "# Example sampling to avoid driver OOM:\n",
    "# pdf = spark.sql(\"SELECT * FROM sc_gold.Features_Table TABLESAMPLE (50 PERCENT)\").toPandas()\n",
    "\n",
    "# Identify columns\n",
    "cat_cols = pdf.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "num_cols = pdf.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Preprocessing blocks\n",
    "numeric_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Version-safe OneHotEncoder kwargs\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "if V(sklearn.__version__) >= V(\"1.2\"):\n",
    "    ohe_kwargs[\"sparse_output\"] = True      # modern scikit-learn\n",
    "else:\n",
    "    ohe_kwargs[\"sparse\"] = True             # older scikit-learn\n",
    "\n",
    "categorical_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_tf, num_cols),\n",
    "        (\"cat\", categorical_tf, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0  # keep matrix sparse if possible\n",
    ")\n",
    "\n",
    "# Transform to model matrix\n",
    "X = preprocess.fit_transform(pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb7cc5e-73fd-4a50-9c23-0892203aff24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Search k by silhouette\n",
    "k_range = range(2, 9)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels) if len(set(labels)) > 1 else np.nan\n",
    "    scores.append((k, sil))\n",
    "\n",
    "best_k = max(scores, key=lambda x: (-1 if np.isnan(x[1]) else x[1]))[0]\n",
    "print(\"Best k:\", best_k)\n",
    "print(pd.DataFrame(scores, columns=[\"k\",\"silhouette\"]).sort_values(\"silhouette\", ascending=False))\n",
    "\n",
    "# Fit final model\n",
    "km = KMeans(n_clusters=best_k, n_init=50, random_state=42)\n",
    "labels = km.fit_predict(X)\n",
    "pdf[\"cluster\"] = labels\n",
    "\n",
    "# Cluster sizes & peek\n",
    "print(\"Cluster sizes:\\n\", pdf[\"cluster\"].value_counts().sort_index())\n",
    "preview_cols = [c for c in [\"lead_source\",\"lead_status\",\"lead_status_2\",\"is_converted\",\"distrito\",\"pais\"] if c in pdf.columns]\n",
    "print(pdf.head(10)[[\"cluster\"] + preview_cols])\n",
    "\n",
    "# Write back to Delta (works from a cluster notebook; may be blocked on SQL Warehouse)\n",
    "try:\n",
    "    spark.createDataFrame(pdf).write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"sc_gold.Features_Table_kmeans\")\n",
    "    print(\"Saved table: sc_gold.Features_Table_kmeans\")\n",
    "except Exception as e:\n",
    "    print(\"Could not save to Delta from this environment:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a406e380-36b9-41af-8319-6646bf2200d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \"lead_status\",                    # leakage: deal-stage info\n",
    "    \"lead_status_2\",                  # leakage: deal-stage variant\n",
    "    \"stage_deals\",                          # leakage: pipeline stage after lead\n",
    "    \"forecast_type_deals\",                  # leakage\n",
    "    \"motivo_fecho_negocio_deals\",           # leakage: reason for closing\n",
    "     \"categoria_negocio_venda_deals\",        # leakage; also mostly null in many exports\n",
    "    \"modelo_de_venda_deals\",                # leakage; often mostly null\n",
    "    \"tipo_de_negocio_deals\",                # leakage; often mostly null\n",
    "        \"tipo_de_pedido\",                 # leakage\n",
    "         \"tipo_de_pedido_contactos\",                 # leakage\n",
    "           \"categoria_proposta_propostas\",                       # leakage\n",
    "           \"activities_involved_deals\",            # leakage: totals up to close\n",
    " \n",
    "\n",
    "    # ----------------------------\n",
    "    # ID-like / keys (non-predictive & high-unique)\n",
    "    # ----------------------------\n",
    "    \"contrato_concessionario_deals\",        # ID-like (deal)\n",
    "    \"contrato_concessionario\",              # ID-like\n",
    "    \"contrato_concessionario_contactos\",    # ID-like\n",
    "    \"concessionario_escolhido\",             # ID-like / chosen dealer\n",
    "    \"estado_do_pedido_propostas\",           # often key-like / admin-ish\n",
    "\n",
    "    # ----------------------------\n",
    "    # Data quality: free text / high null / near-constant / high-cardinality\n",
    "    # ----------------------------\n",
    "    \"descricao_classe_propostas\",           # free text / mostly null\n",
    "    \"subject_propostas\",                    # free text / high-cardinality\n",
    "    \"margem_frota_eni_propostas\",           # mostly null\n",
    "    \"sub_total_com_extras_propostas\",       # mostly null\n",
    "    \"valor_aprovado_propostas\",             # mostly null\n",
    "    \"valor_extras_propostas\",               # mostly null\n",
    "    \"valor_do_apoio_pedido_propostas\",      # mostly null; also key-like in some exports\n",
    "    \"n__matriculas_associadas_deals\"        # near-constant / mostly null (deal-level)\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950e162e-5e6e-4914-a4fa-16b25ea7f4cd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757809256273}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b87046-3c3c-448e-a0dd-64049529f782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "tbl = \"sc_gold.Features_Table\"\n",
    "cols = [\n",
    "    \"concessao\",\n",
    "    \"instalacao\",\n",
    "    \"agrupamento_performance_vendas\",\n",
    "    \"agrupamento_cliente\",\n",
    "    \"concessao_contactos\",\n",
    "    \"instalacao_contactos\",\n",
    "    \"agrupamento_performance_vendas_contactos\",\n",
    "    \"agrupamento_cliente_contactos\",\n",
    "    \"concessao_propostas\",\n",
    "    \"concessao_deals\",\n",
    "    \"instalacao_deals\",\n",
    "    \"agrupamento_performance_vendas_deals\",\n",
    "    \"agrupamento_cliente_deals\",\n",
    "]\n",
    "\n",
    "df = spark.table(tbl)\n",
    "present = [c for c in cols if c in df.columns]\n",
    "exprs = [\n",
    "    (F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)) for c in present\n",
    "]\n",
    "res = df.agg(*exprs).collect()[0].asDict()\n",
    "total = df.count()\n",
    "\n",
    "out = [{\"column\": c, \"null_count\": int(res.get(c, 0)), \"rows_total\": total,\n",
    "        \"null_pct\": f\"{(res.get(c, 0)/total)*100:.2f}%\"} for c in present]\n",
    "for c in cols:\n",
    "    if c not in present:\n",
    "        out.append({\"column\": c, \"null_count\": \"—\", \"rows_total\": total, \"null_pct\": \"not in table\"})\n",
    "\n",
    "spark.createDataFrame(out).orderBy(\"column\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2975a9a6-7462-4ab8-b4b7-a2982216f37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \"lead_status\",                    # leakage: deal-stage info\n",
    "    \"instalacao\",\n",
    "    \"agrupamento_performance_vendas\",\n",
    "    \"agrupamento_cliente\",\n",
    "    \"instalacao_contactos\",\n",
    "    \"agrupamento_performance_vendas_contactos\",\n",
    "    \"agrupamento_cliente_contactos\",\n",
    "    \"concessao_propostas\",\n",
    "    \"concessao_deals\",\n",
    "    \"instalacao_deals\",\n",
    "    \"agrupamento_performance_vendas_deals\",\n",
    "    \"agrupamento_cliente_deals\" ]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c1b3306-e4fb-41e3-a3c7-dd2bd14acaab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "--I will replace the blanks with No campaign because I really want to study this column on the model\n",
    "\n",
    "UPDATE sc_gold.Features_Table\n",
    "SET campaign_name_campaigns = 'No Campaign'\n",
    "WHERE campaign_name_campaigns IS NULL;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb297a0-78dd-410e-891e-7eec7a5fe058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "--I will replace the blanks with Unknown because I really want to study this column on the model\n",
    "\n",
    "UPDATE sc_gold.Features_Table\n",
    "SET motivo_da_perda_deals = 'Unknown'\n",
    "WHERE motivo_da_perda_deals IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61a1a7c-cb11-4d4e-9880-03a91c974dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE sc_gold.Features_Table\n",
    "SET modelo = 'Unknown'\n",
    "WHERE modelo IS NULL;\n",
    "\n",
    "UPDATE sc_gold.Features_Table\n",
    "SET versao_propostas = 'Unknown'\n",
    "WHERE versao_propostas IS NULL;\n",
    "\n",
    "UPDATE sc_gold.Features_Table\n",
    "SET combustivel_propostas = 'Unknown'\n",
    "WHERE combustivel_propostas IS NULL;\n",
    "\n",
    "UPDATE sc_gold.Features_Table\n",
    "SET origem_do_negocio_deals = 'Unknown'\n",
    "WHERE origem_do_negocio_deals IS NULL;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179b8217-86cf-44ed-bbcd-acdc09f36885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#since nulls are only 0.07%, filling with the mode (most frequent value) is a very good option.\n",
    "#Fill with mean/median\n",
    "#Median = more robust to skew/outliers.\n",
    "\n",
    "col_name = \"tipo_cliente_contactos\"\n",
    "\n",
    "# get mode (most frequent value)\n",
    "mode_value = (\n",
    "    df.groupBy(col_name)\n",
    "      .count()\n",
    "      .orderBy(F.desc(\"count\"))\n",
    "      .first()[0]\n",
    ")\n",
    "\n",
    "# fill nulls with mode\n",
    "df = df.na.fill({col_name: mode_value})\n",
    "\n",
    "# overwrite the existing table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"sc_gold.Features_Table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36809eaf-f4af-4d3f-bbca-f54fe3f7f9a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#this one looks numeric (proposals_per_vehicle) with small percentage of nulls (2.53%). That’s easy to handle.\n",
    "\n",
    "\n",
    "col_name = \"proposals_per_vehicle\"\n",
    "\n",
    "# calculate median\n",
    "median_val = df.approxQuantile(col_name, [0.5], 0.01)[0]  # 0.5 = median, tolerance=0.01\n",
    "\n",
    "# print the median\n",
    "print(f\"Median value for {col_name}: {median_val}\")\n",
    "\n",
    "# fill nulls with median\n",
    "df = df.na.fill({col_name: median_val})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c376ce8f-684d-4b44-9b1d-bb48c5ad2ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(\"sc_gold.Features_Table\")\n",
    "\n",
    "# ===== Median fill for proposals_per_vehicle =====\n",
    "col_name = \"proposals_per_vehicle\"\n",
    "\n",
    "# calculate median with approxQuantile\n",
    "median_val = df.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "\n",
    "# fill nulls with the median\n",
    "df = df.na.fill({col_name: median_val})\n",
    "\n",
    "# ===== Overwrite the table =====\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"sc_gold.Features_Table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f31219aa-3f90-489c-b6a9-a71ac45d84a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pvp_propostas is the price of sales of the cars , we have the models , maybe do a replace by median of each model and when the model is Unknown replace by the overall median?\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 0) Load\n",
    "df = spark.table(TBL)\n",
    "\n",
    "# Count nulls before\n",
    "before_nulls = df.filter(F.col(\"pvp_propostas\").isNull()).count()\n",
    "print(f\"Nulls in pvp_propostas BEFORE: {before_nulls:,}\")\n",
    "\n",
    "# 1) Compute per-model median\n",
    "model_medians = (\n",
    "    df.filter(F.col(\"pvp_propostas\").isNotNull())\n",
    "      .groupBy(\"modelo\")\n",
    "      .agg(F.expr(\"percentile_approx(pvp_propostas, 0.5)\").alias(\"median_pvp\"))\n",
    ")\n",
    "\n",
    "# 2) Join medians\n",
    "df = df.join(model_medians, on=\"modelo\", how=\"left\")\n",
    "\n",
    "# 3) Compute overall median\n",
    "overall_median = df.approxQuantile(\"pvp_propostas\", [0.5], 0.01)[0]\n",
    "print(f\"Overall median pvp_propostas: {overall_median}\")\n",
    "\n",
    "# 4) Impute\n",
    "df = df.withColumn(\n",
    "    \"pvp_propostas\",\n",
    "    F.when(\n",
    "        F.col(\"pvp_propostas\").isNull(),\n",
    "        F.when(F.col(\"modelo\") == \"Unknown\", F.lit(overall_median))\n",
    "         .otherwise(F.coalesce(F.col(\"median_pvp\"), F.lit(overall_median)))\n",
    "    ).otherwise(F.col(\"pvp_propostas\"))\n",
    ")\n",
    "\n",
    "# 5) Drop helper column\n",
    "df = df.drop(\"median_pvp\")\n",
    "\n",
    "# 6) Cast back to decimal(29,2)\n",
    "df = df.withColumn(\"pvp_propostas\", F.col(\"pvp_propostas\").cast(DecimalType(29, 2)))\n",
    "\n",
    "# Count nulls after\n",
    "after_nulls = df.filter(F.col(\"pvp_propostas\").isNull()).count()\n",
    "print(f\"Nulls in pvp_propostas AFTER: {after_nulls:,}\")\n",
    "\n",
    "# 7) Overwrite the table\n",
    "df.write.mode(\"overwrite\").saveAsTable(TBL)\n",
    "print(f\"Table {TBL} overwritten successfully.\")\n",
    "\n",
    "#Computes the median price per modelo.\n",
    "\n",
    "#Fills missing pvp_propostas with that median; if the row’s modelo is \"Unknown\" (or the model median is unavailable), it falls back to the overall median.\n",
    "\n",
    "#Overwrites sc_gold.Features_Table with the imputed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55eb3215-ba3e-4790-84c0-b0555bdc6459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Technic Median imputation + missing flag (baseline for ML)\n",
    "#Fill nulls with the median (robust to skew).\n",
    "#Add _missing indicator columns.\n",
    "#That way ML models can separate “normal duration” vs. “missing duration”.\n",
    "#🔹 Why it helps ML\n",
    "#If missing values are completely random, the model just uses the imputed median and ignores the flag.\n",
    "#If missing values are systematic (e.g., certain customers or deals often have missing durations), then the flag lets the model capture that pattern.\n",
    "#You don’t distort the data too much (everything stays numeric).\n",
    "#Works with tree-based models (XGBoost, LightGBM, Random Forest) and linear models.\n",
    "#Is it the best option?\n",
    "#For a first baseline ML pipeline: ✅ Yes, it’s the safest, most widely used approach.\n",
    "#For business-critical features like your durations: it’s a solid start, but may not be the final solution. You can improve later with group-based medians or domain-specific imputations (e.g., different medians for B2B vs B2C customers).\n",
    "#Think of Option 1 as the “default robust preprocessing”. Once you see feature importance and model performance, you can refine further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cb02c4-0a27-480b-91b9-562da328c056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df  = spark.table(TBL)\n",
    "\n",
    "# Total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Count how many rows have 1 in that column + percentage\n",
    "df.filter(F.col(\"overall_sales_duration_deals_missing\") == 1) \\\n",
    "  .agg(\n",
    "      F.count(\"*\").alias(\"count_1\")\n",
    "  ) \\\n",
    "  .withColumn(\"pct_1\", (F.col(\"count_1\") / total_rows) * 100) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f33d7c4-e15c-4e61-a043-5d26bb873cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "duration_cols = [\"overall_sales_duration_deals\", \"lead_to_deal_days\"]\n",
    "\n",
    "for c in duration_cols:\n",
    "    med = df.approxQuantile(c, [0.5], 0.01)[0]\n",
    "    print(f\"Median for {c}: {med}\")\n",
    "\n",
    "    # add missing flag + impute\n",
    "    df = df.withColumn(f\"{c}_missing\", F.when(F.col(c).isNull(), 1).otherwise(0).cast(IntegerType()))\n",
    "    df = df.na.fill({c: med})\n",
    "    df = df.withColumn(c, F.col(c).cast(IntegerType()))  # keep dtype consistent\n",
    "\n",
    "# Overwrite and allow schema change (because of new *_missing columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TBL)\n",
    "print(f\"✅ Overwrote {TBL} with median imputations + missing flags.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16088a91-a52f-4cbd-a735-6a6b4a63e476",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757862983349}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2641fe-21a2-4d17-bdd3-91d556279a3d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"column\":239},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757857561261}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "row_count = df.count()\n",
    "dtypes = dict(df.dtypes)\n",
    "\n",
    "# --- STRICT NULLS ONLY (what you asked) ---\n",
    "null_aggs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    "null_row = df.agg(*null_aggs).collect()[0].asDict()\n",
    "\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    n_null = int(null_row.get(c, 0))\n",
    "    rows.append((c, dtypes[c], n_null, row_count, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"dtype\", \"null_count\", \"rows_total\", \"null_pct\"])\n",
    "display(out.orderBy(F.desc(\"null_pct\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a49f0f-339f-49c6-8c0a-b70ec7a4d02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \"lead_status\",  \n",
    "      \"apoio_total_propostas\",\n",
    "    \"forma_de_pagamento_propostas\",\n",
    "    \"apoio_concessionario_propostas\",\n",
    "    \"comparticipacao_retoma_concessao_propostas\",\n",
    "    \"apoio_retoma_hyundai_propostas\",\n",
    "    \"apoio_financiamento_cetelem_propostas\",\n",
    "    \"formulario_deals\",\n",
    "    \"comparticipacao_campanha_da_concessao_propostas\",\n",
    "    \"lead_conversion_time_deals\",\n",
    "    \"apoio_campanha_hyundai_propostas\",\n",
    "    \"probability_____deals\",\n",
    "    \"distrito\",\n",
    "    \"apoio_hyundai_portugal_propostas\",\n",
    "    \"valor_campanha_propostas\",\n",
    "    \"desconto_total__c__apoio_de_importador__propostas\",\n",
    "    \"categoria_negocio_deals\",\n",
    "        \"distrito_contactos\",\n",
    "    \"estado_ordering_propostas\",\n",
    "    \"tasks_involved_deals\",\n",
    "    \"events_involved_deals\",\n",
    "    \"calls_involved_deals\",\n",
    "    \"desconto_total_propostas\",\n",
    "    \"pais\"                 \n",
    "   ]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4802e8ef-484f-43c4-92f0-cab74ceda634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "CHECK_COLS = [\"modelo\", \"modelo_contactos\", \"modelo_propostas\", \"modelos_deals\"]\n",
    "\n",
    "df = spark.table(TBL)\n",
    "present = [c for c in CHECK_COLS if c in df.columns]\n",
    "missing = [c for c in CHECK_COLS if c not in df.columns]\n",
    "\n",
    "row_count = df.count()\n",
    "# null counts for present columns\n",
    "nulls = (df.agg(*[F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in present])\n",
    "           .collect()[0]\n",
    "           .asDict())\n",
    "\n",
    "# build result rows\n",
    "rows = []\n",
    "for c in present:\n",
    "    n_null = int(nulls.get(c, 0))\n",
    "    rows.append((c, row_count, n_null, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"rows_total\", \"null_count\", \"null_pct\"])\n",
    "display(out.orderBy(\"column\"))\n",
    "\n",
    "if missing:\n",
    "    print(\"Not found in table (skipped):\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a960154-8fde-45c8-80b1-7bf5cb31c5fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \"modelo_contactos\",\n",
    "    \"modelo_propostas\",\n",
    "    \"modelos_deals\", \n",
    "    \"formulario\",\n",
    "    \"tasks_only_deals\",\n",
    "    \"events_only_deals\",\n",
    "    \"calls_only_deals\",\n",
    "    \"tasks_involved\",\n",
    "    \"events_involved\",\n",
    "    \"calls_involved\",\n",
    "               \n",
    "   ]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "267bcd05-ee19-4397-a8af-a75fe02e3fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "row_count = df.count()\n",
    "dtypes = dict(df.dtypes)\n",
    "\n",
    "# --- STRICT NULLS ONLY (what you asked) ---\n",
    "null_aggs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    "null_row = df.agg(*null_aggs).collect()[0].asDict()\n",
    "\n",
    "rows = []\n",
    "for c in df.columns:\n",
    "    n_null = int(null_row.get(c, 0))\n",
    "    rows.append((c, dtypes[c], n_null, row_count, f\"{(n_null/row_count)*100:.2f}%\"))\n",
    "\n",
    "out = spark.createDataFrame(rows, [\"column\", \"dtype\", \"null_count\", \"rows_total\", \"null_pct\"])\n",
    "display(out.orderBy(F.desc(\"null_pct\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d26c8f-2630-4403-9057-9c5e83d45d42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIRST TRY OF KMEANS - THE RESULTS WERE NOT GOOD\n",
    "\n",
    "# =========================\n",
    "# 1) STRATIFIED SAMPLE (multi-key, exact per-stratum counts)\n",
    "# =========================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "SRC = \"sc_gold.Features_Table\"\n",
    "OUT = \"sc_gold.Features_Table_sampled\"\n",
    "\n",
    "# Choose desired strata; only the ones present will be used\n",
    "STRATA_DESIRED = [\"is_converted\", \"lead_source\", \"pais\", \"lead_month\"]\n",
    "SAMPLE_TARGET  = 30_000   # total rows you want in the sample (adjust)\n",
    "MIN_PER_STRAT  = 200      # minimum rows per stratum (adjust)\n",
    "SEED           = 42\n",
    "\n",
    "df = spark.table(SRC)\n",
    "present_strata = [c for c in STRATA_DESIRED if c in df.columns]\n",
    "\n",
    "if present_strata:\n",
    "    # Build a stratum key (treat NULLs as their own bucket)\n",
    "    key_exprs = [F.coalesce(F.col(c).cast(\"string\"), F.lit(\"∅\")) for c in present_strata]\n",
    "    dfk = df.withColumn(\"_stratum\", F.concat_ws(\"||\", *key_exprs))\n",
    "\n",
    "    total = dfk.count()\n",
    "\n",
    "    # Per-stratum counts + target take_n\n",
    "    cnt = (dfk.groupBy(\"_stratum\").count()\n",
    "             .withColumn(\"take_n\",\n",
    "                         F.least(\n",
    "                             F.col(\"count\"),\n",
    "                             F.greatest(F.lit(MIN_PER_STRAT),\n",
    "                                        F.ceil(F.col(\"count\") * F.lit(SAMPLE_TARGET) / F.lit(total)))\n",
    "                         )))\n",
    "\n",
    "    # Exact sampling per stratum (top-k by random within each stratum)\n",
    "    win = Window.partitionBy(\"_stratum\").orderBy(F.rand(SEED))\n",
    "    sampled = (dfk.join(cnt, on=\"_stratum\", how=\"left\")\n",
    "                  .withColumn(\"_rn\", F.row_number().over(win))\n",
    "                  .where(F.col(\"_rn\") <= F.col(\"take_n\"))\n",
    "                  .drop(\"_stratum\", \"_rn\", \"count\", \"take_n\"))\n",
    "else:\n",
    "    # Fallback: simple uniform sample if no strata columns exist\n",
    "    frac = min(1.0, SAMPLE_TARGET / df.count()) if df.count() else 1.0\n",
    "    sampled = df.sample(withReplacement=False, fraction=frac, seed=SEED)\n",
    "\n",
    "# Save sample (Delta) so you can reuse consistently\n",
    "sampled.write.mode(\"overwrite\").format(\"delta\").saveAsTable(OUT)\n",
    "\n",
    "print(f\"Stratified sample saved to {OUT}. Rows: {spark.table(OUT).count()}  |  Strata used: {present_strata}\")\n",
    "#  we need to use pandas/scikit-learn for Kmeans, the Pyspark is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb483466-ff93-4d2c-b767-636d73e24b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- This is just to check the sample ---\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "SRC = \"sc_gold.Features_Table\"\n",
    "OUT = \"sc_gold.Features_Table_sampled\"\n",
    "\n",
    "df  = spark.table(SRC)\n",
    "smp = spark.table(OUT)\n",
    "\n",
    "print(\"FULL rows:\", df.count(), \"| SAMPLE rows:\", smp.count())\n",
    "display(smp.limit(20))  # quick preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c866ce20-14e1-4725-81b0-7b0341c74e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === K-Means on stratified sample (pandas + scikit) ===\n",
    "import numpy as np, pandas as pd, sklearn, time\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SAMPLE_TBL = \"sc_gold.Features_Table_sampled\"   # <- uses your stratified sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ea6cd5-1f77-4556-85bf-cb87105bb4a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#KMEANS PART 1\n",
    "\n",
    "# 1) Pull the sample to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SAMPLE_TBL}\").toPandas()\n",
    "print(\"Sample shape:\", pdf.shape)\n",
    "\n",
    "# 2) Column typing\n",
    "cat_cols = pdf.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = pdf.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "# 3) Preprocess: impute+scale numerics, impute+OHE categoricals (sparse)\n",
    "numeric_tf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "categorical_tf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    [(\"num\", numeric_tf, num_cols),\n",
    "     (\"cat\", categorical_tf, cat_cols)],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0  # keep sparse matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb784da-4192-43c9-83a9-5c1639b013b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KMEANS Part 2\n",
    "\n",
    "# 4) Transform -> sparse, then reduce dimension (fast & cluster-friendly)\n",
    "Xs = preprocess.fit_transform(pdf)  # sparse\n",
    "print(\"Encoded shape:\", Xs.shape)\n",
    "\n",
    "n_comp = min(30, max(2, Xs.shape[1]-1))  # up to 30 comps, but safe if few features\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)  # dense (n_rows x n_comp)\n",
    "\n",
    "# scale components (helps K-Means)\n",
    "X = StandardScaler().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c64a90-ff51-4c21-822c-33b708b49228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KMEANS PART 3\n",
    "\n",
    "# 5) Search k quickly (MiniBatchKMeans + sampled silhouette)\n",
    "k_range = range(2, 9)\n",
    "scores, best = [], None\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    scores.append((k, sil))\n",
    "    print(f\"k={k:2d} | silhouette={sil:.4f} | elapsed={time.perf_counter()-t1:.1f}s\")\n",
    "    if not best or sil > best[1]:\n",
    "        best = (k, sil, km)\n",
    "\n",
    "print(f\"Total search time: {time.perf_counter()-t0:.1f}s\")\n",
    "best_k, best_sil, best_model = best\n",
    "print(\"Best k:\", best_k, \"| silhouette:\", round(best_sil, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9af908-be27-48b6-9e95-5930613f1bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KMEANS PART 4\n",
    "# 6) Final fit on the sample (already fit above; reuse)\n",
    "pdf[\"cluster\"] = best_model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5d145a-58b1-4098-bcc3-68028a0b9ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  KMEANS PART 5\n",
    "# 7) Quick results\n",
    "print(\"Cluster sizes:\\n\", pdf[\"cluster\"].value_counts().sort_index())\n",
    "peek_cols = [c for c in [\"lead_source\",\"is_converted\",\"pais\",\"concessao\",\"instalacao\"] if c in pdf.columns]\n",
    "print(pdf[[*peek_cols, \"cluster\"]].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a25a37-a7d7-41de-a90c-5ff9bb8cf640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KMEANS PART 6\n",
    "# 8) Save clusters back to Delta (sampled table results)\n",
    "spark.createDataFrame(pdf).write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"sc_gold.Features_Table_sampled_kmeans\")\n",
    "print(\"Saved table: sc_gold.Features_Table_sampled_kmeans\")\n",
    "\n",
    "#Best k=2, silhouette≈0.094 → clusters are overlapping/weak. After the tweaks above (merge & bucket modelo, pick one activity style, drop near-constants, SVD to 15–20), re-run the k-scan (2–8). If silhouette stays ~0.1, consider a non-spherical method (UMAP→HDBSCAN) or k-prototypes for mixed data.\n",
    "#You don’t need a bigger sample yet—the weak silhouette is almost always a feature issue, not a sample-size issue. With 30k rows you already have more than enough signal to see structure if it’s there. I’d first tighten the feature set, then (optionally) bump the sample to ~50–60k only if you want a bit more stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344f3d25-3859-4c36-bdce-1e08adb520a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # DEAL / POST-OUTCOME LEAKAGE  (*_deals)\n",
    "    # ----------------------------\n",
    "    \"formulario_contactos\",\n",
    "    \"is_converted\",\n",
    "    \"tasks_involved\",\n",
    "    \"events_involved\",\n",
    "    \"calls_involded\",\n",
    "    \"tasks_involved_contactos\",\n",
    "    \"events_involved_contactos\",\n",
    "    \"calls_involded_contactos\",\n",
    "    \"activities_involved_contactos\",\n",
    "    \"email_opt_out_contactos\",\n",
    "    \"salutation_contactos\",\n",
    "    \"pais_contactos\",\n",
    "    \"formulario_contactos\",\n",
    "    \"caracterizacao_contactos\",\n",
    "    \"tipo_cliente_propostas\",\n",
    "      \"isv_propostas\",\n",
    "    \"pintura_propostas\",\n",
    "    \"base_tributavel_propostas\",\n",
    "    \"legalizacao_propostas\",\n",
    "    \"iuc_propostas\",\n",
    "    \"preco_total_propostas\",\n",
    "    \"iva_propostas\",\n",
    "    \"sub_total_com_desconto_propostas\",\n",
    "    \"taxa_propostas\",\n",
    "    \"total_final_propostas\",\n",
    "    \"s_g_p_u__propostas\",\n",
    "     \"tasks_only_deals\",\n",
    "    \"events_only_deals\",\n",
    "    \"calls_only_deals\",\n",
    "    \"caracterizacao_contactos\",\n",
    "        \"caracterizacao_deals\",\n",
    "    \"n__matriculas_por_associar_deals\",\n",
    "    \"tipo_cliente_negocio_deals\",\n",
    "    \"sales_cycle_duration_deals\",\n",
    "    \"calls_involved_contactos\",\n",
    "    \"concessao\",\n",
    "    \"origem_contactos\",\n",
    "    \"proposta_realizada_stage_propostas\",\n",
    "    \"valor_base_propostas\",\n",
    "    \"preco_base_propostas\",\n",
    "    \"age_tier_deals\",\n",
    "    \"num_de_propostas_deals\",\n",
    "    \"qtd_viaturas_propostas\",\n",
    "    \"qtd_viaturas_deals\",\n",
    "    \"age_in_days_deals\"\n",
    "]\n",
    " \n",
    "\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "spark.sql(\"DROP VIEW _src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d440fe5a-ec12-4037-a097-f668f37eb2d7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"overall_sales_duration_deals\":254},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757852181940}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a125d1-ca0f-45f6-a9de-ff8b3d72a4ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (pandas + scikit) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load full table to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Exclude obvious non-features (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    # timestamps/dates\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    # IDs / admin\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    # outcome (not a clustering feature)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "feature_df = pdf.drop(columns=[c for c in exclude if c in pdf.columns])\n",
    "print(\"Using features:\", feature_df.shape[1], \"| Excluded:\", len(exclude))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Version-safe OneHotEncoder config\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "# 5) ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions — check your table/filters.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 6) Encode -> SVD to 20 comps -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)              # usually sparse\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))             # keep <=20 components\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)                              # dense (n_rows x n_comp)\n",
    "X = StandardScaler().fit_transform(X)                 # scale components\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 7) k-scan (fast): MiniBatchKMeans + sampled silhouette\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.1f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.1f}s\")\n",
    "\n",
    "best_k, best_sil = max(metrics, key=lambda r: r[1])[:2]\n",
    "print(\"Best k:\", best_k, \"| silhouette:\", round(best_sil, 4))\n",
    "\n",
    "# 8) Final fit + labels\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "labels = best_model.predict(X)\n",
    "pdf[\"cluster\"] = labels\n",
    "print(\"Cluster sizes:\\n\", pdf[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0ee952c-caca-4d1d-a4d3-9aed31550908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We really needed to change the features , we tried several times to run the model with no interesting results \n",
    "# lets try to use now KMEANS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fc53f7-47c7-4867-9c08-e72e594e0d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (pandas + scikit) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load full table to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Exclude obvious non-features (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    # timestamps/dates\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    # IDs / admin\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    # outcome (not a clustering feature)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "feature_df = pdf.drop(columns=[c for c in exclude if c in pdf.columns])\n",
    "print(\"Using features:\", feature_df.shape[1], \"| Excluded:\", len(exclude))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Version-safe OneHotEncoder config\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "# 5) ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions — check your table/filters.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 6) Encode -> SVD to 20 comps -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)              # usually sparse\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))             # keep <=20 components\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)                              # dense (n_rows x n_comp)\n",
    "X = StandardScaler().fit_transform(X)                 # scale components\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 7) k-scan (fast): MiniBatchKMeans + sampled silhouette\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.1f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.1f}s\")\n",
    "\n",
    "best_k, best_sil = max(metrics, key=lambda r: r[1])[:2]\n",
    "print(\"Best k:\", best_k, \"| silhouette:\", round(best_sil, 4))\n",
    "\n",
    "# 8) Final fit + labels\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "labels = best_model.predict(X)\n",
    "pdf[\"cluster\"] = labels\n",
    "print(\"Cluster sizes:\\n\", pdf[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01c15c71-8119-4697-b4a4-cc56e2105694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Is not readable , I will try a different code to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeab804b-dd4f-4bfa-aacd-9c58bec43e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Neater K-scan summary ----------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# metrics was built in your loop as: metrics.append((k, sil, inertia, loop_time))\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\", \"silhouette\", \"inertia\", \"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "metrics_df = metrics_df.sort_values([\"rank_sil\",\"k\"]).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(metrics_df)   # Databricks\n",
    "except Exception:\n",
    "    print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a10892b-691d-4488-8a1f-d30202e6a218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Elbow (inertia) + Silhouette plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\")\n",
    "ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\")\n",
    "ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k = {best_k}  |  best silhouette = {best_sil:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d10b9bf-80da-4a09-883a-6b545fd1a84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Final fit & labels (reuse best_k) ---------------------------------------\n",
    "best_model = MiniBatchKMeans(\n",
    "    n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED\n",
    ").fit(X)\n",
    "labels = best_model.labels_\n",
    "pdf[\"cluster\"] = labels\n",
    "\n",
    "# Cluster sizes table (count + %)\n",
    "sizes = (pdf[\"cluster\"].value_counts()\n",
    "         .sort_index()\n",
    "         .rename_axis(\"cluster\")\n",
    "         .to_frame(\"count\"))\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "\n",
    "try:\n",
    "    display(sizes)\n",
    "except Exception:\n",
    "    print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa1d50a8-fd7c-4112-870d-a1469ce2f98b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Per-cluster profile (readable) ------------------------------------------\n",
    "# Choose which features to summarize (keep it concise).\n",
    "# If you want \"all\", leave as None; otherwise list important columns.\n",
    "focus_numeric = None   # e.g., [\"lead_to_deal_days\",\"overall_sales_duration_deals\",\"pvp_propostas\"]\n",
    "focus_categ   = None   # e.g., [\"modelo\",\"tipo_cliente_contactos\",\"origem_do_negocio_deals\"]\n",
    "\n",
    "num_cols_all = pdf.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols_all = pdf.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "num_cols = [c for c in (focus_numeric or num_cols_all) if c != \"cluster\"]\n",
    "cat_cols = [c for c in (focus_categ or cat_cols_all) if c != \"cluster\"]\n",
    "\n",
    "profiles = []\n",
    "\n",
    "for clu, g in pdf.groupby(\"cluster\", sort=True):\n",
    "    row = {\"cluster\": clu, \"n\": len(g), \"pct\": round(len(g)*100/len(pdf), 2)}\n",
    "    # numeric: median (use .median(skipna=True))\n",
    "    for c in num_cols[:12]:  # cap to ~12 to keep the table readable\n",
    "        row[f\"med_{c}\"] = np.nanmedian(g[c].values)\n",
    "    # categorical: top category + share\n",
    "    for c in cat_cols[:12]:\n",
    "        vc = g[c].value_counts(dropna=True, normalize=True)\n",
    "        if len(vc):\n",
    "            top_cat = vc.index[0]\n",
    "            top_pct = round(vc.iloc[0]*100, 1)\n",
    "            row[f\"top_{c}\"] = f\"{top_cat} ({top_pct}%)\"\n",
    "        else:\n",
    "            row[f\"top_{c}\"] = \"—\"\n",
    "    profiles.append(row)\n",
    "\n",
    "profile_df = pd.DataFrame(profiles).sort_values(\"cluster\").reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(profile_df)\n",
    "except Exception:\n",
    "    # wider print without truncation\n",
    "    with pd.option_context(\"display.max_columns\", None, \"display.width\", 200):\n",
    "        print(profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b04c53a-2d85-41ef-91b4-41b1bf3fdcd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Step 1: Column selection for clustering ===\n",
    "# \n",
    "# ✅ Drop columns that ended up as \"Unknown (~100%)\" in Cluster 1:\n",
    "#   - versao_propostas\n",
    "#   - combustivel_propostas\n",
    "#   - origem_do_negocio_deals\n",
    "#   - motivo_da_perda_deals\n",
    "#   - campaign_name_campaigns\n",
    "#\n",
    "# These don’t help K-Means — they act like noise and just “push” incomplete rows into one group.\n",
    "# Keep them for other analyses if needed, but not for clustering.\n",
    "#\n",
    "# ✅ Keep only business-critical features:\n",
    "#   - pvp_propostas (price)\n",
    "#   - modelo\n",
    "#   - tipo_cliente_contactos\n",
    "#   - lead_to_deal_days\n",
    "#   - overall_sales_duration_deals\n",
    "#   - lead_source\n",
    "#   - deal_speed_category (optional, since it clearly differentiates Fast vs Slow groups)\n",
    "#\n",
    "# This reduced feature set will make clustering more meaningful and interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586a0301-5933-45b0-97cc-b1e3264d6cd6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757865439491}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a281331-1081-4b32-891e-b4c57d54eda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one (excludes columns you already removed) ===\n",
    "cols_to_delete = [\n",
    "    # ----------------------------\n",
    "    # LEAKAGE (deal/post-outcome) — info not available at lead time\n",
    "    # ----------------------------\n",
    "    \"versao_propostas\",\n",
    "    \"motivo_da_perda_deals\",\n",
    "    \"origem_do_negocio_deals\",\n",
    "    \"campaign_name_campaigns\",           \n",
    "   ]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# compute keep list dynamically from live schema\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting\n",
    "\n",
    "# snapshot and recreate table with only keep_cols\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP VIEW _src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5510365d-9ebc-4b02-a24b-a9c22e58f1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I dont want to drop combustivel but is making a lot of noise , I will replace the unknown by the mode of fuel considering the model of the car but I will also create a new column witht the flag to know is unknown , this is  a good practise for ML to model learns about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ebda0d-7f5a-463a-9021-52f185145c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "col_fuel  = \"combustivel_propostas\"\n",
    "col_model = \"modelo\"\n",
    "flag_col  = f\"{col_fuel}_missing\"\n",
    "\n",
    "# Load\n",
    "df = spark.table(TBL)\n",
    "\n",
    "# --- BEFORE counts (for sanity) ---\n",
    "before_unknown = spark.table(TBL)\\\n",
    "    .filter(F.col(col_fuel).isNull() | (F.col(col_fuel) == \"Unknown\"))\\\n",
    "    .count()\n",
    "print(f\"Unknown/NULL {col_fuel} BEFORE: {before_unknown:,}\")\n",
    "\n",
    "# 1) Flag rows that were missing/Unknown originally\n",
    "df = df.withColumn(\n",
    "    flag_col,\n",
    "    F.when(F.col(col_fuel).isNull() | (F.col(col_fuel) == \"Unknown\"), 1).otherwise(0).cast(IntegerType())\n",
    ")\n",
    "\n",
    "# 2) Per-model MODE (exclude Unknown/NULL)\n",
    "w = Window.partitionBy(col_model).orderBy(F.desc(\"cnt\"))\n",
    "mode_per_model = (\n",
    "    df.filter(F.col(col_fuel).isNotNull() & (F.col(col_fuel) != \"Unknown\"))\n",
    "      .groupBy(col_model, col_fuel)\n",
    "      .count().withColumnRenamed(\"count\", \"cnt\")\n",
    "      .withColumn(\"rn\", F.row_number().over(w))\n",
    "      .filter(F.col(\"rn\") == 1)\n",
    "      .select(col_model, F.col(col_fuel).alias(\"mode_fuel\"))\n",
    ")\n",
    "\n",
    "# 3) Global mode (exclude Unknown/NULL)\n",
    "glob = (\n",
    "    df.filter(F.col(col_fuel).isNotNull() & (F.col(col_fuel) != \"Unknown\"))\n",
    "      .groupBy(col_fuel).count().orderBy(F.desc(\"count\")).first()\n",
    ")\n",
    "global_mode_val = glob[0] if glob else \"Unknown\"\n",
    "print(f\"Global mode for {col_fuel}: {global_mode_val}\")\n",
    "\n",
    "# 4) Join the per-model mode and impute:\n",
    "#    - If originally missing/Unknown AND modelo != \"Unknown\" -> use per-model mode, else global mode\n",
    "#    - If modelo == \"Unknown\", keep fuel as \"Unknown\"\n",
    "df = df.join(mode_per_model, on=col_model, how=\"left\")\n",
    "\n",
    "df = df.withColumn(\n",
    "    col_fuel,\n",
    "    F.when(\n",
    "        (F.col(flag_col) == 1) & (F.col(col_model) != \"Unknown\"),\n",
    "        F.coalesce(F.col(\"mode_fuel\"), F.lit(global_mode_val))\n",
    "    ).otherwise(F.col(col_fuel))\n",
    ").drop(\"mode_fuel\")\n",
    "\n",
    "# 5) AFTER counts\n",
    "after_unknown = df.filter(F.col(col_fuel).isNull() | (F.col(col_fuel) == \"Unknown\")).count()\n",
    "print(f\"Unknown/NULL {col_fuel} AFTER:  {after_unknown:,}\")\n",
    "print(f\"Flag column created: {flag_col}\")\n",
    "\n",
    "# 6) Overwrite the table (allow schema change)\n",
    "df.write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .saveAsTable(TBL)\n",
    "\n",
    "print(f\"✅ Overwrote {TBL} with per-model mode imputation (keeping 'Unknown' when modelo='Unknown') and '{flag_col}' flag.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c22a80-b2f3-4dfd-bb32-cabee1639c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (pandas + scikit) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load full table to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Exclude obvious non-features (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    # timestamps/dates\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    # IDs / admin\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    # outcome (not a clustering feature)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "feature_df = pdf.drop(columns=[c for c in exclude if c in pdf.columns])\n",
    "print(\"Using features:\", feature_df.shape[1], \"| Excluded:\", len(exclude))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Version-safe OneHotEncoder config\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "# 5) ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions — check your table/filters.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 6) Encode -> SVD to 20 comps -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)              # usually sparse\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))             # keep <=20 components\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)                              # dense (n_rows x n_comp)\n",
    "X = StandardScaler().fit_transform(X)                 # scale components\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 7) k-scan (fast): MiniBatchKMeans + sampled silhouette\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.1f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.1f}s\")\n",
    "\n",
    "best_k, best_sil = max(metrics, key=lambda r: r[1])[:2]\n",
    "print(\"Best k:\", best_k, \"| silhouette:\", round(best_sil, 4))\n",
    "\n",
    "# 8) Final fit + labels\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "labels = best_model.predict(X)\n",
    "pdf[\"cluster\"] = labels\n",
    "print(\"Cluster sizes:\\n\", pdf[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfc4409-2c87-4426-baec-0baf82d61728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Neater K-scan summary ----------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# metrics was built in your loop as: metrics.append((k, sil, inertia, loop_time))\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\", \"silhouette\", \"inertia\", \"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "metrics_df = metrics_df.sort_values([\"rank_sil\",\"k\"]).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(metrics_df)   # Databricks\n",
    "except Exception:\n",
    "    print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f682ed-7cc3-4b2f-99f3-8084b34d3834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Elbow (inertia) + Silhouette plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\")\n",
    "ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\")\n",
    "ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k = {best_k}  |  best silhouette = {best_sil:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e1d20a-3d4e-4b1a-9853-1e940ebb2f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Final fit & labels (reuse best_k) ---------------------------------------\n",
    "best_model = MiniBatchKMeans(\n",
    "    n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED\n",
    ").fit(X)\n",
    "labels = best_model.labels_\n",
    "pdf[\"cluster\"] = labels\n",
    "\n",
    "# Cluster sizes table (count + %)\n",
    "sizes = (pdf[\"cluster\"].value_counts()\n",
    "         .sort_index()\n",
    "         .rename_axis(\"cluster\")\n",
    "         .to_frame(\"count\"))\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "\n",
    "try:\n",
    "    display(sizes)\n",
    "except Exception:\n",
    "    print(sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa50f0fc-27e8-498b-af39-7c1d860b9025",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757865520558}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Per-cluster profile (readable) ------------------------------------------\n",
    "# Choose which features to summarize (keep it concise).\n",
    "# If you want \"all\", leave as None; otherwise list important columns.\n",
    "focus_numeric = None   # e.g., [\"lead_to_deal_days\",\"overall_sales_duration_deals\",\"pvp_propostas\"]\n",
    "focus_categ   = None   # e.g., [\"modelo\",\"tipo_cliente_contactos\",\"origem_do_negocio_deals\"]\n",
    "\n",
    "num_cols_all = pdf.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols_all = pdf.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "num_cols = [c for c in (focus_numeric or num_cols_all) if c != \"cluster\"]\n",
    "cat_cols = [c for c in (focus_categ or cat_cols_all) if c != \"cluster\"]\n",
    "\n",
    "profiles = []\n",
    "\n",
    "for clu, g in pdf.groupby(\"cluster\", sort=True):\n",
    "    row = {\"cluster\": clu, \"n\": len(g), \"pct\": round(len(g)*100/len(pdf), 2)}\n",
    "    # numeric: median (use .median(skipna=True))\n",
    "    for c in num_cols[:12]:  # cap to ~12 to keep the table readable\n",
    "        row[f\"med_{c}\"] = np.nanmedian(g[c].values)\n",
    "    # categorical: top category + share\n",
    "    for c in cat_cols[:12]:\n",
    "        vc = g[c].value_counts(dropna=True, normalize=True)\n",
    "        if len(vc):\n",
    "            top_cat = vc.index[0]\n",
    "            top_pct = round(vc.iloc[0]*100, 1)\n",
    "            row[f\"top_{c}\"] = f\"{top_cat} ({top_pct}%)\"\n",
    "        else:\n",
    "            row[f\"top_{c}\"] = \"—\"\n",
    "    profiles.append(row)\n",
    "\n",
    "profile_df = pd.DataFrame(profiles).sort_values(\"cluster\").reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(profile_df)\n",
    "except Exception:\n",
    "    # wider print without truncation\n",
    "    with pd.option_context(\"display.max_columns\", None, \"display.width\", 200):\n",
    "        print(profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e927426a-903e-44ae-80b0-cf9cce826439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "FLAG_COL = \"deal_before_lead\"\n",
    "\n",
    "df = spark.table(TBL)\n",
    "\n",
    "# Count how many flagged (==1)\n",
    "flag_count = df.filter(F.col(FLAG_COL) == 1).count()\n",
    "total_count = df.count()\n",
    "\n",
    "print(f\"Rows with {FLAG_COL} = 1 : {flag_count:,} out of {total_count:,} ({flag_count*100/total_count:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71afa380-c531-4700-b5d9-fd5158fe419f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# são apenas 7,95% então vamos prosseguir com o uso da moda nestas colunas deal_before_lead .. eu deixei a negativo mas está a fazer noise e os dados começam a não fazer sentido \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "TBL       = \"sc_gold.Features_Table\"\n",
    "COL_DAYS  = \"lead_to_deal_days\"\n",
    "FLAG_COL  = \"deal_before_lead\"   # <-- if your flag column has a different name, change this\n",
    "\n",
    "# Load\n",
    "df = spark.table(TBL)\n",
    "\n",
    "# Sanity: how many negatives/flagged before\n",
    "neg_before  = df.filter(F.col(COL_DAYS) < 0).count()\n",
    "flag_before = df.filter(F.col(FLAG_COL) == 1).count()\n",
    "print(f\"Negatives BEFORE: {neg_before:,}  |  Flagged BEFORE: {flag_before:,}\")\n",
    "\n",
    "# 1) Compute mode from valid (>=0) non-null values\n",
    "mode_row = (\n",
    "    df.filter(F.col(COL_DAYS).isNotNull() & (F.col(COL_DAYS) >= 0))\n",
    "      .groupBy(COL_DAYS).count()\n",
    "      .orderBy(F.desc(\"count\"))\n",
    "      .first()\n",
    ")\n",
    "mode_val = int(mode_row[0]) if mode_row else 0\n",
    "print(f\"Mode for {COL_DAYS}: {mode_val}\")\n",
    "\n",
    "# 2) Replace where flagged OR negative, else keep original\n",
    "df = df.withColumn(\n",
    "    COL_DAYS,\n",
    "    F.when((F.col(FLAG_COL) == 1) | (F.col(COL_DAYS) < 0), F.lit(mode_val))\n",
    "     .otherwise(F.col(COL_DAYS))\n",
    ").withColumn(COL_DAYS, F.col(COL_DAYS).cast(IntegerType()))  # keep dtype consistent\n",
    "\n",
    "# Sanity: check after\n",
    "neg_after  = df.filter(F.col(COL_DAYS) < 0).count()\n",
    "print(f\"Negatives AFTER:  {neg_after:,}\")\n",
    "\n",
    "# 3) Overwrite table (no schema change)\n",
    "df.write.mode(\"overwrite\").saveAsTable(TBL)\n",
    "print(f\"✅ Overwrote {TBL} with mode-imputed {COL_DAYS} for rows flagged by '{FLAG_COL}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a126b74-45d4-47f9-a9c9-c7980fc93d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KMEANS AGAIN WITH ALL THOSE CHANGES\n",
    "\n",
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (pandas + scikit) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load full table to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Exclude obvious non-features (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    # timestamps/dates\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    # IDs / admin\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    # outcome (not a clustering feature)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "feature_df = pdf.drop(columns=[c for c in exclude if c in pdf.columns])\n",
    "print(\"Using features:\", feature_df.shape[1], \"| Excluded:\", len(exclude))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Version-safe OneHotEncoder config\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "# 5) ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions — check your table/filters.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 6) Encode -> SVD to 20 comps -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)              # usually sparse\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))             # keep <=20 components\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)                              # dense (n_rows x n_comp)\n",
    "X = StandardScaler().fit_transform(X)                 # scale components\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 7) k-scan (fast): MiniBatchKMeans + sampled silhouette\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.1f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.1f}s\")\n",
    "\n",
    "best_k, best_sil = max(metrics, key=lambda r: r[1])[:2]\n",
    "print(\"Best k:\", best_k, \"| silhouette:\", round(best_sil, 4))\n",
    "\n",
    "# 8) Final fit + labels\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "labels = best_model.predict(X)\n",
    "pdf[\"cluster\"] = labels\n",
    "print(\"Cluster sizes:\\n\", pdf[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eff1a58-665b-4460-9b4f-3213190525f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Neater K-scan summary ----------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# metrics was built in your loop as: metrics.append((k, sil, inertia, loop_time))\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\", \"silhouette\", \"inertia\", \"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "metrics_df = metrics_df.sort_values([\"rank_sil\",\"k\"]).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(metrics_df)   # Databricks\n",
    "except Exception:\n",
    "    print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ac79f4-f57e-4224-9341-6f218f31eaa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Elbow (inertia) + Silhouette plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\")\n",
    "ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\")\n",
    "ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k = {best_k}  |  best silhouette = {best_sil:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b935aeb-8a91-4d85-ab38-1b95b0ce4942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Final fit & labels (reuse best_k) ---------------------------------------\n",
    "best_model = MiniBatchKMeans(\n",
    "    n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED\n",
    ").fit(X)\n",
    "labels = best_model.labels_\n",
    "pdf[\"cluster\"] = labels\n",
    "\n",
    "# Cluster sizes table (count + %)\n",
    "sizes = (pdf[\"cluster\"].value_counts()\n",
    "         .sort_index()\n",
    "         .rename_axis(\"cluster\")\n",
    "         .to_frame(\"count\"))\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "\n",
    "try:\n",
    "    display(sizes)\n",
    "except Exception:\n",
    "    print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec963c4-c18b-4505-8baf-f2bdb3d8366b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Per-cluster profile (readable) ------------------------------------------\n",
    "# Choose which features to summarize (keep it concise).\n",
    "# If you want \"all\", leave as None; otherwise list important columns.\n",
    "focus_numeric = None   # e.g., [\"lead_to_deal_days\",\"overall_sales_duration_deals\",\"pvp_propostas\"]\n",
    "focus_categ   = None   # e.g., [\"modelo\",\"tipo_cliente_contactos\",\"origem_do_negocio_deals\"]\n",
    "\n",
    "num_cols_all = pdf.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols_all = pdf.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "num_cols = [c for c in (focus_numeric or num_cols_all) if c != \"cluster\"]\n",
    "cat_cols = [c for c in (focus_categ or cat_cols_all) if c != \"cluster\"]\n",
    "\n",
    "profiles = []\n",
    "\n",
    "for clu, g in pdf.groupby(\"cluster\", sort=True):\n",
    "    row = {\"cluster\": clu, \"n\": len(g), \"pct\": round(len(g)*100/len(pdf), 2)}\n",
    "    # numeric: median (use .median(skipna=True))\n",
    "    for c in num_cols[:12]:  # cap to ~12 to keep the table readable\n",
    "        row[f\"med_{c}\"] = np.nanmedian(g[c].values)\n",
    "    # categorical: top category + share\n",
    "    for c in cat_cols[:12]:\n",
    "        vc = g[c].value_counts(dropna=True, normalize=True)\n",
    "        if len(vc):\n",
    "            top_cat = vc.index[0]\n",
    "            top_pct = round(vc.iloc[0]*100, 1)\n",
    "            row[f\"top_{c}\"] = f\"{top_cat} ({top_pct}%)\"\n",
    "        else:\n",
    "            row[f\"top_{c}\"] = \"—\"\n",
    "    profiles.append(row)\n",
    "\n",
    "profile_df = pd.DataFrame(profiles).sort_values(\"cluster\").reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(profile_df)\n",
    "except Exception:\n",
    "    # wider print without truncation\n",
    "    with pd.option_context(\"display.max_columns\", None, \"display.width\", 200):\n",
    "        print(profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f77c56-1e7d-4e2b-9812-784b4560a39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === KMEANS AGAIN WITH ALL THOSE CHANGES (exclude combustivel* only for clustering) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load full table to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Exclude obvious non-features (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    # timestamps/dates\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    # IDs / admin\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    # outcomes\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "# --- EXCLUDE ONLY FOR CLUSTERING (keep in pdf, drop from features) ---\n",
    "exclude_for_clustering = {\"combustivel_propostas\", \"combustivel_propostas_missing\"}\n",
    "\n",
    "feature_df = pdf.drop(columns=[c for c in (exclude | exclude_for_clustering) if c in pdf.columns])\n",
    "print(\"Using features:\", feature_df.shape[1], \"| Excluded:\", len(exclude), \"| Dropped-for-clustering:\", exclude_for_clustering & set(pdf.columns))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Version-safe OneHotEncoder config\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if version.parse(sklearn.__version__) >= version.parse(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "# 5) ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions — check your table/filters.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 6) Encode -> SVD to 20 comps -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)              # usually sparse\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))             # keep <=20 components\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)                              # dense (n_rows x n_comp)\n",
    "X = StandardScaler().fit_transform(X)                 # scale components\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 7) k-scan (fast): MiniBatchKMeans + sampled silhouette\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0c2eeb-f575-481e-a699-36dd7e620722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Robust k-scan (2..8) + safe display ===\n",
    "import time, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    try:\n",
    "        km = MiniBatchKMeans(\n",
    "            n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED\n",
    "        )\n",
    "        labels = km.fit_predict(X)\n",
    "\n",
    "        # guard: sometimes k-means collapses to 1 cluster or all labels nan\n",
    "        uniq = np.unique(labels)\n",
    "        if uniq.size < 2:\n",
    "            print(f\"⚠️  Skipping k={k}: only {uniq.size} unique label(s).\")\n",
    "            continue\n",
    "\n",
    "        sil = silhouette_score(\n",
    "            X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED\n",
    "        )\n",
    "        loop_s = time.perf_counter() - t1\n",
    "        metrics.append((k, float(sil), float(km.inertia_), float(loop_s)))\n",
    "        print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={loop_s:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed k={k}: {e}\")\n",
    "\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "# ---- Build metrics table (if any) ----\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"])\n",
    "if metrics_df.empty:\n",
    "    raise RuntimeError(\n",
    "        \"No valid k results collected. Check earlier messages for 'Failed k=' or 'Skipping k=' \"\n",
    "        \"and verify that X has > 1 sample and reasonable variance.\"\n",
    "    )\n",
    "\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "metrics_df = metrics_df.sort_values([\"rank_sil\",\"k\"]).reset_index(drop=True)\n",
    "print(\"\\nK-scan metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# ---- Plots ----\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\"); ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\"); ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "# ---- Pick best k safely ----\n",
    "best_row = metrics_df.loc[metrics_df[\"silhouette\"].idxmax()]\n",
    "best_k, best_sil = int(best_row[\"k\"]), float(best_row[\"silhouette\"])\n",
    "print(f\"Best k = {best_k}  |  best silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# ---- Final fit + sizes ----\n",
    "best_model = MiniBatchKMeans(\n",
    "    n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED\n",
    ").fit(X)\n",
    "pdf[\"cluster\"] = best_model.labels_\n",
    "\n",
    "sizes = (pdf[\"cluster\"].value_counts()\n",
    "         .sort_index()\n",
    "         .rename_axis(\"cluster\")\n",
    "         .to_frame(\"count\"))\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(sizes.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe69191-4a28-464f-886b-00d340d673b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to try to understand the 8% \n",
    "\n",
    "# ==== Cluster profiling for current K-Means result (2 clusters) ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "assert \"cluster\" in pdf.columns, \"Run K-Means first to create pdf['cluster']\"\n",
    "\n",
    "# 1) Sizes\n",
    "sizes = (pdf[\"cluster\"].value_counts()\n",
    "         .sort_index()\n",
    "         .rename_axis(\"cluster\")\n",
    "         .to_frame(\"count\"))\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(sizes.to_string())\n",
    "spark.createDataFrame(sizes.reset_index()).show(truncate=False)\n",
    "\n",
    "# 2) Choose columns to summarize (add/remove as you like)\n",
    "focus_numeric = [\n",
    "    \"pvp_propostas\",\n",
    "    \"overall_sales_duration_deals\",\n",
    "    \"lead_to_deal_days\",\n",
    "    \"proposals_per_vehicle\"\n",
    "]\n",
    "focus_categ = [\n",
    "    \"modelo\",\n",
    "    \"lead_source\",\n",
    "    \"tipo_cliente_contactos\",\n",
    "    \"deal_speed_category\",\n",
    "    \"Season\",\n",
    "    # we excluded fuel from clustering, but we *do* profile it:\n",
    "    \"combustivel_propostas\",\n",
    "    # optionally:\n",
    "    \"campaign_name_campaigns\",\n",
    "    \"origem_do_negocio_deals\",\n",
    "    \"motivo_da_perda_deals\"\n",
    "]\n",
    "\n",
    "num_cols = [c for c in focus_numeric if c in pdf.columns]\n",
    "cat_cols = [c for c in focus_categ   if c in pdf.columns]\n",
    "\n",
    "# 3) Per-cluster profile (medians for numeric, top category + % for categorical)\n",
    "profiles = []\n",
    "for clu, g in pdf.groupby(\"cluster\", sort=True):\n",
    "    row = {\"cluster\": int(clu), \"n\": len(g), \"pct\": round(len(g)*100/len(pdf), 2)}\n",
    "    for c in num_cols:\n",
    "        row[f\"med_{c}\"] = float(np.nanmedian(g[c].values))\n",
    "    for c in cat_cols:\n",
    "        vc = g[c].value_counts(dropna=True, normalize=True)\n",
    "        row[f\"top_{c}\"] = f\"{vc.index[0]} ({round(vc.iloc[0]*100,1)}%)\" if len(vc) else \"—\"\n",
    "    profiles.append(row)\n",
    "\n",
    "profile_df = pd.DataFrame(profiles).sort_values(\"cluster\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nCluster profile (medians + top categories):\")\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", 220):\n",
    "    print(profile_df)\n",
    "\n",
    "spark.createDataFrame(profile_df).show(truncate=False)\n",
    "\n",
    "# 4) Side-by-side numeric comparison (delta = Cluster1 − Cluster0)\n",
    "if set([0,1]).issubset(set(pdf[\"cluster\"].unique())) and num_cols:\n",
    "    piv = (pdf[[\"cluster\"] + num_cols]\n",
    "           .groupby(\"cluster\").median(numeric_only=True)\n",
    "           .rename_axis(\"cluster\")\n",
    "           .sort_index())\n",
    "    piv_delta = (piv.loc[1] - piv.loc[0]).to_frame(\"delta_c1_minus_c0\").round(2)\n",
    "    print(\"\\nNumeric medians by cluster:\")\n",
    "    print(piv.round(2).to_string())\n",
    "    print(\"\\nDelta (Cluster 1 − Cluster 0):\")\n",
    "    print(piv_delta.to_string())\n",
    "    spark.createDataFrame(piv.reset_index()).show(truncate=False)\n",
    "    spark.createDataFrame(piv_delta.reset_index().rename(columns={\"index\":\"feature\"})).show(truncate=False)\n",
    "\n",
    "# 5) Categorical lift: which categories are most over-represented in Cluster 1 vs 0\n",
    "def top_lift(col, topn=8):\n",
    "    # normalized frequency per cluster\n",
    "    tab = (pdf.groupby(\"cluster\")[col]\n",
    "             .value_counts(normalize=True, dropna=True)\n",
    "             .rename(\"pct\")\n",
    "             .reset_index())\n",
    "    c0 = tab[tab[\"cluster\"]==0][[\"level_1\",\"pct\"]].set_index(\"level_1\")\n",
    "    c1 = tab[tab[\"cluster\"]==1][[\"level_1\",\"pct\"]].set_index(\"level_1\")\n",
    "    join = c0.join(c1, how=\"outer\", lsuffix=\"_c0\", rsuffix=\"_c1\").fillna(0.0)\n",
    "    join[\"lift_c1_vs_c0\"] = (join[\"pct_c1\"] + 1e-9) / (join[\"pct_c0\"] + 1e-9)\n",
    "    return (join.sort_values(\"lift_c1_vs_c0\", ascending=False)\n",
    "                .head(topn)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"level_1\": col}))\n",
    "\n",
    "for c in cat_cols:\n",
    "    try:\n",
    "        lift = top_lift(c, topn=8)\n",
    "        print(f\"\\nTop lift in Cluster 1 vs 0 — {c}:\")\n",
    "        print(lift.to_string(index=False, float_format=lambda x: f\"{x:.3f}\"))\n",
    "        spark.createDataFrame(lift).show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"(skip lift for {c}: {e})\")\n",
    "\n",
    "# 6) (Optional) persist for BI/SQL\n",
    "# spark.createDataFrame(profile_df).write.mode(\"overwrite\").saveAsTable(\"sc_gold.cluster_profile_latest\")\n",
    "# spark.createDataFrame(sizes.reset_index()).write.mode(\"overwrite\").saveAsTable(\"sc_gold.cluster_sizes_latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68626729-0ef1-4aa5-a2c3-c4b6b2b41380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Replace your cols_to_delete with this one ===\n",
    "#The problem with lead_to_deal_days\n",
    "#40% nulls → already weak, imputation risks creating artificial patterns.\n",
    "#Negative values → logically impossible (deal before lead). That indicates data quality issues in the source.\n",
    "#High correlation with overall_sales_duration_deals → they’re both \"time-based\" and may overlap in meaning.\n",
    "#Business interpretation issue → if it doesn’t make sense to the business, clusters built on it won’t be actionable.\n",
    "\n",
    "\n",
    "\n",
    "cols_to_delete = [\n",
    "    # --- Columns you no longer want ---\n",
    "    \"combustivel_propostas\",\n",
    "    \"lead_to_deal_days\",\n",
    "    \"deal_before_lead\",\n",
    "    \"lead_to_deal_days_missing\",\n",
    "    \"combustivel_propostas_missing\",\n",
    "    \"deal_speed_category\",\n",
    "    \"modelo\",\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "src_tbl = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Get all columns from current table\n",
    "all_cols = spark.table(src_tbl).columns\n",
    "\n",
    "# 2) Keep only those not in cols_to_delete\n",
    "keep_cols = [c for c in all_cols if c not in cols_to_delete]\n",
    "keep_sql = \", \".join([f\"`{c}`\" for c in keep_cols])  # safe quoting for names\n",
    "\n",
    "# 3) Snapshot table into a temp view\n",
    "spark.sql(f\"CREATE OR REPLACE TEMP VIEW _src AS SELECT * FROM {src_tbl}\")\n",
    "\n",
    "# 4) Recreate the table with only kept columns\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {src_tbl}\n",
    "USING DELTA AS\n",
    "SELECT {keep_sql} FROM _src\n",
    "\"\"\")\n",
    "\n",
    "# 5) Drop temp view\n",
    "spark.sql(\"DROP VIEW _src\")\n",
    "\n",
    "print(f\"✅ Table {src_tbl} overwritten with {len(keep_cols)} columns (dropped {len(cols_to_delete)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95bdc48-5a9a-411b-8173-c6d6f686d4fa",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757877028978}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql          \n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308a9c1a-c8cb-4cd5-999e-73146636727b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (pandas + scikit) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load full table to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Exclude obvious non-features (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    # timestamps/dates\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    # IDs / admin\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    # outcome (not a clustering feature)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "feature_df = pdf.drop(columns=[c for c in exclude if c in pdf.columns])\n",
    "print(\"Using features:\", feature_df.shape[1], \"| Excluded:\", len(exclude))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Version-safe OneHotEncoder config\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "# 5) ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions — check your table/filters.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 6) Encode -> SVD to 20 comps -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)              # usually sparse\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))             # keep <=20 components\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)                              # dense (n_rows x n_comp)\n",
    "X = StandardScaler().fit_transform(X)                 # scale components\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 7) k-scan (fast): MiniBatchKMeans + sampled silhouette\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.1f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.1f}s\")\n",
    "\n",
    "best_k, best_sil = max(metrics, key=lambda r: r[1])[:2]\n",
    "print(\"Best k:\", best_k, \"| silhouette:\", round(best_sil, 4))\n",
    "\n",
    "# 8) Final fit + labels\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "labels = best_model.predict(X)\n",
    "pdf[\"cluster\"] = labels\n",
    "print(\"Cluster sizes:\\n\", pdf[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e27b78-1226-45db-bd17-8d5814260706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Neater K-scan summary ----------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# metrics was built in your loop as: metrics.append((k, sil, inertia, loop_time))\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\", \"silhouette\", \"inertia\", \"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "metrics_df = metrics_df.sort_values([\"rank_sil\",\"k\"]).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(metrics_df)   # Databricks\n",
    "except Exception:\n",
    "    print(metrics_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71988b6-8273-4054-a33f-e3a4fb1d7be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Elbow (inertia) + Silhouette plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\")\n",
    "ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\")\n",
    "ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k = {best_k}  |  best silhouette = {best_sil:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9edd4199-cd14-4433-a3bf-0f70e9e3d246",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757877107047}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Choose which features to summarize (keep it concise).\n",
    "# If you want \"all\", leave as None; otherwise list important columns.\n",
    "focus_numeric = None   # e.g., [\"lead_to_deal_days\",\"overall_sales_duration_deals\",\"pvp_propostas\"]\n",
    "focus_categ   = None   # e.g., [\"modelo\",\"tipo_cliente_contactos\",\"origem_do_negocio_deals\"]\n",
    "\n",
    "num_cols_all = pdf.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols_all = pdf.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "num_cols = [c for c in (focus_numeric or num_cols_all) if c != \"cluster\"]\n",
    "cat_cols = [c for c in (focus_categ or cat_cols_all) if c != \"cluster\"]\n",
    "\n",
    "profiles = []\n",
    "\n",
    "for clu, g in pdf.groupby(\"cluster\", sort=True):\n",
    "    row = {\"cluster\": clu, \"n\": len(g), \"pct\": round(len(g)*100/len(pdf), 2)}\n",
    "    # numeric: median (use .median(skipna=True))\n",
    "    for c in num_cols[:12]:  # cap to ~12 to keep the table readable\n",
    "        row[f\"med_{c}\"] = np.nanmedian(g[c].values)\n",
    "    # categorical: top category + share\n",
    "    for c in cat_cols[:12]:\n",
    "        vc = g[c].value_counts(dropna=True, normalize=True)\n",
    "        if len(vc):\n",
    "            top_cat = vc.index[0]\n",
    "            top_pct = round(vc.iloc[0]*100, 1)\n",
    "            row[f\"top_{c}\"] = f\"{top_cat} ({top_pct}%)\"\n",
    "        else:\n",
    "            row[f\"top_{c}\"] = \"—\"\n",
    "    profiles.append(row)\n",
    "\n",
    "profile_df = pd.DataFrame(profiles).sort_values(\"cluster\").reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(profile_df)\n",
    "except Exception:\n",
    "    # wider print without truncation\n",
    "    with pd.option_context(\"display.max_columns\", None, \"display.width\", 200):\n",
    "        print(profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402d717f-58f2-41a8-8a32-ef339bbb2dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "col = \"concessao_contactos\"\n",
    "\n",
    "df.select(\n",
    "    F.count(\"*\").alias(\"total_rows\"),\n",
    "    F.sum(F.when(F.col(col).isNull(), 1).otherwise(0)).alias(\"null_count\")\n",
    ").withColumn(\n",
    "    \"null_pct\", (F.col(\"null_count\") / F.col(\"total_rows\") * 100).cast(\"double\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "084eed55-b187-4186-9397-41526132f63a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "col = \"concessao_contactos\"\n",
    "\n",
    "# 1) Find mode (most frequent non-null value)\n",
    "mode_val = (\n",
    "    df.filter(F.col(col).isNotNull())\n",
    "      .groupBy(col).count()\n",
    "      .orderBy(F.desc(\"count\"))\n",
    "      .first()\n",
    ")\n",
    "mode_str = mode_val[0] if mode_val else None\n",
    "print(f\"Mode for {col}: {mode_str}\")\n",
    "\n",
    "# 2) Replace nulls with mode\n",
    "if mode_str is not None:\n",
    "    df = df.withColumn(\n",
    "        col,\n",
    "        F.when(F.col(col).isNull(), F.lit(mode_str)).otherwise(F.col(col))\n",
    "    )\n",
    "\n",
    "# 3) Overwrite the table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TBL)\n",
    "print(f\"✅ Overwrote {TBL} with nulls in '{col}' replaced by mode = '{mode_str}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c19801-0f0b-4406-9fce-a4e54b0286ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TBL = \"sc_gold.Features_Table\"\n",
    "df = spark.table(TBL)\n",
    "\n",
    "df.select(\"modelo\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ece12c6-0f78-4240-a3de-b7160c3431b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(\"modelo\") \\\n",
    "  .count() \\\n",
    "  .orderBy(F.desc(\"count\")) \\\n",
    "  .show(50, truncate=False)  # change 50 to see more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a666df10-da45-4f3c-acde-0f0a67d5e812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "TBL = \"sc_gold.Features_Table\"\n",
    "df  = spark.table(TBL)\n",
    "\n",
    "# --- Normalize once for pattern matching ---\n",
    "m = F.lower(F.regexp_replace(F.trim(F.col(\"modelo\")), r\"\\s+\", \" \"))\n",
    "is_null_or_unknown = m.isNull() | (m == \"\") | (m == \"unknown\")\n",
    "\n",
    "# --- Regex patterns (no \\b; use lookarounds to avoid Spark parser issues) ---\n",
    "# Put bare \"ioniq\" into EV, but ONLY if not HEV/PHEV (handled via precedence below)\n",
    "bev_pat     = r\"(^ioniq$|nexo|ioniq\\s*5(?:\\s*n)?|ioniq\\s*6|ioniq\\s*ev|kona\\s*electric|kauai\\s*ev|(?<!\\w)ev(?!\\w)|eléctrico|elétrico|electric|battery)\"\n",
    "phev_pat    = r\"(?<!\\w)phev(?!\\w)|plug[-\\s]*in\"\n",
    "hev_pat     = r\"(?<!\\w)hev(?!\\w)|híbrido|hibrido|hybrid\"\n",
    "van_pat     = r\"(?:^|[\\s-])h1(?:$|\\s)|h[-\\s]?1\\b|h350|\\bvan\\b|\\blugares\\b\"\n",
    "suv_pat     = r\"(tucson|santa\\s*fe|kauai|kona(?!\\s*electric)|bayon|ix35|palisade|creta|venue)\"\n",
    "compact_pat = r\"(i10|i20|i30|i40|fastback)\"\n",
    "\n",
    "# --- Build the new column with clear precedence ---\n",
    "# IMPORTANT: PHEV and HEV FIRST, then EV — so \"Ioniq HEV\" maps to HEV, while bare \"Ioniq\" maps to EV.\n",
    "df = df.withColumn(\n",
    "    \"modelo_segment\",\n",
    "    F.when(is_null_or_unknown, F.lit(\"Unknown\"))\n",
    "     .when(F.regexp_like(m, F.lit(phev_pat)),     F.lit(\"PHEV\"))\n",
    "     .when(F.regexp_like(m, F.lit(hev_pat)),      F.lit(\"HEV\"))\n",
    "     .when(F.regexp_like(m, F.lit(bev_pat)),      F.lit(\"EV\"))\n",
    "     .when(F.regexp_like(m, F.lit(van_pat)),      F.lit(\"Commercial/Van\"))\n",
    "     .when(F.regexp_like(m, F.lit(suv_pat)),      F.lit(\"SUV\"))\n",
    "     .when(F.regexp_like(m, F.lit(compact_pat)),  F.lit(\"Compact/City\"))\n",
    "     .otherwise(F.lit(\"Other\"))\n",
    ")\n",
    "\n",
    "# --- Distribution with a proper window (percentages) ---\n",
    "w_all = Window.orderBy(F.lit(1)).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "dist = (df.groupBy(\"modelo_segment\")\n",
    "          .count()\n",
    "          .withColumn(\"total\", F.sum(\"count\").over(w_all))\n",
    "          .withColumn(\"pct\", (F.col(\"count\") / F.col(\"total\") * 100))\n",
    "          .drop(\"total\")\n",
    "          .orderBy(F.desc(\"count\")))\n",
    "\n",
    "print(\"Modelo segment distribution:\")\n",
    "dist.show(truncate=False)\n",
    "\n",
    "# --- Sanity check: show how top raw model strings mapped ---\n",
    "print(\"Top 30 (modelo, modelo_segment) pairs by frequency:\")\n",
    "(df.groupBy(\"modelo\", \"modelo_segment\")\n",
    "   .count()\n",
    "   .orderBy(F.desc(\"count\"))\n",
    "   .show(30, truncate=False))\n",
    "\n",
    "# --- Persist (adds the new column to the table) ---\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TBL)\n",
    "print(\"✅ Added 'modelo_segment' (with plain 'Ioniq' → EV and proper precedence) and overwrote table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2c0d74-637d-4390-a7f0-e2c4a09c2c23",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757880605764}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql          \n",
    "SELECT * FROM sc_gold.Features_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8bd862-25af-4014-a0f8-645528b3afb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === KMEANS AGAIN (exclude modelo* only for clustering) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load full table to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Exclude obvious non-features (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    # timestamps/dates\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    # IDs / admin\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    # outcomes\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "# --- EXCLUDE ONLY FOR CLUSTERING (keep in pdf, drop from features) ---\n",
    "# Swap from combustivel_* to modelo* as requested\n",
    "exclude_for_clustering = {\"modelo\", \"modelo_segment\"}  # only dropped from features if present\n",
    "\n",
    "feature_df = pdf.drop(columns=[c for c in (exclude | exclude_for_clustering) if c in pdf.columns])\n",
    "dropped = (exclude_for_clustering & set(pdf.columns))\n",
    "print(\"Using features:\", feature_df.shape[1],\n",
    "      \"| Excluded(always):\", len(exclude),\n",
    "      \"| Dropped-for-clustering:\", dropped if dropped else \"{}\")\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Version-safe OneHotEncoder config\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if version.parse(sklearn.__version__) >= version.parse(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "# 5) ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions — check your table/filters.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 6) Encode -> SVD to 20 comps -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)              # usually sparse\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))             # keep <=20 components\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)                              # dense (n_rows x n_comp)\n",
    "X = StandardScaler().fit_transform(X)                 # scale components\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 7) k-scan (fast): MiniBatchKMeans + sampled silhouette\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.2f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "# 8) Plot + pick best k\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "print(\"\\nK-scan metrics:\")\n",
    "print(metrics_df.sort_values([\"rank_sil\",\"k\"]).to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\")\n",
    "ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\")\n",
    "ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "best_k = int(metrics_df.sort_values([\"rank_sil\",\"k\"]).iloc[0][\"k\"])\n",
    "best_sil = float(metrics_df.sort_values([\"rank_sil\",\"k\"]).iloc[0][\"silhouette\"])\n",
    "print(f\"\\nBest k = {best_k}  |  best silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# 9) Final fit + labels and basic cluster sizes\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "labels = best_model.labels_\n",
    "pdf[\"cluster\"] = labels\n",
    "sizes = pdf[\"cluster\"].value_counts().sort_index().to_frame(\"count\")\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "print(\"\\nCluster sizes:\\n\", sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a08a7e1-1737-4af0-9436-efa02671fd51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (pandas + scikit) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load full table to pandas\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Exclude obvious non-features (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    # timestamps/dates\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    # IDs / admin\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    # outcome (not a clustering feature)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "feature_df = pdf.drop(columns=[c for c in exclude if c in pdf.columns])\n",
    "print(\"Using features:\", feature_df.shape[1], \"| Excluded:\", len(exclude))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Version-safe OneHotEncoder config\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "# 5) ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions — check your table/filters.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 6) Encode -> SVD to 20 comps -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)              # usually sparse\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))             # keep <=20 components\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)                              # dense (n_rows x n_comp)\n",
    "X = StandardScaler().fit_transform(X)                 # scale components\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 7) k-scan (fast): MiniBatchKMeans + sampled silhouette\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.1f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.1f}s\")\n",
    "\n",
    "best_k, best_sil = max(metrics, key=lambda r: r[1])[:2]\n",
    "print(\"Best k:\", best_k, \"| silhouette:\", round(best_sil, 4))\n",
    "\n",
    "# 8) Final fit + labels\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "labels = best_model.predict(X)\n",
    "pdf[\"cluster\"] = labels\n",
    "print(\"Cluster sizes:\\n\", pdf[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f325d6e7-c9d9-4449-b258-e768b115b14d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Neater K-scan summary ----------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# metrics was built in your loop as: metrics.append((k, sil, inertia, loop_time))\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\", \"silhouette\", \"inertia\", \"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "metrics_df = metrics_df.sort_values([\"rank_sil\",\"k\"]).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    display(metrics_df)   # Databricks\n",
    "except Exception:\n",
    "    print(metrics_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385df836-b459-4f77-bf61-92ea3692a57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (modelo* excluded for clustering) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load table\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Generic exclusions (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "# --- IMPORTANT: drop only for clustering (keep for profiling) ---\n",
    "exclude_for_clustering = {\"modelo\", \"modelo_segment\"}\n",
    "drop_cols = [c for c in (exclude | exclude_for_clustering) if c in pdf.columns]\n",
    "feature_df = pdf.drop(columns=drop_cols)\n",
    "\n",
    "print(\"Using features:\", feature_df.shape[1])\n",
    "print(\"Excluded(always):\", len(exclude))\n",
    "print(\"Dropped-for-clustering:\", sorted(exclude_for_clustering & set(pdf.columns)))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Encoders\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 5) Encode -> SVD(<=20) -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 6) k-scan\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.2f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "# 7) Summarize + plots\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "print(\"\\nK-scan metrics:\")\n",
    "print(metrics_df.sort_values([\"rank_sil\",\"k\"]).to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\"); ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\"); ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "best_row = metrics_df.sort_values([\"rank_sil\",\"k\"]).iloc[0]\n",
    "best_k = int(best_row[\"k\"])\n",
    "best_sil = float(best_row[\"silhouette\"])\n",
    "print(f\"\\nBest k = {best_k}  |  best silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# 8) Final fit + cluster sizes\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = best_model.labels_\n",
    "sizes = pdf[\"cluster\"].value_counts().sort_index().to_frame(\"count\")\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "print(\"\\nCluster sizes:\\n\", sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c852afd5-1444-44ad-890e-d96a03ea85bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (concessao_contactos excluded for clustering) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load table\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Generic exclusions (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "# --- IMPORTANT: drop only for clustering (keep for profiling) ---\n",
    "exclude_for_clustering = {\"concessao_contactos\"}   # <— your change\n",
    "drop_cols = [c for c in (exclude | exclude_for_clustering) if c in pdf.columns]\n",
    "feature_df = pdf.drop(columns=drop_cols)\n",
    "\n",
    "print(\"Using features:\", feature_df.shape[1])\n",
    "print(\"Excluded(always):\", len(exclude))\n",
    "print(\"Dropped-for-clustering:\", sorted(exclude_for_clustering & set(pdf.columns)))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Encoders\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 5) Encode -> SVD(<=20) -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 6) k-scan\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.2f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "# 7) Summarize + plots\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "print(\"\\nK-scan metrics:\")\n",
    "print(metrics_df.sort_values([\"rank_sil\",\"k\"]).to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\"); ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\"); ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "best_row = metrics_df.sort_values([\"rank_sil\",\"k\"]).iloc[0]\n",
    "best_k = int(best_row[\"k\"]); best_sil = float(best_row[\"silhouette\"])\n",
    "print(f\"\\nBest k = {best_k}  |  best silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# 8) Final fit + cluster sizes\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = best_model.labels_\n",
    "sizes = pdf[\"cluster\"].value_counts().sort_index().to_frame(\"count\")\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "print(\"\\nCluster sizes:\\n\", sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057eb112-5ee3-437f-9a5c-e9b4483e7fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table \n",
    "# (exclude concessao_contactos + modelo_segment only for clustering) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load table\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Generic exclusions (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "# --- IMPORTANT: drop only for clustering (keep for profiling) ---\n",
    "exclude_for_clustering = {\"concessao_contactos\", \"modelo_segment\"}\n",
    "drop_cols = [c for c in (exclude | exclude_for_clustering) if c in pdf.columns]\n",
    "feature_df = pdf.drop(columns=drop_cols)\n",
    "\n",
    "print(\"Using features:\", feature_df.shape[1])\n",
    "print(\"Excluded(always):\", len(exclude))\n",
    "print(\"Dropped-for-clustering:\", sorted(exclude_for_clustering & set(pdf.columns)))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Encoders\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 5) Encode -> SVD(<=20) -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 6) k-scan\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.2f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "# 7) Summarize + plots\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "print(\"\\nK-scan metrics:\")\n",
    "print(metrics_df.sort_values([\"rank_sil\",\"k\"]).to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\"); ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\"); ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "best_row = metrics_df.sort_values([\"rank_sil\",\"k\"]).iloc[0]\n",
    "best_k = int(best_row[\"k\"]); best_sil = float(best_row[\"silhouette\"])\n",
    "print(f\"\\nBest k = {best_k}  |  best silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# 8) Final fit + cluster sizes\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = best_model.labels_\n",
    "sizes = pdf[\"cluster\"].value_counts().sort_index().to_frame(\"count\")\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "print(\"\\nCluster sizes:\\n\", sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f5cca8-a0d0-43d5-b484-70acc2b760a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === KMEANS (exclude specific categoricals only for clustering) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load table\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Generic exclusions (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "# --- EXCLUDE ONLY FOR CLUSTERING (keep in pdf, drop from features) ---\n",
    "exclude_for_clustering = {\n",
    "    \"lead_source\",\n",
    "    \"tipo_cliente_contactos\",\n",
    "    \"concessao_contactos\",\n",
    "    \"modelo_segment\",\n",
    "    \"Season\"\n",
    "}\n",
    "\n",
    "drop_cols = [c for c in (exclude | exclude_for_clustering) if c in pdf.columns]\n",
    "feature_df = pdf.drop(columns=drop_cols)\n",
    "\n",
    "print(\"Using features:\", feature_df.shape[1])\n",
    "print(\"Excluded(always):\", len(exclude))\n",
    "print(\"Dropped-for-clustering:\", sorted(exclude_for_clustering & set(pdf.columns)))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Encoders\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 5) Encode -> SVD(<=20) -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 6) k-scan\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.2f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "# 7) Summarize + plots\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "print(\"\\nK-scan metrics:\")\n",
    "print(metrics_df.sort_values([\"rank_sil\",\"k\"]).to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\"); ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\"); ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "best_row = metrics_df.sort_values([\"rank_sil\",\"k\"]).iloc[0]\n",
    "best_k = int(best_row[\"k\"])\n",
    "best_sil = float(best_row[\"silhouette\"])\n",
    "print(f\"\\nBest k = {best_k}  |  best silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# 8) Final fit + cluster sizes\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = best_model.labels_\n",
    "sizes = pdf[\"cluster\"].value_counts().sort_index().to_frame(\"count\")\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "print(\"\\nCluster sizes:\\n\", sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05e71f4-55f5-4ff7-b7c1-bf3f7d31d64f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FULL, SELF-CONTAINED K-Means on sc_gold.Features_Table (modelo* excluded for clustering) ===\n",
    "import time, numpy as np, pandas as pd, sklearn\n",
    "from packaging.version import parse as V\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "SRC_TBL = \"sc_gold.Features_Table\"\n",
    "\n",
    "# 1) Load table\n",
    "pdf = spark.sql(f\"SELECT * FROM {SRC_TBL}\").toPandas()\n",
    "print(\"Full shape:\", pdf.shape)\n",
    "\n",
    "# 2) Generic exclusions (timestamps, IDs/admin, outcomes, near-constants)\n",
    "name_l = {c: c.lower() for c in pdf.columns}\n",
    "def has_any(s, toks): return any(t in s for t in toks)\n",
    "\n",
    "exclude = set()\n",
    "for c, lc in name_l.items():\n",
    "    if has_any(lc, [\"_time\",\"timestamp\"]) or lc.endswith(\"_at\"):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"date\",\"data_\"]) and (\"year\" not in lc and \"month\" not in lc):\n",
    "        exclude.add(c)\n",
    "    if has_any(lc, [\"_id\",\"id_\",\"uuid\",\"guid\",\"hash\",\"chave\",\"contrato\",\"codigo\",\"code\",\"tylacode\",\"key\",\n",
    "                    \"created_by\",\"modified_by\",\"updated_by\",\"owner\",\"gestor\",\"assigned\",\"pipeline\",\"stage_id\"]):\n",
    "        exclude.add(c)\n",
    "    if lc in (\"is_converted\",\"converted\",\"target\"):\n",
    "        exclude.add(c)\n",
    "\n",
    "# near-constant columns (>=99.5% same value)\n",
    "nrows = len(pdf)\n",
    "for c in pdf.columns:\n",
    "    vc = pdf[c].value_counts(dropna=True)\n",
    "    if len(vc) and (vc.iloc[0] / max(1, nrows)) >= 0.995:\n",
    "        exclude.add(c)\n",
    "\n",
    "# --- IMPORTANT: drop only for clustering (keep for profiling) ---\n",
    "exclude_for_clustering = {\"modelo\", \"modelo_segment\"}\n",
    "drop_cols = [c for c in (exclude | exclude_for_clustering) if c in pdf.columns]\n",
    "feature_df = pdf.drop(columns=drop_cols)\n",
    "\n",
    "print(\"Using features:\", feature_df.shape[1])\n",
    "print(\"Excluded(always):\", len(exclude))\n",
    "print(\"Dropped-for-clustering:\", sorted(exclude_for_clustering & set(pdf.columns)))\n",
    "\n",
    "# 3) Column groups\n",
    "cat_cols = feature_df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "num_cols = feature_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(f\"Found {len(num_cols)} numeric, {len(cat_cols)} categorical\")\n",
    "\n",
    "# 4) Encoders\n",
    "ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
    "ohe_kwargs[\"sparse_output\" if V(sklearn.__version__) >= V(\"1.2\") else \"sparse\"] = True\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(**ohe_kwargs))\n",
    "    ]), cat_cols))\n",
    "if not transformers:\n",
    "    raise ValueError(\"No usable feature columns after exclusions.\")\n",
    "\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=1.0)\n",
    "\n",
    "# 5) Encode -> SVD(<=20) -> scale\n",
    "Xs = preprocess.fit_transform(feature_df)\n",
    "n_comp = max(2, min(20, Xs.shape[1] - 1))\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=SEED)\n",
    "X = svd.fit_transform(Xs)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(f\"Encoded: {Xs.shape}  |  Reduced (SVD): {X.shape}\")\n",
    "\n",
    "# 6) k-scan\n",
    "k_range = range(2, 9)\n",
    "metrics = []\n",
    "t0 = time.perf_counter()\n",
    "for k in k_range:\n",
    "    t1 = time.perf_counter()\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=4096, n_init=20, max_iter=200, random_state=SEED)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels, sample_size=min(10000, X.shape[0]), random_state=SEED)\n",
    "    metrics.append((k, sil, float(km.inertia_), time.perf_counter()-t1))\n",
    "    print(f\"k={k:2d}  sil={sil:.4f}  inertia={km.inertia_:.0f}  loop={metrics[-1][3]:.2f}s\")\n",
    "print(f\"Total k-scan: {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "# 7) Summarize + plots\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"k\",\"silhouette\",\"inertia\",\"sec_per_fit\"])\n",
    "metrics_df[\"rank_sil\"] = metrics_df[\"silhouette\"].rank(ascending=False).astype(int)\n",
    "print(\"\\nK-scan metrics:\")\n",
    "print(metrics_df.sort_values([\"rank_sil\",\"k\"]).to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia (lower is better)\"); ax.set_title(\"Elbow curve\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\")\n",
    "ax.set_xlabel(\"k\"); ax.set_ylabel(\"Silhouette (higher is better)\"); ax.set_title(\"Silhouette vs k\")\n",
    "plt.show()\n",
    "\n",
    "best_row = metrics_df.sort_values([\"rank_sil\",\"k\"]).iloc[0]\n",
    "best_k = int(best_row[\"k\"])\n",
    "best_sil = float(best_row[\"silhouette\"])\n",
    "print(f\"\\nBest k = {best_k}  |  best silhouette = {best_sil:.4f}\")\n",
    "\n",
    "# 8) Final fit + cluster sizes\n",
    "best_model = MiniBatchKMeans(n_clusters=best_k, batch_size=4096, n_init=50, max_iter=300, random_state=SEED).fit(X)\n",
    "pdf[\"cluster\"] = best_model.labels_\n",
    "sizes = pdf[\"cluster\"].value_counts().sort_index().to_frame(\"count\")\n",
    "sizes[\"pct\"] = (sizes[\"count\"] / len(pdf) * 100).round(2)\n",
    "print(\"\\nCluster sizes:\\n\", sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a7898e-51d8-4403-8572-50708ac09f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Cluster Profiling ===\n",
    "import numpy as np\n",
    "\n",
    "# 1) Cluster sizes (already computed, just show again cleanly)\n",
    "print(\"Cluster sizes:\")\n",
    "print(sizes)\n",
    "\n",
    "# 2) Median of numeric variables by cluster\n",
    "num_summary = (\n",
    "    pdf.groupby(\"cluster\")[num_cols]\n",
    "       .median()\n",
    "       .round(2)\n",
    ")\n",
    "print(\"\\nNumeric medians by cluster:\")\n",
    "print(num_summary)\n",
    "\n",
    "# Also: deltas vs cluster 0 for interpretation\n",
    "deltas = num_summary.subtract(num_summary.iloc[0], axis=1)\n",
    "print(\"\\nDeltas vs cluster 0:\")\n",
    "print(deltas)\n",
    "\n",
    "# 3) Top categories for each categorical variable\n",
    "def top_categories(df, col, n=3):\n",
    "    return (\n",
    "        df.groupby(\"cluster\")[col]\n",
    "          .apply(lambda x: x.value_counts(normalize=True)\n",
    "                            .head(n)\n",
    "                            .round(3)\n",
    "                            .to_dict())\n",
    "    )\n",
    "\n",
    "cat_summary = {}\n",
    "for c in cat_cols + [\"modelo\",\"modelo_segment\",\"lead_source\",\"tipo_cliente_contactos\",\"concessao_contactos\"]:\n",
    "    if c in pdf.columns:  # only those present\n",
    "        cat_summary[c] = top_categories(pdf, c, n=3)\n",
    "\n",
    "print(\"\\nTop categories per cluster (showing top 3 per cluster):\")\n",
    "for col, series in cat_summary.items():\n",
    "    print(f\"\\n▶ {col}\")\n",
    "    print(series)\n",
    "\n",
    "# 4) Attach profiling back to cluster sizes\n",
    "cluster_profile = sizes.join(num_summary, how=\"left\")\n",
    "print(\"\\nCluster profile:\")\n",
    "print(cluster_profile)\n",
    "\n",
    "# 5) Optional: visualize numeric medians\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for col in num_cols[:10]:  # first 10 numerics\n",
    "    plt.figure(figsize=(6,3))\n",
    "    pdf.groupby(\"cluster\")[col].median().plot(kind=\"bar\", title=f\"Median {col} by cluster\")\n",
    "    plt.ylabel(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "327a4619-3bc6-4518-9459-a17b1e018bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 📊 CLUSTER INTERPRETATION\n",
    "# =========================\n",
    "#\n",
    "# Cluster Sizes:\n",
    "# - Cluster 0 → 3.35% (~6,100 rows), small minority / outlier.\n",
    "# - Cluster 1 → 96.65% (~176,400 rows), large majority.\n",
    "#\n",
    "# Numeric Medians:\n",
    "# - Overall Sales Duration (deals):\n",
    "#     • Cluster 0 = ~530 days (!!)\n",
    "#     • Cluster 1 = ~35 days\n",
    "#     → Gap of ~495 days → Cluster 0 = extremely slow-moving deals.\n",
    "# - Other numeric fields did not show strong separation.\n",
    "#\n",
    "# Categorical Profiles:\n",
    "# - Lead Source:\n",
    "#     • Both dominated by Facebook & Site Hyundai Portugal.\n",
    "#     • Cluster 0 has slightly more Site Hyundai (38.6% vs 33.3%).\n",
    "# - Tipo Cliente (customer type):\n",
    "#     • Both are mostly Particular (individuals).\n",
    "#     • Cluster 0 has more ENI customers (13% vs 7.9%).\n",
    "#     → ENI clients = longer sales cycles.\n",
    "# - Concessão (dealerships):\n",
    "#     • Cluster 0 more concentrated in specific dealers \n",
    "#       (Entreposto Lisboa, Madeira Motores, etc.).\n",
    "#     • Suggests dealership process can influence deal duration.\n",
    "# - PVP Propostas (price bands):\n",
    "#     • Cluster 0 has higher share of expensive cars (48k+, 40k+).\n",
    "#     • Cluster 1 covers cheaper ranges (26k, 21k, 38k).\n",
    "#     → Expensive vehicles take longer to close.\n",
    "# - Seasonality:\n",
    "#     • Cluster 0 = more Winter & Spring.\n",
    "#     • Cluster 1 = more Summer & Autumn.\n",
    "#     • Secondary effect, but may point to timing differences.\n",
    "# - Modelo Segment:\n",
    "#     • Cluster 0 = more SUVs (46%) and EVs (22%).\n",
    "#     • Cluster 1 = fewer SUVs/EVs, more Compact/City (27%).\n",
    "#     → Bigger cars = longer decision cycles.\n",
    "#\n",
    "# =========================\n",
    "# 📝 SUMMARY\n",
    "# =========================\n",
    "# - Cluster 0 (~3%):\n",
    "#     • Slow, complex, high-value deals.\n",
    "#     • SUVs/EVs, higher price points, ENI customers, web leads,\n",
    "#       and specific dealerships.\n",
    "#     • Sales cycle ~530 days.\n",
    "#\n",
    "# - Cluster 1 (~97%):\n",
    "#     • Mainstream, faster-moving deals.\n",
    "#     • Smaller/cheaper cars, mostly individuals.\n",
    "#     • Sales cycle ~35 days.\n",
    "#\n",
    "# Business Implications:\n",
    "# - Cluster 0 should NOT be discarded as noise → it is a \"premium / \n",
    "#   complex sales segment\". These deals are fewer but higher-value.\n",
    "# - They require more tailored nurturing and CRM strategies.\n",
    "# - Cluster 1 is the backbone of the pipeline (fast conversions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473642cb-cf7b-4f37-9f14-3bb112cf010f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Cluster Comparison Report ===\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1) Numeric summary ---\n",
    "num_summary = (\n",
    "    pdf.groupby(\"cluster\")[num_cols]\n",
    "       .median()\n",
    "       .round(2)\n",
    "       .T  # transpose → vars as rows, clusters as columns\n",
    ")\n",
    "num_summary.columns = [f\"Cluster {c}\" for c in num_summary.columns]\n",
    "\n",
    "# --- 2) Top categories per categorical column ---\n",
    "def top_cat_summary(df, col, n=2):\n",
    "    \"\"\"Return top n categories with percentages per cluster.\"\"\"\n",
    "    out = {}\n",
    "    for cl, sub in df.groupby(\"cluster\"):\n",
    "        vc = sub[col].value_counts(normalize=True).head(n).round(3) * 100\n",
    "        out[f\"Cluster {cl}\"] = [f\"{idx} ({val:.1f}%)\" for idx, val in vc.items()]\n",
    "    return pd.DataFrame.from_dict(out, orient=\"columns\")\n",
    "\n",
    "cat_tables = {}\n",
    "for c in [\"lead_source\", \"tipo_cliente_contactos\", \n",
    "          \"concessao_contactos\", \"pvp_propostas\", \n",
    "          \"Season\", \"modelo_segment\"]:\n",
    "    if c in pdf.columns:\n",
    "        cat_tables[c] = top_cat_summary(pdf, c, n=2)\n",
    "\n",
    "# --- 3) Build report dictionary ---\n",
    "report = {\"Numeric Medians\": num_summary}\n",
    "report.update(cat_tables)\n",
    "\n",
    "# --- 4) Display nicely ---\n",
    "for section, table in report.items():\n",
    "    print(f\"\\n==== {section} ====\")\n",
    "    display(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad140f2d-0d10-439a-bfb4-69539fcf2576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Sales cycle distribution: overall_sales_duration_deals ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "col = \"overall_sales_duration_deals\"\n",
    "\n",
    "# 0) Guard\n",
    "assert col in pdf.columns, f\"{col} not found in pdf.columns\"\n",
    "assert \"cluster\" in pdf.columns, \"cluster column not found\"\n",
    "\n",
    "# Keep only valid rows\n",
    "df_sc = pdf[[col, \"cluster\"]].dropna().copy()\n",
    "df_sc[col] = pd.to_numeric(df_sc[col], errors=\"coerce\")\n",
    "df_sc = df_sc.dropna()\n",
    "\n",
    "# 1) Summary stats by cluster\n",
    "def q(s, p): \n",
    "    return np.nanpercentile(s, p)\n",
    "\n",
    "summary = (\n",
    "    df_sc.groupby(\"cluster\")[col]\n",
    "         .agg([\n",
    "             (\"n\", \"count\"),\n",
    "             (\"min\", \"min\"),\n",
    "             (\"median\", \"median\"),\n",
    "             (\"mean\", \"mean\"),\n",
    "             (\"p90\", lambda s: q(s, 90)),\n",
    "             (\"p95\", lambda s: q(s, 95)),\n",
    "             (\"p99\", lambda s: q(s, 99)),\n",
    "             (\"max\", \"max\")\n",
    "         ])\n",
    "         .round(2)\n",
    ")\n",
    "print(\"=== Sales cycle summary (days) ===\")\n",
    "display(summary)\n",
    "\n",
    "# 2) Bucketed distribution table (days)\n",
    "# tweak the bins if you need finer/coarser buckets\n",
    "bins = [-np.inf, 7, 14, 21, 30, 45, 60, 90, 120, 180, np.inf]\n",
    "labels = [\"0–7\",\"8–14\",\"15–21\",\"22–30\",\"31–45\",\"46–60\",\"61–90\",\"91–120\",\"121–180\",\">180\"]\n",
    "\n",
    "df_sc[\"bucket\"] = pd.cut(df_sc[col], bins=bins, labels=labels, right=True)\n",
    "\n",
    "bucket_tab = (\n",
    "    df_sc.pivot_table(index=\"bucket\", columns=\"cluster\", values=col, aggfunc=\"size\", fill_value=0)\n",
    "         .assign(All=lambda t: t.sum(axis=1))\n",
    ")\n",
    "bucket_tab_pct = (bucket_tab.div(bucket_tab.sum(axis=0), axis=1) * 100).round(2)\n",
    "\n",
    "print(\"\\n=== Bucket counts (rows = days bucket; cols = cluster) ===\")\n",
    "display(bucket_tab)\n",
    "print(\"\\n=== Bucket % (rows = days bucket; cols = cluster) ===\")\n",
    "display(bucket_tab_pct)\n",
    "\n",
    "# Also show compact KPI buckets that business likes\n",
    "kpi_bins = [0, 30, 60, 90, np.inf]\n",
    "kpi_labels = [\"<=30\",\"31–60\",\"61–90\",\">90\"]\n",
    "df_sc[\"kpi_bucket\"] = pd.cut(df_sc[col], bins=kpi_bins, labels=kpi_labels, right=True, include_lowest=True)\n",
    "\n",
    "kpi_tab = (\n",
    "    df_sc.pivot_table(index=\"kpi_bucket\", columns=\"cluster\", values=col, aggfunc=\"size\", fill_value=0)\n",
    "         .assign(All=lambda t: t.sum(axis=1))\n",
    ")\n",
    "kpi_tab_pct = (kpi_tab.div(kpi_tab.sum(axis=0), axis=1) * 100).round(2)\n",
    "\n",
    "print(\"\\n=== KPI buckets (counts) ===\")\n",
    "display(kpi_tab)\n",
    "print(\"\\n=== KPI buckets (%) ===\")\n",
    "display(kpi_tab_pct)\n",
    "\n",
    "# 3) ECDF (cumulative) plot per cluster\n",
    "plt.figure(figsize=(7,5))\n",
    "for cl, sub in df_sc.groupby(\"cluster\"):\n",
    "    x = np.sort(sub[col].values)\n",
    "    y = np.arange(1, len(x)+1) / len(x)\n",
    "    plt.plot(x, y, label=f\"Cluster {cl}\")\n",
    "plt.xlabel(\"Sales cycle (days)\")\n",
    "plt.ylabel(\"Cumulative share\")\n",
    "plt.title(\"ECDF of sales cycle by cluster\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4) Histogram per cluster, same bins\n",
    "plt.figure(figsize=(7,5))\n",
    "for cl, sub in df_sc.groupby(\"cluster\"):\n",
    "    plt.hist(sub[col].values, bins=bins, alpha=0.5, label=f\"Cluster {cl}\")\n",
    "plt.xlabel(\"Sales cycle (days)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Sales cycle histogram by cluster\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 5) Quick text interpretation helpers (optional)\n",
    "def quick_readout(s):\n",
    "    return {\n",
    "        \"median\": np.median(s),\n",
    "        \"% <=30\": (s <= 30).mean()*100,\n",
    "        \"% <=60\": (s <= 60).mean()*100,\n",
    "        \"% >90\":  (s > 90).mean()*100\n",
    "    }\n",
    "\n",
    "print(\"\\n=== Quick readout per cluster ===\")\n",
    "for cl, sub in df_sc.groupby(\"cluster\"):\n",
    "    stats = quick_readout(sub[col].values)\n",
    "    print(f\"Cluster {cl}: median={stats['median']:.1f}d | <=30d={stats['% <=30']:.1f}% | <=60d={stats['% <=60']:.1f}% | >90d={stats['% >90']:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6725012492152357,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_Models_Pedro_KMEANS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
