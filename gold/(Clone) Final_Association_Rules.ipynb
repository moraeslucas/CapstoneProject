{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7b64c7-cd24-4e60-b98b-a3a99d8865d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Viaturas **table****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e294c67-96e4-480b-8bcd-bfbba36ccf16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.viaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75c6ca9-c19d-4f13-bd6a-93b2cb1f4ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "DROP TABLE IF EXISTS sc_gold.viaturas_2;\n",
    "\n",
    "CREATE TABLE sc_gold.viaturas_2 AS\n",
    "SELECT id,modelo,motorizacao,data_de_matricula,cilindrada__cm3_,potencia_maxima__kw_,combustivel,production_date,gwms_engine\n",
    "FROM sc_gold.viaturas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d45503d8-1daa-4278-9f94-0ed3b59dc29e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762284631290}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.viaturas_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f132080c-4826-45c5-9530-6f255c21841a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType, DecimalType\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60969c5c-540c-4d33-a3b1-5f79e3f358c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"sc_gold.viaturas_2\"\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Get total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate percentage of nulls for each column\n",
    "null_percentages = (\n",
    "    df.select([\n",
    "        (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100)\n",
    "        .alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee5775e-cae8-4198-b37d-f203df1fad83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#passo usado para remover linhas (excluir linhas vazias na coluna modelo -486 linhas)\n",
    "table_name = \"sc_gold.viaturas_2\"\n",
    "df0 = spark.table(table_name)\n",
    "\n",
    "\n",
    "# Aplicar o filtro (excluir linhas vazias na coluna modelo -486 linhas)\n",
    "df = df0.filter(\n",
    "    (F.col(\"modelo\").isNotNull())\n",
    "    & (F.trim(F.col(\"modelo\")) != \"\")\n",
    "    & (F.lower(F.trim(F.col(\"modelo\"))) != \"null\")\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3062ef74-05b5-40d0-a8b3-fa9b2a6ca20d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#descobrir o numero de dias em media de diferen√ßa entre a data de produ√ß√£o e a data da matricula\n",
    "\n",
    "# Carregar a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Converter para DATE (com v√°rios formatos tolerados, se necess√°rio)\n",
    "df = df.withColumn(\n",
    "    \"data_de_matricula_dt\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'dd/MM/yyyy')\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"production_date_dt\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(production_date, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'dd/MM/yyyy')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Remover linhas com valores nulos em qualquer das datas\n",
    "df_valid = df.filter(F.col(\"data_de_matricula_dt\").isNotNull() & F.col(\"production_date_dt\").isNotNull())\n",
    "\n",
    "# Calcular diferen√ßa em dias\n",
    "df_valid = df_valid.withColumn(\"diff_days\", \n",
    "                               F.datediff(F.col(\"data_de_matricula_dt\"), F.col(\"production_date_dt\")))\n",
    "\n",
    "# Somat√≥rio e m√©dia\n",
    "agg = df_valid.agg(\n",
    "    F.sum(\"diff_days\").alias(\"soma_dias\"),\n",
    "    F.count(\"diff_days\").alias(\"n_linhas\"),\n",
    "    F.avg(\"diff_days\").alias(\"media_dias\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"üîπ Soma total dos dias:\", agg[\"soma_dias\"])\n",
    "print(\"üîπ N√∫mero de linhas usadas:\", agg[\"n_linhas\"])\n",
    "print(\"üîπ M√©dia de dias:\", round(agg[\"media_dias\"], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07033e44-a51e-441a-a916-b7d4a37be57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#substituir data de produ√ß√£o e a data da matricula +/-131 com base no calculo anterior\n",
    "\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Tenta converter em v√°rios formatos comuns\n",
    "df = df.withColumn(\n",
    "    \"data_de_matricula\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'dd/MM/yyyy')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'yyyy/MM/dd')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"production_date\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(production_date, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'dd/MM/yyyy')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'yyyy/MM/dd')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Aplicar as regras dos ¬±131 dias\n",
    "df = df.withColumn(\n",
    "    \"production_date\",\n",
    "    F.when(F.col(\"production_date\").isNull() & F.col(\"data_de_matricula\").isNotNull(),\n",
    "           F.date_sub(F.col(\"data_de_matricula\"), 131))\n",
    "     .otherwise(F.col(\"production_date\"))\n",
    ").withColumn(\n",
    "    \"data_de_matricula\",\n",
    "    F.when(F.col(\"data_de_matricula\").isNull() & F.col(\"production_date\").isNotNull(),\n",
    "           F.date_add(F.col(\"production_date\"), 131))\n",
    "     .otherwise(F.col(\"data_de_matricula\"))\n",
    ")\n",
    "\n",
    "# Reescrever a tabela (permitindo schema overwrite se necess√°rio)\n",
    "(df.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "116cb73c-b93d-4026-90c7-4e5d6564e728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#criar nova coluna que √© o ano de produ√ß√£o do carro\n",
    "# Carregar tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Converter production_date para DATE (caso ainda seja string)\n",
    "df = df.withColumn(\n",
    "    \"production_date_dt\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(production_date, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'yyyy-MM-dd')\"),\n",
    "     F.expr(\"try_to_date(production_date, 'dd/MM/yyyy')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extrair o ano de produ√ß√£o\n",
    "df = df.withColumn(\"production_year\", F.year(\"production_date_dt\"))\n",
    "\n",
    "# Calcular idade em anos at√© a data de hoje\n",
    "df = df.withColumn(\n",
    "    \"age_year\",\n",
    "    F.floor(F.datediff(F.current_date(), F.col(\"production_date_dt\")) / 365.25)\n",
    ")\n",
    "\n",
    "# Regravar a tabela com as novas colunas\n",
    "(df.write\n",
    "   .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\", \"true\")  # garante que aceita as novas colunas\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5fbb1c6-fe6a-4c28-aedf-100ec861c1b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calcular a m√©dia de cilindrada (cilindrada__cm3_) por (gwms_engine + modelo + motoriza√ß√£o)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Janela por atributos do grupo\n",
    "w = W.partitionBy(\"gwms_engine\", \"motorizacao\", \"modelo\")\n",
    "\n",
    "# 3) m√©dia global (fallback) ‚Äî j√° arredondada para inteiro\n",
    "global_avg = df.select(F.avg(\"cilindrada__cm3_\").alias(\"g\")).first()[\"g\"]\n",
    "if global_avg is not None:\n",
    "    global_avg = int(round(global_avg))\n",
    "\n",
    "# 4) Preencher nulos com a m√©dia do grupo arredondada (ou fallback global)\n",
    "df_filled = (\n",
    "    df\n",
    "    .withColumn(\"avg_grupo\", F.avg(\"cilindrada__cm3_\").over(w))\n",
    "    .withColumn(\n",
    "        \"cilindrada__cm3_\",\n",
    "        F.when(\n",
    "            F.col(\"cilindrada__cm3_\").isNull(),\n",
    "            F.coalesce(F.round(F.col(\"avg_grupo\")).cast(IntegerType()), F.lit(global_avg))\n",
    "        ).otherwise(F.col(\"cilindrada__cm3_\").cast(IntegerType()))\n",
    "    )\n",
    "    .drop(\"avg_grupo\")\n",
    ")\n",
    "\n",
    "\n",
    "# 5) Escrever o RESULTADO correto\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf5d4339-dfbd-41cd-bc56-d5aecd5a6afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calcular a m√©dia de potencia (potencia_maxima__kw_) por (gwms_engine + modelo + motorizacao)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Limpeza e cast:\n",
    "#    - troca v√≠rgula decimal por ponto\n",
    "#    - remove qualquer caractere n√£o num√©rico (p.ex. ' kW', espa√ßos, etc.)\n",
    "pot_clean = F.regexp_replace(F.col(\"potencia_maxima__kw_\"), \",\", \".\")\n",
    "pot_clean = F.regexp_replace(pot_clean, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"potencia_maxima__kw_\", pot_clean.cast(DoubleType()))\n",
    "\n",
    "# 3) M√©dia global (fallback), arredondada a 1 casa\n",
    "global_avg = df.select(F.avg(\"potencia_maxima__kw_\").alias(\"g\")).first()[\"g\"]\n",
    "global_avg_1d = round(global_avg, 1) if global_avg is not None else None\n",
    "\n",
    "# 4) M√©dia por grupo\n",
    "keys = [\"gwms_engine\", \"motorizacao\", \"modelo\"]\n",
    "avg_by_group = (\n",
    "    df.groupBy(*keys)\n",
    "      .agg(F.avg(\"potencia_maxima__kw_\").alias(\"avg_grp\"))\n",
    ")\n",
    "\n",
    "# 5) Preencher apenas nulos com a m√©dia do grupo (1 casa decimal);\n",
    "#    se o grupo for todo nulo, usa m√©dia global\n",
    "df_filled = (\n",
    "    df.join(avg_by_group, on=keys, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"potencia_maxima__kw_\",\n",
    "          F.when(\n",
    "              F.col(\"potencia_maxima__kw_\").isNull(),\n",
    "              F.coalesce(F.round(F.col(\"avg_grp\"), 1), F.lit(global_avg_1d))\n",
    "          ).otherwise(F.col(\"potencia_maxima__kw_\"))\n",
    "      )\n",
    "      .drop(\"avg_grp\")\n",
    ")\n",
    "\n",
    "# (Opcional) Se quiser NORMALIZAR toda a coluna para 1 casa decimal, inclusive n√£o nulos:\n",
    "# df_filled = df_filled.withColumn(\"potencia_maxima__kw_\", F.round(F.col(\"potencia_maxima__kw_\"), 1))\n",
    "\n",
    "# (Opcional) Fixar o tipo para Decimal(10,1) no schema (em vez de double):\n",
    "# df_filled = df_filled.withColumn(\"potencia_maxima__kw_\", F.col(\"potencia_maxima__kw_\").cast(DecimalType(10,1)))\n",
    "\n",
    "# 6) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3726b2-7a1b-49dc-8a88-9ee73c39302a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preencher 'motorizacao' com a MODA por grupo (gwms_engine, modelo, potencia_maxima__kw_, combustivel)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Pot√™ncia: v√≠rgula -> ponto, remover ru√≠do e cast para double (porque √© chave do grupo)\n",
    "pot = F.regexp_replace(F.col(\"potencia_maxima__kw_\"), \",\", \".\")\n",
    "pot = F.regexp_replace(pot, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"potencia_maxima__kw_\", pot.cast(DoubleType()))\n",
    "\n",
    "# 3) Higienizar texto relevante\n",
    "for col in [\"motorizacao\", \"combustivel\", \"gwms_engine\", \"modelo\"]:\n",
    "    df = df.withColumn(col, F.trim(F.col(col)))\n",
    "df = df.withColumn(\"motorizacao\", F.when(F.col(\"motorizacao\") == \"\", None).otherwise(F.col(\"motorizacao\")))\n",
    "df = df.withColumn(\"combustivel\", F.when(F.col(\"combustivel\") == \"\", None).otherwise(F.col(\"combustivel\")))\n",
    "df = df.withColumn(\"gwms_engine\", F.when(F.col(\"gwms_engine\") == \"\", None).otherwise(F.col(\"gwms_engine\")))\n",
    "\n",
    "# 4) Chaves do grupo\n",
    "keys_mot = [\"gwms_engine\", \"modelo\", \"potencia_maxima__kw_\", \"combustivel\"]\n",
    "\n",
    "# 5) Moda de 'motorizacao' por grupo (desempate alfab√©tico)\n",
    "counts_mot = (\n",
    "    df.filter(F.col(\"motorizacao\").isNotNull())\n",
    "      .groupBy(*keys_mot, \"motorizacao\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    ")\n",
    "w_mot = W.partitionBy(*keys_mot).orderBy(F.col(\"cnt\").desc(), F.col(\"motorizacao\").asc())\n",
    "mode_motorizacao = (\n",
    "    counts_mot.withColumn(\"rn\", F.row_number().over(w_mot))\n",
    "              .filter(F.col(\"rn\") == 1)\n",
    "              .select(*keys_mot, F.col(\"motorizacao\").alias(\"mode_motorizacao\"))\n",
    ")\n",
    "\n",
    "# 6) Moda global de 'motorizacao' (fallback opcional)\n",
    "row_mot = (\n",
    "    df.filter(F.col(\"motorizacao\").isNotNull())\n",
    "      .groupBy(\"motorizacao\").agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    "      .orderBy(F.col(\"cnt\").desc(), F.col(\"motorizacao\").asc())\n",
    "      .limit(1).first()\n",
    ")\n",
    "global_mode_mot = row_mot[\"motorizacao\"] if row_mot else None\n",
    "\n",
    "# 7) Preencher APENAS nulos de 'motorizacao' com a moda do grupo (ou global)\n",
    "df_filled = (\n",
    "    df.join(mode_motorizacao, on=keys_mot, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"motorizacao\",\n",
    "          F.when(F.col(\"motorizacao\").isNull(),\n",
    "                 F.coalesce(F.col(\"mode_motorizacao\"), F.lit(global_mode_mot)))\n",
    "           .otherwise(F.col(\"motorizacao\"))\n",
    "      )\n",
    "      .drop(\"mode_motorizacao\")\n",
    ")\n",
    "\n",
    "# 8) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c920e5-95e8-4cc6-a2ad-669986cdb13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preencher 'gwms_engine' pela MODA por grupo (cilindrada__cm3_, potencia_maxima__kw_, modelo, motorizacao)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Higienizar num√©ricos (v√≠rgula -> ponto; remover ru√≠do) e fazer cast\n",
    "cil_clean = F.regexp_replace(F.col(\"cilindrada__cm3_\"), \",\", \".\")\n",
    "cil_clean = F.regexp_replace(cil_clean, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"cilindrada__cm3_\", cil_clean.cast(DoubleType()))\n",
    "\n",
    "pot_clean = F.regexp_replace(F.col(\"potencia_maxima__kw_\"), \",\", \".\")\n",
    "pot_clean = F.regexp_replace(pot_clean, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"potencia_maxima__kw_\", pot_clean.cast(DoubleType()))\n",
    "\n",
    "# 3) Higienizar texto: trim e strings vazias -> NULL\n",
    "df = df.withColumn(\"gwms_engine\", F.when(F.trim(F.col(\"gwms_engine\")) == \"\", None)\n",
    "                                  .otherwise(F.trim(F.col(\"gwms_engine\"))))\n",
    "df = df.withColumn(\"modelo\", F.trim(F.col(\"modelo\")))\n",
    "# \"motorizacao\" sem acento ‚Äî confirme o nome exato na tabela\n",
    "df = df.withColumn(\"motorizacao\", F.when(F.trim(F.col(\"motorizacao\")) == \"\", None)\n",
    "                                   .otherwise(F.trim(F.col(\"motorizacao\"))))\n",
    "\n",
    "# 4) Chaves do grupo\n",
    "keys = [\"cilindrada__cm3_\", \"potencia_maxima__kw_\", \"modelo\", \"motorizacao\"]\n",
    "\n",
    "# 5) Calcular a MODA de gwms_engine por grupo\n",
    "counts = (\n",
    "    df.filter(F.col(\"gwms_engine\").isNotNull())\n",
    "      .groupBy(*keys, \"gwms_engine\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    ")\n",
    "\n",
    "w = W.partitionBy(*keys).orderBy(F.col(\"cnt\").desc(), F.col(\"gwms_engine\").asc())\n",
    "\n",
    "mode_by_group = (\n",
    "    counts.withColumn(\"rn\", F.row_number().over(w))\n",
    "          .filter(F.col(\"rn\") == 1)\n",
    "          .select(*keys, F.col(\"gwms_engine\").alias(\"mode_gwms_engine\"))\n",
    ")\n",
    "\n",
    "# 6) (Opcional) Moda GLOBAL como fallback se o grupo n√£o tiver ocorr√™ncias v√°lidas\n",
    "row_global = (\n",
    "    df.filter(F.col(\"gwms_engine\").isNotNull())\n",
    "      .groupBy(\"gwms_engine\").agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    "      .orderBy(F.col(\"cnt\").desc(), F.col(\"gwms_engine\").asc())\n",
    "      .limit(1).first()\n",
    ")\n",
    "global_mode = row_global[\"gwms_engine\"] if row_global else None\n",
    "\n",
    "# 7) Preencher APENAS nulos com a moda do grupo (ou moda global se necess√°rio)\n",
    "df_filled = (\n",
    "    df.join(mode_by_group, on=keys, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"gwms_engine\",\n",
    "          F.when(\n",
    "              F.col(\"gwms_engine\").isNull(),\n",
    "              F.coalesce(F.col(\"mode_gwms_engine\"), F.lit(global_mode))\n",
    "          ).otherwise(F.col(\"gwms_engine\"))\n",
    "      )\n",
    "      .drop(\"mode_gwms_engine\")\n",
    ")\n",
    "\n",
    "# 8) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442c0b5f-47b1-476d-8988-dc5108d9e237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MODA de combustivel por (gwms_engine, cilindrada__cm3_, modelo, motorizacao)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Higienizar colunas num√©ricas (garantir double)\n",
    "cil = F.regexp_replace(F.col(\"cilindrada__cm3_\"), \",\", \".\")\n",
    "cil = F.regexp_replace(cil, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"cilindrada__cm3_\", cil.cast(DoubleType()))\n",
    "\n",
    "# 3) Higienizar colunas de texto\n",
    "df = df.withColumn(\"gwms_engine\", F.when(F.length(F.trim(\"gwms_engine\")) == 0, None)\n",
    "                                   .otherwise(F.trim(F.col(\"gwms_engine\")).cast(\"string\")))\n",
    "df = df.withColumn(\"modelo\", F.trim(F.col(\"modelo\")).cast(\"string\"))\n",
    "df = df.withColumn(\"motorizacao\", F.when(F.length(F.trim(\"motorizacao\")) == 0, None)\n",
    "                                   .otherwise(F.trim(F.col(\"motorizacao\")).cast(\"string\")))\n",
    "df = df.withColumn(\"combustivel\", F.when(F.length(F.trim(\"combustivel\")) == 0, None)\n",
    "                                   .otherwise(F.trim(F.col(\"combustivel\")).cast(\"string\")))\n",
    "\n",
    "# 4) Chave do grupo\n",
    "keys = [\"gwms_engine\", \"cilindrada__cm3_\", \"modelo\", \"motorizacao\"]\n",
    "\n",
    "# 5) Moda de combustivel por grupo\n",
    "counts = (\n",
    "    df.filter(F.col(\"combustivel\").isNotNull())\n",
    "      .groupBy(*keys, \"combustivel\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    ")\n",
    "\n",
    "w = W.partitionBy(*keys).orderBy(F.col(\"cnt\").desc(), F.col(\"combustivel\").asc())\n",
    "\n",
    "mode_by_group = (\n",
    "    counts.withColumn(\"rn\", F.row_number().over(w))\n",
    "          .filter(F.col(\"rn\") == 1)\n",
    "          .select(*keys, F.col(\"combustivel\").alias(\"mode_combustivel\"))\n",
    ")\n",
    "\n",
    "# 6) Moda global como fallback\n",
    "row_global = (\n",
    "    df.filter(F.col(\"combustivel\").isNotNull())\n",
    "      .groupBy(\"combustivel\").agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    "      .orderBy(F.col(\"cnt\").desc(), F.col(\"combustivel\").asc())\n",
    "      .limit(1).first()\n",
    ")\n",
    "global_mode = row_global[\"combustivel\"] if row_global else None\n",
    "\n",
    "# 7) Preencher apenas nulos\n",
    "df_filled = (\n",
    "    df.join(mode_by_group, on=keys, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"combustivel\",\n",
    "          F.when(\n",
    "              F.col(\"combustivel\").isNull(),\n",
    "              F.coalesce(F.col(\"mode_combustivel\"), F.lit(global_mode))\n",
    "          ).otherwise(F.col(\"combustivel\"))\n",
    "      )\n",
    "      .drop(\"mode_combustivel\")\n",
    ")\n",
    "\n",
    "# 8) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee8a643-bbb1-4249-ac40-a040be8eebc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Passo usado para remover nulos das datas (confirmar)\n",
    "#  Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Remover linhas onde production_date √© NULL\n",
    "df_clean = df.filter(F.col(\"age_year\").isNotNull())\n",
    "\n",
    "\n",
    "# (opcional) sobrescrever a tabela com o dataset limpo\n",
    "(df_clean.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05992e25-2823-4557-abe3-4b05c3ed53c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Passo usado para remover coluna \"production_year\" vai distorcer o clustering porque est√°s a ‚Äúcontar duas vezes‚Äù o mesmo fator \"age_year\". Alem disso removemos \"data_de_matricula\",\"production_date\" pois nao √© bom usar datas para os clusters\n",
    "\n",
    "#  Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Remover a coluna 'production_date_dt'\n",
    "df_clean = df.drop(\"production_year\",\"data_de_matricula\",\"production_date\",\"production_date_dt\",'gwms_engine')\n",
    "\n",
    "# (opcional) sobrescrever a tabela com o dataset limpo\n",
    "(df_clean.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9413fbf7-7126-4916-8e77-656705df4bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converter age_year para inteiro\n",
    "df = df.withColumn(\"age_year\", F.col(\"age_year\").cast(IntegerType()))\n",
    "\n",
    "# Guardar a tabela sobrescrevendo a anterior\n",
    "(df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n",
    "\n",
    "# Confirmar no DF atualizado\n",
    "df.select(\"age_year\").distinct().show(20)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20f4a73-d9a0-4ea3-a19f-cb7dbe211133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#colocar anos em intervalos\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"age_interval\",\n",
    "    F.when(F.col(\"age_year\") >= 20, \"20+\")\n",
    "     .otherwise(\n",
    "         F.concat(\n",
    "             (F.floor(F.col(\"age_year\") / 5) * 5).cast(\"string\"),\n",
    "             F.lit(\"-\"),\n",
    "             ((F.floor(F.col(\"age_year\") / 5) * 5) + 4).cast(\"string\")\n",
    "         )\n",
    "     )\n",
    ")\n",
    "\n",
    "\n",
    "(df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e7285f-3552-4236-8c61-57e7447c8a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#escolher primeira palavra da coluna modelo para diminuir a granularidade desta coluna\n",
    "#  Ler a tabela\n",
    "#df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "#df = df.withColumn(\"modelo\", F.split(F.col(\"modelo\"), \" \").getItem(0))\n",
    "\n",
    "#(df.write\n",
    "#   .mode(\"overwrite\")\n",
    " #  .option(\"overwriteSchema\", \"true\")\n",
    " #  .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f0b3d5-d9ef-4a5c-87bb-c93025176901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#excluir modelos inv√°lidos\n",
    "# Lista de valores a excluir\n",
    "valores_a_remover = [\n",
    "    \"Outro\",\n",
    "    \"Outras Marcas\",\n",
    "    \"Modelo\",\n",
    "    \"900\",\n",
    "    \"Jobs\",\n",
    "    \"Fdh\",\n",
    "    \"Pdei49ggyipaetry\",\n",
    "    \"Fs\",\n",
    "    \"800mt\",\n",
    "    \"Fct\",\n",
    "    \"Gsc4d261f\"\n",
    "]\n",
    "\n",
    "# Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Filtrar removendo essas linhas\n",
    "df = df.filter(~F.col(\"modelo\").isin(valores_a_remover))\n",
    "\n",
    "# Se quiseres guardar ou mostrar:\n",
    "(df.write \n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29eaef1c-b116-41ca-b543-94c54533f046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para ve√≠culos a combust√£o (Gasolina, Diesel, H√≠bridos HEV/PHEV, GPL, etc.)\n",
    "Classe\tIntervalo (kW)\tEquivalente em cv\n",
    "Baixa\t< 75 kW\t< 100 cv\n",
    "M√©dia\t75‚Äì110 kW\t100‚Äì150 cv\n",
    "Alta\t> 110 kW\t> 150 cv\n",
    "Para ve√≠culos 100% el√©tricos (combustivel = 'El√©trico' ou equivalente)\n",
    "Classe\tIntervalo (kW)\tObserva√ß√£o\n",
    "Baixa\t< 100 kW\tEx.: Ioniq Electric 28kWh (88 kW)\n",
    "M√©dia\t100‚Äì150 kW\tEx.: Kona EV 64kWh (150 kW)\n",
    "Alta\t> 150 kW\tEx.: Ioniq 5, Tesla, EVs potentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a55782-ffe4-4d6b-a102-262f55fef357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"classe_potencia\",\n",
    "    F.when(\n",
    "        (F.col(\"combustivel\").isin(\"El√©trico\", \"Eletrico\", \"EV\", \"BEV\")) & \n",
    "        (F.col(\"potencia_maxima__kw_\") < 100),\n",
    "        \"Baixa\"\n",
    "    ).when(\n",
    "        (F.col(\"combustivel\").isin(\"El√©trico\", \"Eletrico\", \"EV\", \"BEV\")) & \n",
    "        (F.col(\"potencia_maxima__kw_\").between(100, 150)),\n",
    "        \"M√©dia\"\n",
    "    ).when(\n",
    "        (F.col(\"combustivel\").isin(\"El√©trico\", \"Eletrico\", \"EV\", \"BEV\")) & \n",
    "        (F.col(\"potencia_maxima__kw_\") > 150),\n",
    "        \"Alta\"\n",
    "    ).when(F.col(\"potencia_maxima__kw_\") < 75, \"Baixa\"\n",
    "    ).when(F.col(\"potencia_maxima__kw_\").between(75, 110), \"M√©dia\"\n",
    "    ).when(F.col(\"potencia_maxima__kw_\") > 110, \"Alta\"\n",
    "    ).otherwise(\"Desconhecida\")\n",
    ")\n",
    "\n",
    "df.select(\"potencia_maxima__kw_\", \"combustivel\", \"classe_potencia\").show(20)\n",
    "\n",
    "# Se quiseres guardar ou mostrar:\n",
    "(df.write \n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d0ae91-e6d9-4596-8716-734d86ab3634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "table_name = \"sc_gold.viaturas_2\"\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Get total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate percentage of nulls for each column\n",
    "null_percentages = (\n",
    "    df.select([\n",
    "        (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100)\n",
    "        .alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc566ee2-a930-4bc5-8be2-31d5253a99bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Historico de servi√ßos table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9753b67-791f-4e3a-8304-fa2f777c987e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM workspace.sc_gold.historico_de_servicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0c1edf-d770-405e-8f21-64a54b7c0002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS sc_gold.historico_de_servicos_2;\n",
    "CREATE TABLE sc_gold.historico_de_servicos_2 AS\n",
    "SELECT viatura,descricao_servico_pos_venda,tipo_de_servico\n",
    "FROM sc_gold.historico_de_servicos;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de08fa57-db3c-4320-8946-367c1665d16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM workspace.sc_gold.historico_de_servicos_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6869548e-6f3c-445a-a0b6-2232468cdfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE workspace.sc_gold.historico_de_servicos_2 AS\n",
    "SELECT *\n",
    "FROM workspace.sc_gold.historico_de_servicos_2\n",
    "WHERE tipo_de_servico IS NOT NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3dafb41-9b31-4dcf-ad42-3e2c7eea38ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#passo usado para remover linhas (excluir linhas vazias na coluna tipo_de_servico)\n",
    "table_name = \"sc_gold.historico_de_servicos_2\"\n",
    "df0 = spark.table(table_name)\n",
    "\n",
    "\n",
    "# Aplicar o filtro (excluir linhas vazias na coluna tipo_de_servico)\n",
    "df = df0.filter(\n",
    "    (F.col(\"tipo_de_servico\").isNotNull())\n",
    "    & (F.trim(F.col(\"tipo_de_servico\")) != \"\")\n",
    "    & (F.lower(F.trim(F.col(\"tipo_de_servico\"))) != \"null\")\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e05be2-bf78-4562-a71a-85744d38ba8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#passo usado para remover linhas (excluir linhas vazias na coluna viatura)\n",
    "table_name = \"sc_gold.historico_de_servicos_2\"\n",
    "df0 = spark.table(table_name)\n",
    "\n",
    "\n",
    "# Aplicar o filtro (excluir linhas vazias na coluna viatura)\n",
    "df = df0.filter(\n",
    "    (F.col(\"viatura\").isNotNull())\n",
    "    & (F.trim(F.col(\"viatura\")) != \"\")\n",
    "    & (F.lower(F.trim(F.col(\"viatura\"))) != \"null\")\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507f84e1-2f67-444f-ba84-e5bb049f500f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"workspace.sc_gold.historico_de_servicos_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77854eb-5200-4cf5-83c3-27284cba03d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "table_name = \"workspace.sc_gold.historico_de_servicos_2\"\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Get total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate percentage of nulls for each column\n",
    "null_percentages = (\n",
    "    df.select([\n",
    "        (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100)\n",
    "        .alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_percentages)\n",
    "#os nulos na coluna descricao_servico_pos_venda n√£o √£o relevantes pois em principio nao ser√° uma coluna usada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce8db2d3-7065-413a-bcf9-61d23f3f7053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Count of distinct descriptions\n",
    "SELECT COUNT(DISTINCT tipo_de_servico) AS distinct_count\n",
    "FROM workspace.sc_gold.historico_de_servicos_2;\n",
    "\n",
    "-- Show the distinct text values\n",
    "SELECT DISTINCT tipo_de_servico\n",
    "FROM workspace.sc_gold.historico_de_servicos_2\n",
    "ORDER BY tipo_de_servico;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94662c08-3cdc-40e0-9010-f9f1d32de7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#fazer join entre viatura e historico de servicos\n",
    "\n",
    "# %sql\n",
    "# CREATE OR REPLACE TABLE workspace.sc_gold.join_viatura_historico AS\n",
    "# SELECT\n",
    "#   v.*,\n",
    "#   h.*\n",
    "# FROM sc_gold.viaturas_2 AS v\n",
    "# LEFT JOIN workspace.sc_gold.historico_de_servicos_2 AS h\n",
    "#   ON v.id = h.viatura;\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE workspace.sc_gold.join_viatura_historico AS\n",
    "SELECT\n",
    "  v.*,\n",
    "  h.*\n",
    "FROM sc_gold.viaturas_2 AS v\n",
    "LEFT JOIN workspace.sc_gold.historico_de_servicos_2 AS h\n",
    "  ON v.id = h.viatura\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ce92a7-f30d-41bc-95e5-88b49ca41069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Ler a tabela\n",
    "df = spark.table(\"sc_gold.join_viatura_historico\")\n",
    "\n",
    "# Remover a coluna 'production_date_dt'\n",
    "df_clean = df.drop(\"viatura\",\"id\",'descricao_servico_pos_venda','age_year','cilindrada__cm3_', 'motorizacao','potencia_maxima__kw_')\n",
    "\n",
    "# (opcional) sobrescrever a tabela com o dataset limpo\n",
    "(df_clean.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"sc_gold.join_viatura_historico\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fddb4131-774f-4018-96e9-e3d1385cce8a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762299806458}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM workspace.sc_gold.join_viatura_historico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a0c8ad-13a8-4f7f-b3db-2e13e0b3a6c4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762300492331}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#entender nulos depois do join\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the new joined table\n",
    "table_name = \"workspace.sc_gold.join_viatura_historico\"\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Count total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate percentage of nulls for each column\n",
    "null_percentages = (\n",
    "    df.select([\n",
    "        (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db20e0d0-2853-4f5d-8710-ac1053fc1bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#passo usado para remover linhas (excluir linhas vazias na coluna tipo_de_servio) pois o join nao trouxe resultados 98 879 linhas excluidas\n",
    "table_name = \"sc_gold.join_viatura_historico\"\n",
    "df0 = spark.table(table_name)\n",
    "\n",
    "\n",
    "# Aplicar o filtro (excluir linhas vazias na coluna modelo -486 linhas)\n",
    "df = df0.filter(\n",
    "    (F.col(\"tipo_de_servico\").isNotNull())\n",
    "    & (F.trim(F.col(\"tipo_de_servico\")) != \"\")\n",
    "    & (F.lower(F.trim(F.col(\"tipo_de_servico\"))) != \"null\")\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbebf25a-6165-4a89-8e92-f7b5687d05eb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762300499811}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the existing Delta table into a Spark DataFrame\n",
    "df = spark.table(\"workspace.sc_gold.join_viatura_historico\")\n",
    "\n",
    "# Preview the data\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9ca1cfa-8bef-4c6c-aa01-449d0ce999e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99b4c9d-2ebd-4618-af91-3d0d55af1c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Fonte\n",
    "df = spark.table(\"workspace.sc_gold.join_viatura_historico\")\n",
    "\n",
    "GROUP_COLS = [\"modelo\", \"combustivel\", \"age_interval\", \"classe_potencia\"]\n",
    "SERVICE_COL = \"tipo_de_servico\"\n",
    "\n",
    "# 1) Manter s√≥ linhas v√°lidas e remover duplicados por grupo+servi√ßo\n",
    "df_distinct = (\n",
    "    df.select(*GROUP_COLS, SERVICE_COL)\n",
    "      .where(F.col(SERVICE_COL).isNotNull())\n",
    "      .dropDuplicates(GROUP_COLS + [SERVICE_COL])\n",
    ")\n",
    "\n",
    "# 2) Pivot: uma coluna por servi√ßo (count==1 por design ap√≥s dropDuplicates)\n",
    "pivot_df = (\n",
    "    df_distinct\n",
    "      .groupBy(*GROUP_COLS)\n",
    "      .pivot(SERVICE_COL)\n",
    "      .agg(F.count(F.lit(1)))\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "# 3) Garantir bin√°rio (int) em todas as colunas de servi√ßo\n",
    "service_cols = [c for c in pivot_df.columns if c not in GROUP_COLS]\n",
    "for c in service_cols:\n",
    "    # colunas com espa√ßos/acentos existem; usa backticks para referenciar sem erro\n",
    "    pivot_df = pivot_df.withColumn(f\"{c}\", (F.col(f\"`{c}`\") > 0).cast(\"int\"))\n",
    "\n",
    "flags_df = pivot_df\n",
    "\n",
    "display(flags_df)\n",
    "flags_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.sc_gold.join_viatura_historico_flags\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180838fd-e4ce-496c-85ef-e472b1de5356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    " %pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f12c7f-68c9-4adc-8095-4705f4b4469e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1) Imports ---\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# --- 2) Ler e preparar em Spark ---\n",
    "group_cols = [\"modelo\", \"combustivel\", \"age_interval\", \"classe_potencia\"]\n",
    "target_col = \"tipo_de_servico\"\n",
    "\n",
    "df_s = (\n",
    "    spark.table(\"workspace.sc_gold.join_viatura_historico\")\n",
    "         .select(*(group_cols + [target_col]))\n",
    ")\n",
    "\n",
    "# remover nulos nas colunas chave e alvo\n",
    "df_s = df_s.na.drop(subset=group_cols + [target_col])\n",
    "\n",
    "# normalizar para string (evita warnings mais √† frente)\n",
    "df_s = df_s.withColumn(target_col, F.col(target_col).cast(\"string\"))\n",
    "\n",
    "# --- 3) Construir transa√ß√µes em Spark (um basket por combina√ß√£o de atributos) ---\n",
    "transactions_s = (\n",
    "    df_s.groupBy(*[F.col(c) for c in group_cols])\n",
    "        .agg(\n",
    "            F.array_sort(\n",
    "                F.array_distinct(F.collect_list(F.col(target_col)))\n",
    "            ).alias(\"items\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Se quiseres ver amostra:\n",
    "# transactions_s.show(truncate=False)\n",
    "\n",
    "# --- 4) Converter apenas a coluna 'items' para Pandas ---\n",
    "transactions = transactions_s.select(\"items\").toPandas()\n",
    "transactions_list = [list(x) for x in transactions[\"items\"]]\n",
    "num_tx = len(transactions_list)\n",
    "\n",
    "# --- 5) One-hot encode (matriz transa√ß√µes x itens) ---\n",
    "all_items = sorted({it for ts in transactions_list for it in ts})\n",
    "ohe = pd.DataFrame(0, index=range(num_tx), columns=all_items, dtype=int)\n",
    "for i, items in enumerate(transactions_list):\n",
    "    for it in items:\n",
    "        ohe.iat[i, ohe.columns.get_loc(it)] = 1\n",
    "\n",
    "# --- 6) Apriori + Regras ---\n",
    "min_support = 0.05      # ajusta conforme o n¬∫ de cestos\n",
    "min_confidence = 0.6\n",
    "\n",
    "freq = apriori(ohe, min_support=min_support, use_colnames=True)\n",
    "rules = association_rules(freq, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# manter apenas consequente unit√°rio (um √∫nico tipo_de_servico como ‚Äúprevis√£o‚Äù)\n",
    "rules = rules[rules[\"consequents\"].apply(lambda s: len(s) == 1)].copy()\n",
    "\n",
    "# ordenar por lift/conf/sup\n",
    "rules = rules.sort_values([\"lift\", \"confidence\", \"support\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Transa√ß√µes:\", num_tx)\n",
    "print(\"Itemsets frequentes:\", len(freq))\n",
    "print(\"Regras:\", len(rules))\n",
    "rules.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9aee640-6176-4b39-a5f9-bb34d79252ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "BLOCO 44 IGUAL AO 43 MAS COM GRUPOS INCLUIDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd8578c-7d98-4f81-8daa-7fbdcac9e21b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1) Imports ---\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# --- 2) Par√¢metros ---\n",
    "group_cols = [\"modelo\", \"combustivel\", \"age_interval\", \"classe_potencia\"]\n",
    "target_col = \"tipo_de_servico\"\n",
    "min_support = 0.05\n",
    "min_confidence = 0.6\n",
    "\n",
    "# --- 3) Ler e preparar em Spark ---\n",
    "df_s = (\n",
    "    spark.table(\"workspace.sc_gold.join_viatura_historico\")\n",
    "         .select(*(group_cols + [target_col]))\n",
    "         .na.drop(subset=group_cols + [target_col])\n",
    "         .withColumn(target_col, F.col(target_col).cast(\"string\"))\n",
    ")\n",
    "\n",
    "# --- 4) Construir transa√ß√µes em Spark (um basket por combina√ß√£o de atributos) ---\n",
    "transactions_s = (\n",
    "    df_s.groupBy(*[F.col(c) for c in group_cols])\n",
    "        .agg(F.array_sort(F.array_distinct(F.collect_list(F.col(target_col)))).alias(\"items\"))\n",
    ")\n",
    "\n",
    "# Anexar um ID est√°vel para alinhar com o OHE em pandas\n",
    "transactions_s = transactions_s.withColumn(\"tx_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Guardar meta-dados dos cestos (para mostrar grupos depois)\n",
    "tx_meta_pdf = (\n",
    "    transactions_s.select(\"tx_id\", *group_cols, \"items\")\n",
    "                  .orderBy(\"tx_id\")\n",
    "                  .toPandas()\n",
    ")\n",
    "\n",
    "# --- 5) Converter items para listas e construir OHE (bool) ---\n",
    "transactions_list = tx_meta_pdf[\"items\"].apply(list).tolist()\n",
    "num_tx = len(transactions_list)\n",
    "\n",
    "all_items = sorted({it for ts in transactions_list for it in ts})\n",
    "ohe = pd.DataFrame(False, index=tx_meta_pdf[\"tx_id\"], columns=all_items)  # bool desde in√≠cio\n",
    "for tx_id, items in zip(tx_meta_pdf[\"tx_id\"], transactions_list):\n",
    "    for it in items:\n",
    "        ohe.at[tx_id, it] = True\n",
    "\n",
    "# --- 6) Apriori + Regras ---\n",
    "freq = apriori(ohe, min_support=min_support, use_colnames=True)\n",
    "rules = association_rules(freq, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# manter apenas consequente unit√°rio\n",
    "rules = rules[rules[\"consequents\"].apply(lambda s: len(s) == 1)].copy()\n",
    "\n",
    "# ordenar por lift/conf/sup\n",
    "rules = rules.sort_values([\"lift\", \"confidence\", \"support\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Transa√ß√µes:\", num_tx)\n",
    "print(\"Itemsets frequentes:\", len(freq))\n",
    "print(\"Regras:\", len(rules))\n",
    "\n",
    "# --- 7) Mapear cada regra aos cestos (grupos) onde ela acontece ---\n",
    "def baskets_mask_for_itemset(itemset_frozenset):\n",
    "    \"\"\"M√°scara booleana (index=tx_id) onde TODOS os itens do itemset est√£o presentes no cesto.\"\"\"\n",
    "    cols = list(itemset_frozenset)\n",
    "    if len(cols) == 0:\n",
    "        return pd.Series(False, index=ohe.index)\n",
    "    # if algum item n√£o existe nas colunas (raro), devolve tudo False\n",
    "    missing = [c for c in cols if c not in ohe.columns]\n",
    "    if missing:\n",
    "        return pd.Series(False, index=ohe.index)\n",
    "    return ohe[cols].all(axis=1)\n",
    "\n",
    "# Suporte (antecedente ‚à™ consequente) -> cestos onde a regra de facto ocorre\n",
    "support_masks = []\n",
    "support_counts = []\n",
    "examples = []\n",
    "\n",
    "max_examples = 10  # quantos grupos mostrar por regra (amostra)\n",
    "\n",
    "for _, r in rules.iterrows():\n",
    "    ant = r[\"antecedents\"]\n",
    "    cons = r[\"consequents\"]\n",
    "    both = frozenset(set(ant) | set(cons))\n",
    "    m = baskets_mask_for_itemset(both)\n",
    "    idx = ohe.index[m]  # tx_id dos cestos onde a regra ocorre\n",
    "    support_masks.append(m)\n",
    "    support_counts.append(int(m.sum()))\n",
    "    # amostra de grupos (at√© max_examples)\n",
    "    sample_ids = list(idx[:max_examples])\n",
    "    sample_rows = (\n",
    "        tx_meta_pdf[tx_meta_pdf[\"tx_id\"].isin(sample_ids)][[\"tx_id\"] + group_cols]\n",
    "        .to_dict(orient=\"records\")\n",
    "    )\n",
    "    examples.append(sample_rows)\n",
    "\n",
    "# anexar √†s regras\n",
    "rules[\"group_count_support\"] = support_counts\n",
    "rules[\"groups_example_support\"] = examples\n",
    "\n",
    "# (Opcional) tamb√©m podes querer ver onde o ANTECEDENTE (sem exigir o consequente) aparece:\n",
    "ante_counts = []\n",
    "for _, r in rules.iterrows():\n",
    "    m_ant = baskets_mask_for_itemset(r[\"antecedents\"])\n",
    "    ante_counts.append(int(m_ant.sum()))\n",
    "rules[\"group_count_antecedent\"] = ante_counts\n",
    "\n",
    "# Mostrar top 10 com exemplos de grupos\n",
    "cols_to_show = [\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\",\n",
    "                \"group_count_support\", \"group_count_antecedent\", \"groups_example_support\"]\n",
    "rules[cols_to_show].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8283d71f-9d34-4603-82fa-ce84733a921c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rule_idx = 0\n",
    "ant = rules.loc[rule_idx, \"antecedents\"]\n",
    "cons = rules.loc[rule_idx, \"consequents\"]\n",
    "mask_full = baskets_mask_for_itemset(frozenset(set(ant)|set(cons)))\n",
    "tx_ids = ohe.index[mask_full]\n",
    "tx_meta_pdf[tx_meta_pdf[\"tx_id\"].isin(tx_ids)][[\"tx_id\"] + group_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bc95338-ca93-41be-8df8-3b5eba84d861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "OUTRA ALTERNATIVA COM ANTECEDENTE GRUPOS E TIPO SERVI√áO DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d900d191-464f-4972-b121-bae6f80a7d42",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ante_str\":576},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762304409879}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1) Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# --- 2) Par√¢metros ---\n",
    "group_cols   = [\"modelo\", \"combustivel\", \"age_interval\", \"classe_potencia\"]  # ANTECEDENTE (sempre estes 4)\n",
    "target_col   = \"tipo_de_servico\"                                             # CONSEQUENTE (1 servi√ßo)\n",
    "min_support  = 0.005    # ~0.5% (ajusta conforme volume)\n",
    "min_conf     = 0.60\n",
    "lift_min     = 1.0\n",
    "\n",
    "# --- 3) Ler em Spark apenas colunas necess√°rias ---\n",
    "df_s = (spark.table(\"workspace.sc_gold.join_viatura_historico\")\n",
    "            .select(*(group_cols + [target_col]))\n",
    "            .na.drop(subset=group_cols + [target_col])\n",
    "            .withColumn(target_col, F.col(target_col).cast(\"string\")))\n",
    "\n",
    "# (Opcional) normalizar combustivel/modelo aqui, se quiseres agrupar melhor\n",
    "\n",
    "# --- 4) Converter para Pandas (apenas as colunas necess√°rias) ---\n",
    "pdf = df_s.toPandas()\n",
    "\n",
    "# --- 5) Construir transa√ß√µes a n√≠vel de LINHA (cada registo = 1 cesto) ---\n",
    "#   Antecedente: 4 features codificadas como \"chave=valor\"\n",
    "#   Consequente: 1 servi√ßo \"svc:...\"\n",
    "def build_items_row(row):\n",
    "    feats = [f\"{col}={str(row[col])}\" for col in group_cols]\n",
    "    svc   = [f\"svc:{str(row[target_col])}\"]\n",
    "    return feats + svc\n",
    "\n",
    "pdf[\"items_all\"] = pdf.apply(build_items_row, axis=1)\n",
    "transactions_list = pdf[\"items_all\"].tolist()\n",
    "num_tx = len(transactions_list)\n",
    "\n",
    "# --- 6) One-hot encode (bool) ---\n",
    "all_items = sorted({it for ts in transactions_list for it in ts})\n",
    "# cuidado com mem√≥ria: isto cria (num_tx x n_itens). Se ficar grande, podemos amostrar.\n",
    "ohe = pd.DataFrame(False, index=range(num_tx), columns=all_items)\n",
    "for i, items in enumerate(transactions_list):\n",
    "    for it in items:\n",
    "        ohe.iat[i, ohe.columns.get_loc(it)] = True\n",
    "\n",
    "# --- 7) Apriori + Regras ---\n",
    "freq  = apriori(ohe, min_support=min_support, use_colnames=True, max_len=5)  # at√© 5 itens (4 feats + 1 svc)\n",
    "rules = association_rules(freq, metric=\"confidence\", min_threshold=min_conf)\n",
    "\n",
    "# Normalizar tipos\n",
    "rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: frozenset(list(x)))\n",
    "rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: frozenset(list(x)))\n",
    "\n",
    "# --- 8) Filtros: antecedente = EXACTAMENTE 4 features (1 de cada), consequente = 1 servi√ßo ---\n",
    "def is_full_feature_set(s):\n",
    "    if len(s) != 4:\n",
    "        return False\n",
    "    prefixes = {\"modelo=\", \"combustivel=\", \"age_interval=\", \"classe_potencia=\"}\n",
    "    return all(any(str(it).startswith(p) for p in prefixes) for it in s) and \\\n",
    "           {next(p for p in prefixes if str(it).startswith(p)) for it in s} == prefixes\n",
    "\n",
    "def only_service(s):\n",
    "    return len(s) == 1 and all(str(it).startswith(\"svc:\") for it in s)\n",
    "\n",
    "mask_ante = rules[\"antecedents\"].apply(is_full_feature_set).astype(bool)\n",
    "mask_cons = rules[\"consequents\"].apply(only_service).astype(bool)\n",
    "\n",
    "rules = rules[mask_ante & mask_cons].copy()\n",
    "rules = rules[rules[\"lift\"] >= lift_min].copy()\n",
    "\n",
    "# Strings leg√≠veis\n",
    "rules[\"ante_str\"] = rules[\"antecedents\"].apply(lambda x: \", \".join(sorted(list(x))))\n",
    "rules[\"cons_str\"] = rules[\"consequents\"].apply(lambda x: \", \".join(sorted(list(x))))\n",
    "\n",
    "# Ordenar\n",
    "rules = rules.sort_values([\"lift\",\"confidence\",\"support\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Transa√ß√µes:\", num_tx)\n",
    "print(\"Itemsets frequentes:\", len(freq))\n",
    "print(\"Regras (4 features ‚Üí 1 servi√ßo):\", len(rules))\n",
    "display(rules[[\"ante_str\",\"cons_str\",\"support\",\"confidence\",\"lift\"]].head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3da6903-7e87-49e8-9164-1a4f673eee09",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"cons_str\":146},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762303848938}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1) Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from pyspark.sql import functions as F\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# --- 2) Par√¢metros ---\n",
    "group_cols = [\"modelo\", \"combustivel\", \"age_interval\", \"classe_potencia\"]\n",
    "target_col = \"tipo_de_servico\"\n",
    "min_support = 0.05          # ajusta conforme o n¬∫ de cestos\n",
    "min_confidence = 0.60       # confian√ßa m√≠nima\n",
    "lift_min = 1.0              # >=1 => associa√ß√£o positiva\n",
    "max_examples = 10           # n¬∫ de grupos a mostrar por regra (amostra)\n",
    "\n",
    "# --- 3) Ler e preparar em Spark ---\n",
    "df_s = (\n",
    "    spark.table(\"workspace.sc_gold.join_viatura_historico\")\n",
    "         .select(*(group_cols + [target_col]))\n",
    "         .na.drop(subset=group_cols + [target_col])\n",
    "         .withColumn(target_col, F.col(target_col).cast(\"string\"))\n",
    ")\n",
    "\n",
    "# --- 4) Construir transa√ß√µes (um basket por combina√ß√£o de atributos) ---\n",
    "transactions_s = (\n",
    "    df_s.groupBy(*[F.col(c) for c in group_cols])\n",
    "        .agg(F.array_sort(F.array_distinct(F.collect_list(F.col(target_col)))).alias(\"svc_items\"))\n",
    "        .withColumn(\"tx_id\", F.monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "# Metadados dos cestos (grupos)\n",
    "tx_meta_pdf = (\n",
    "    transactions_s.select(\"tx_id\", *group_cols, \"svc_items\")\n",
    "                  .orderBy(\"tx_id\")\n",
    "                  .toPandas()\n",
    ")\n",
    "\n",
    "# --- 5) Criar itens do basket: features (=antecedente) + servi√ßos (=consequente)\n",
    "def build_items_row(row):\n",
    "    # atributos (antecedente)\n",
    "    feats = [f\"{col}={str(row[col])}\" for col in group_cols]\n",
    "\n",
    "    # servi√ßos (consequente) ‚Äî normalizar para lista de strings (sem usar \"or []\")\n",
    "    val = row.get(\"svc_items\", None)\n",
    "\n",
    "    if isinstance(val, (list, tuple, set)):\n",
    "        items = list(val)\n",
    "    elif isinstance(val, np.ndarray):\n",
    "        items = val.tolist()\n",
    "    elif isinstance(val, str) and val.strip().startswith(\"[\") and val.strip().endswith(\"]\"):\n",
    "        # string com formato de lista\n",
    "        try:\n",
    "            parsed = literal_eval(val)\n",
    "            items = list(parsed) if isinstance(parsed, (list, tuple, set)) else [val]\n",
    "        except Exception:\n",
    "            items = [val]\n",
    "    elif val is None or (isinstance(val, float) and pd.isna(val)):\n",
    "        items = []\n",
    "    else:\n",
    "        items = [val]\n",
    "\n",
    "    svcs = [f\"svc:{str(s)}\" for s in items if pd.notna(s) and str(s).strip() != \"\"]\n",
    "    return feats + svcs\n",
    "\n",
    "tx_meta_pdf[\"items_all\"] = tx_meta_pdf.apply(build_items_row, axis=1)\n",
    "transactions_list = tx_meta_pdf[\"items_all\"].tolist()\n",
    "num_tx = len(transactions_list)\n",
    "\n",
    "# --- 6) One-hot encode (bool) ---\n",
    "all_items = sorted({it for ts in transactions_list for it in ts})\n",
    "ohe = pd.DataFrame(False, index=tx_meta_pdf[\"tx_id\"], columns=all_items)\n",
    "for tx_id, items in zip(tx_meta_pdf[\"tx_id\"], transactions_list):\n",
    "    for it in items:\n",
    "        if it in ohe.columns:\n",
    "            ohe.at[tx_id, it] = True\n",
    "\n",
    "# --- 7) Apriori + Regras ---\n",
    "freq = apriori(ohe, min_support=min_support, use_colnames=True)\n",
    "rules = association_rules(freq, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# Normalizar tipos (garante frozenset)\n",
    "rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: frozenset(list(x)))\n",
    "rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: frozenset(list(x)))\n",
    "\n",
    "# 7.1 Consequente unit√°rio\n",
    "rules = rules[rules[\"consequents\"].apply(lambda s: len(s) == 1)].copy()\n",
    "\n",
    "# 7.2 Filtrar formato (antecedente s√≥ features; consequente s√≥ servi√ßos \"svc:\")\n",
    "def only_features(s):\n",
    "    return all(\n",
    "        it.startswith(\"modelo=\") or\n",
    "        it.startswith(\"combustivel=\") or\n",
    "        it.startswith(\"age_interval=\") or\n",
    "        it.startswith(\"classe_potencia=\")\n",
    "        for it in s\n",
    "    )\n",
    "\n",
    "def only_service(s):\n",
    "    return all(it.startswith(\"svc:\") for it in s)\n",
    "\n",
    "mask_ante = rules[\"antecedents\"].apply(only_features).astype(bool)\n",
    "mask_cons = rules[\"consequents\"].apply(only_service).astype(bool)\n",
    "rules = rules[mask_ante & mask_cons].copy()\n",
    "\n",
    "# 7.3 Filtro de qualidade opcional\n",
    "rules = rules[rules[\"lift\"] >= lift_min].copy()\n",
    "\n",
    "# 7.4 Strings leg√≠veis\n",
    "rules[\"ante_str\"] = rules[\"antecedents\"].apply(lambda x: \", \".join(sorted(list(x))))\n",
    "rules[\"cons_str\"] = rules[\"consequents\"].apply(lambda x: \", \".join(sorted(list(x))))\n",
    "\n",
    "# Ordenar por lift/conf/sup\n",
    "rules = rules.sort_values([\"lift\", \"confidence\", \"support\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Transa√ß√µes:\", num_tx)\n",
    "print(\"Itemsets frequentes:\", len(freq))\n",
    "print(\"Regras (features ‚Üí servi√ßo):\", len(rules))\n",
    "display(rules[[\"ante_str\",\"cons_str\",\"support\",\"confidence\",\"lift\"]].head(20))  # Databricks-friendly\n",
    "\n",
    "# --- 8) (Opcional) Mapear regras aos grupos concretos onde ocorrem (exemplos) ---\n",
    "def baskets_mask_for_itemset(itemset_frozenset):\n",
    "    cols = list(itemset_frozenset)\n",
    "    if not cols:\n",
    "        return pd.Series(False, index=ohe.index)\n",
    "    missing = [c for c in cols if c not in ohe.columns]\n",
    "    if missing:\n",
    "        return pd.Series(False, index=ohe.index)\n",
    "    return ohe[cols].all(axis=1)\n",
    "\n",
    "support_counts, ante_counts, examples = [], [], []\n",
    "for _, r in rules.iterrows():\n",
    "    ant = r[\"antecedents\"]\n",
    "    cons = r[\"consequents\"]\n",
    "    both = frozenset(set(ant) | set(cons))\n",
    "\n",
    "    m_support = baskets_mask_for_itemset(both)     # onde antecedente+consequente ocorrem\n",
    "    m_ante    = baskets_mask_for_itemset(ant)      # onde antecedente ocorre\n",
    "\n",
    "    support_counts.append(int(m_support.sum()))\n",
    "    ante_counts.append(int(m_ante.sum()))\n",
    "\n",
    "    sample_ids = list(ohe.index[m_support][:max_examples])\n",
    "    sample_rows = (\n",
    "        tx_meta_pdf[tx_meta_pdf[\"tx_id\"].isin(sample_ids)][[\"tx_id\"] + group_cols]\n",
    "        .to_dict(orient=\"records\")\n",
    "    )\n",
    "    examples.append(sample_rows)\n",
    "\n",
    "rules[\"group_count_support\"] = support_counts\n",
    "rules[\"group_count_antecedent\"] = ante_counts\n",
    "rules[\"groups_example_support\"] = examples\n",
    "\n",
    "# Ver top 10 com exemplos\n",
    "cols_to_show = [\"ante_str\",\"cons_str\",\"support\",\"confidence\",\"lift\",\n",
    "                \"group_count_support\",\"group_count_antecedent\",\"groups_example_support\"]\n",
    "display(rules[cols_to_show].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18e6bdc-75d6-4371-85fe-7a27d5abaebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Converter frozenset -> string para facilitar o filtro\n",
    "rules[\"ante_str\"] = rules[\"antecedents\"].apply(lambda x: \", \".join(sorted(list(x))))\n",
    "rules[\"cons_str\"] = rules[\"consequents\"].apply(lambda x: \", \".join(sorted(list(x))))\n",
    "\n",
    "# Definir modelos que te interessam (podes alterar)\n",
    "model_keywords = [\"TUCSON\",\"Kauai Ev\" \"I20\", \"I30\", \"KONA\", \"KAUAI\", \"IX35\", \"IONIQ\", \"SANTA FE\"]\n",
    "\n",
    "# Filtrar regras onde o ANTECEDENTE cont√©m um modelo e o CONSEQUENTE √© um servi√ßo\n",
    "rules_filtered = rules[\n",
    "    rules[\"ante_str\"].str.contains(\"|\".join(model_keywords), case=False)\n",
    "    & ~rules[\"cons_str\"].str.contains(\"|\".join(model_keywords), case=False)\n",
    "]\n",
    "\n",
    "# Top 10 por lift\n",
    "top_rules = rules_filtered.nlargest(10, \"lift\").copy()\n",
    "\n",
    "# Criar label \"ante ‚Üí cons\"\n",
    "top_rules[\"rule_label\"] = top_rules[\"ante_str\"] + \"  ‚Üí  \" + top_rules[\"cons_str\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_rules[\"rule_label\"], top_rules[\"lift\"])\n",
    "plt.xlabel(\"Lift (for√ßa da associa√ß√£o)\")\n",
    "plt.title(\"Top 10 Regras Modelo ‚Üí Tipo de Servi√ßo\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62008339-40ca-42cc-9545-b89b0ac0b3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(rules_filtered['support'], rules_filtered['confidence'], alpha=0.7)\n",
    "plt.xlabel(\"Support (frequency)\")\n",
    "plt.ylabel(\"Confidence (reliability)\")\n",
    "plt.title(\"Model ‚Üí Service Rule Strength\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4af496-ae98-4414-ad3b-34e632810aec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a pivot table of lift\n",
    "pivot = rules_filtered.pivot_table(index='antecedents', columns='consequents', values='lift')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Lift by Model and Service Type\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlabel(\"Service Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17fa64b1-38cf-408d-8a52-b0385a85c0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FP-GROWTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f13e96-24f6-4408-86de-59ab6deccaab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 0) (se faltar) ---\n",
    "# %pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Parto do dataframe pandas `pdf` que cri√°mos no Apriori ‚Äúpor linha‚Äù:\n",
    "# pdf tem colunas: modelo, combustivel, age_interval, classe_potencia, tipo_de_servico\n",
    "\n",
    "group_cols = [\"modelo\",\"combustivel\",\"age_interval\",\"classe_potencia\"]\n",
    "target_col = \"tipo_de_servico\"\n",
    "\n",
    "# 1) Transa√ß√µes por linha: 4 features + 1 servi√ßo\n",
    "def build_items_row(row):\n",
    "    feats = [f\"{c}={row[c]}\" for c in group_cols]\n",
    "    svc   = [f\"svc:{row[target_col]}\"]\n",
    "    return feats + svc\n",
    "\n",
    "pdf = pdf.dropna(subset=group_cols + [target_col]).copy()\n",
    "pdf[\"items_all\"] = pdf.apply(build_items_row, axis=1)\n",
    "transactions_list = pdf[\"items_all\"].tolist()\n",
    "\n",
    "# 2) One-hot encode booleano\n",
    "all_items = sorted({it for ts in transactions_list for it in ts})\n",
    "ohe = pd.DataFrame(False, index=range(len(transactions_list)), columns=all_items)\n",
    "for i, items in enumerate(transactions_list):\n",
    "    for it in items:\n",
    "        ohe.iat[i, ohe.columns.get_loc(it)] = True\n",
    "\n",
    "# 3) FP-Growth (mlxtend) + regras\n",
    "min_support = 0.005   # ~0.5% (ajusta)\n",
    "min_conf    = 0.60\n",
    "min_lift    = 1.0\n",
    "\n",
    "freq = fpgrowth(ohe, min_support=min_support, use_colnames=True, max_len=5)   # at√© 5 itens (4 feats + 1 svc)\n",
    "rules = association_rules(freq, metric=\"confidence\", min_threshold=min_conf)\n",
    "\n",
    "# 4) Normalizar tipos\n",
    "rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: frozenset(list(x)))\n",
    "rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: frozenset(list(x)))\n",
    "\n",
    "# 5) Filtros: antecedente = EXACTAMENTE 4 itens, 1 de cada prefixo; consequente = 1 servi√ßo (svc:)\n",
    "def is_full_feature_set(s):\n",
    "    if len(s) != 4:\n",
    "        return False\n",
    "    prefixes = {\"modelo=\", \"combustivel=\", \"age_interval=\", \"classe_potencia=\"}\n",
    "    hits = {next((p for p in prefixes if str(it).startswith(p)), None) for it in s}\n",
    "    return None not in hits and hits == prefixes\n",
    "\n",
    "def only_service(s):\n",
    "    return len(s) == 1 and all(str(it).startswith(\"svc:\") for it in s)\n",
    "\n",
    "mask_ante = rules[\"antecedents\"].apply(is_full_feature_set)\n",
    "mask_cons = rules[\"consequents\"].apply(only_service)\n",
    "rules = rules[mask_ante & mask_cons].copy()\n",
    "\n",
    "rules = rules[rules[\"lift\"] >= min_lift].copy()\n",
    "\n",
    "# 6) Human readable\n",
    "rules[\"ante_str\"] = rules[\"antecedents\"].apply(lambda s: \", \".join(sorted(list(s))))\n",
    "rules[\"cons_str\"] = rules[\"consequents\"].apply(lambda s: \", \".join(sorted(list(s))))\n",
    "rules = rules.sort_values([\"lift\",\"confidence\",\"support\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Regras (4 features ‚Üí 1 servi√ßo):\", len(rules))\n",
    "rules.head(20)[[\"ante_str\",\"cons_str\",\"support\",\"confidence\",\"lift\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd92e1e3-f6e3-4b81-867f-c26072b8ebff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_topN_rules(rules_df, metric=\"lift\", topn=10, title=None):\n",
    "    \"\"\"Desenha Top-N regras usando matplotlib, ordenadas por 'metric'.\"\"\"\n",
    "    df = rules_df.copy()\n",
    "\n",
    "    # Garantir colunas leg√≠veis\n",
    "    if \"ante_str\" not in df:\n",
    "        df[\"ante_str\"] = df[\"antecedents\"].apply(\n",
    "            lambda s: \", \".join(sorted(list(s))) if not isinstance(s, str) else s\n",
    "        )\n",
    "    if \"cons_str\" not in df:\n",
    "        df[\"cons_str\"] = df[\"consequents\"].apply(\n",
    "            lambda s: \", \".join(sorted(list(s))) if not isinstance(s, str) else s\n",
    "        )\n",
    "\n",
    "    # Tirar prefixo 'svc:' do consequente, s√≥ para ficar mais bonito\n",
    "    df[\"cons_str\"] = df[\"cons_str\"].str.replace(r\"^svc:\\s*\", \"\", regex=True)\n",
    "\n",
    "    # Preparar Top-N\n",
    "    df = df.sort_values(metric, ascending=False).head(topn).copy()\n",
    "    df[\"label\"] = df[\"ante_str\"] + \"  ‚Üí  \" + df[\"cons_str\"]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(df[\"label\"], df[metric])\n",
    "    plt.xlabel(metric.title())\n",
    "    plt.title(title or f\"Top {topn} regras por {metric}\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exemplo de uso:\n",
    "# Top-10 por lift\n",
    "plot_topN_rules(rules, metric=\"lift\", topn=10, title=\"Top 10 regras (features ‚Üí servi√ßo) por Lift\")\n",
    "\n",
    "# Top-10 por confidence\n",
    "plot_topN_rules(rules, metric=\"confidence\", topn=10, title=\"Top 10 regras por Confidence\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5224107540740881,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Final_Association_Rules",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
