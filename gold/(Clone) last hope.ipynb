{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a85524-9a08-454a-b277-ae0908102fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c7ed61-bd4e-4e24-b71d-40713762b132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "DROP TABLE IF EXISTS sc_gold.deals_2;\n",
    "\n",
    "CREATE TABLE sc_gold.deals_2 AS\n",
    "SELECT conta_name,data_venda,modelos,data_decisao_negocio,tipo_cliente_negocio\n",
    "FROM sc_gold.deals;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd711d7-42ed-4bb0-83e9-f08ecd29a6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType, DecimalType\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f33ad20-eb5a-46b3-92f3-ec11b2fd9f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"sc_gold.deals_2\"\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Get total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate percentage of nulls for each column\n",
    "null_percentages = (\n",
    "    df.select([\n",
    "        (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100)\n",
    "        .alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b52a4ba-dc97-4729-a376-e678a320a69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#passo usado para remover linhas (excluir linhas vazias na coluna clientes)\n",
    "table_name = \"sc_gold.deals_2\"\n",
    "df0 = spark.table(table_name)\n",
    "\n",
    "\n",
    "# Aplicar o filtro\n",
    "df = df0.filter(\n",
    "    (F.col(\"conta_name\").isNotNull())\n",
    "    & (F.trim(F.col(\"conta_name\")) != \"\")\n",
    "    & (F.lower(F.trim(F.col(\"conta_name\"))) != \"null\")\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9adbb7bb-a12a-47d3-b299-41f6a8c87a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE sc_gold.deals_2\n",
    "SET data_venda = data_decisao_negocio\n",
    "WHERE data_venda IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6940f75-f668-4212-a243-18db269b88a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Passo usado para remover coluna \"data_decisao_negocio\" \n",
    "\n",
    "#  Ler a tabela\n",
    "df = spark.table(\"sc_gold.deals_2\")\n",
    "\n",
    "# Remover a coluna 'data_decisao_negocio'\n",
    "df_clean = df.drop(\"data_decisao_negocio\")\n",
    "\n",
    "# (opcional) sobrescrever a tabela com o dataset limpo\n",
    "(df_clean.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"sc_gold.deals_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd2edde-9d44-4ee9-877d-e4712f9ac816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#passo usado para remover linhas (excluir linhas vazias na coluna data de venda -DEPOIS VER SE POSSO IR BUSCAR ESTA INFORMAÇÃO A ALGUM JOIN)\n",
    "table_name = \"sc_gold.deals_2\"\n",
    "df0 = spark.table(table_name)\n",
    "\n",
    "\n",
    "# Aplicar o filtro\n",
    "df = df0.filter(\n",
    "    (F.col(\"data_venda\").isNotNull())\n",
    "    & (F.trim(F.col(\"data_venda\")) != \"\")\n",
    "    & (F.lower(F.trim(F.col(\"data_venda\"))) != \"null\")\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ef2e4d-67e1-4299-9195-6d6b86746979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#passo usado para remover linhas (excluir linhas vazias na coluna data de modelos -DEPOIS VER SE POSSO IR BUSCAR ESTA INFORMAÇÃO A ALGUM JOIN)\n",
    "table_name = \"sc_gold.deals_2\"\n",
    "df0 = spark.table(table_name)\n",
    "\n",
    "\n",
    "# Aplicar o filtro\n",
    "df = df0.filter(\n",
    "    (F.col(\"modelos\").isNotNull())\n",
    "    & (F.trim(F.col(\"modelos\")) != \"\")\n",
    "    & (F.lower(F.trim(F.col(\"modelos\"))) != \"null\")\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5360a56d-d2e9-4914-b8b5-2af87eb2a6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"sc_gold.deals_2\"\n",
    "\n",
    "# Ler tabela existente\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Filtrar apenas clientes de negócio do tipo 'Frota'\n",
    "df = df.filter(df.tipo_cliente_negocio == 'Frota')\n",
    "\n",
    "# Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baedfb87-d400-495c-b97a-0091dfc92a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"sc_gold.deals_2\"\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Get total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate percentage of nulls for each column\n",
    "null_percentages = (\n",
    "    df.select([\n",
    "        (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100)\n",
    "        .alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691d84b3-d3ea-4dba-a956-1b3d116fa083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Market Basquet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e94328-db2c-4980-b154-e504be3c14f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TABLE_SRC = \"sc_gold.deals_2\"  # origem\n",
    "MIN_SUPPORT = 0.005             # 1% (0.01)\n",
    "MIN_CONFIDENCE = 0.20          # 30% (0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b4ce09-e6f6-4487-8bca-dc0f71bd7963",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"transaction_id\":242,\"H-1 3 Lugares\":133,\"H-1 6 Lugares\":134,\"H-1 8 Lugares\":132},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762467157643}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(\"sc_gold.deals_2\").select(\"conta_name\",\"data_venda\",\"modelos\")\n",
    "\n",
    "# chave de transação\n",
    "df = df.withColumn(\"transaction_id\",\n",
    "                   F.concat_ws(\"_\", \"conta_name\", F.date_format(\"data_venda\",\"yyyyMMdd\")))\n",
    "\n",
    "# pivot para formato wide (binary 0/1)\n",
    "df_flags = (\n",
    "    df.groupBy(\"transaction_id\")\n",
    "      .pivot(\"modelos\")\n",
    "      .agg(F.lit(1))\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "display(df_flags.limit(10))\n",
    "print(\"Total colunas:\", len(df_flags.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db0a0e6-4e11-48fd-9129-67d1d33cbd04",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"count\":137},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762473917276}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      },
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762474002737}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Basket (flags binárias) + Resumo + Pares com Lift (Spark)\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Fonte de dados e basket 0/1\n",
    "# -----------------------------\n",
    "df = spark.table(\"sc_gold.deals_2\").select(\"conta_name\",\"data_venda\",\"modelos\")\n",
    "\n",
    "# ID de transação: por conta + data (ajuste se necessário)\n",
    "df = df.withColumn(\n",
    "    \"transaction_id\",\n",
    "    F.concat_ws(\"_\", \"conta_name\", F.date_format(\"data_venda\",\"yyyyMMdd\"))\n",
    ")\n",
    "\n",
    "# Basket binário (wide)\n",
    "df_flags = (\n",
    "    df.groupBy(\"transaction_id\")\n",
    "      .pivot(\"modelos\")\n",
    "      .agg(F.lit(1))\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "display(df_flags.limit(10))\n",
    "print(\"Total de colunas (inclui transaction_id):\", len(df_flags.columns))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Resumo descritivo (itens)\n",
    "# -----------------------------\n",
    "n_trans = df_flags.count()\n",
    "item_cols = [c for c in df_flags.columns if c != \"transaction_id\"]\n",
    "n_items = len(item_cols)\n",
    "\n",
    "# Suporte individual: sum(col)/n_trans\n",
    "agg_exprs = [F.sum(F.col(c)).alias(c) for c in item_cols]\n",
    "freq_pdf = (\n",
    "    df_flags.agg(*agg_exprs)\n",
    "            .toPandas()\n",
    "            .T.reset_index()\n",
    "            .rename(columns={\"index\":\"item\", 0:\"count\"})\n",
    ")\n",
    "freq_pdf[\"support\"] = freq_pdf[\"count\"] / n_trans\n",
    "freq_pdf = freq_pdf.sort_values(\"support\", ascending=False)\n",
    "\n",
    "# Métricas para o slide\n",
    "top10_pdf = freq_pdf.head(10).copy()\n",
    "cov_top10 = top10_pdf[\"count\"].sum() / n_trans\n",
    "sup_max = freq_pdf[\"support\"].max() if not freq_pdf.empty else 0.0\n",
    "pct_long_tail_lt1 = (freq_pdf[\"support\"] < 0.01).mean() if not freq_pdf.empty else 0.0\n",
    "\n",
    "# Tamanho de cesta (itens por transação)\n",
    "sum_expr = None\n",
    "for c in item_cols:\n",
    "    sum_expr = (F.col(c) if sum_expr is None else sum_expr + F.col(c))\n",
    "df_basket_size = df_flags.select(\"transaction_id\", sum_expr.alias(\"basket_size\"))\n",
    "\n",
    "basket_stats = df_basket_size.groupBy(\"basket_size\").count().orderBy(\"basket_size\")\n",
    "display(basket_stats)\n",
    "\n",
    "avg_basket = df_basket_size.agg(F.avg(\"basket_size\")).first()[0]\n",
    "pct_ge2 = df_basket_size.filter(F.col(\"basket_size\") >= 2).count() / n_trans\n",
    "\n",
    "print(\"\\n===== RESUMO (para slide) =====\")\n",
    "print(f\"Transações: {n_trans:,}\")\n",
    "print(f\"Itens (modelos) distintos: {n_items}\")\n",
    "print(f\"Suporte do item mais frequente: {sup_max:.2%}\")\n",
    "print(f\"Cobertura do Top 10 itens: {cov_top10:.2%}\")\n",
    "print(f\"Média de itens por transação: {avg_basket:.2f}\")\n",
    "print(f\"% de transações com 2+ itens: {pct_ge2:.2%}\")\n",
    "print(f\"% de itens com suporte < 1% (long tail): {pct_long_tail_lt1:.2%}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Gráfico: Top 15 itens\n",
    "# -----------------------------\n",
    "if not freq_pdf.empty:\n",
    "    top15 = freq_pdf.head(15).copy()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.barh(top15[\"item\"], top15[\"support\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Support (proporção das transações)\")\n",
    "    plt.title(\"Top 15 modelos mais frequentes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Sem itens para plotar (freq_pdf vazio).\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) PARES (A,B): suporte, confiança e LIFT\n",
    "#    - reconstrói lista de itens por transação (a partir de df_flags)\n",
    "#    - gera combinações (A,B) com i<j\n",
    "# -----------------------------\n",
    "\n",
    "# 3.1) Long format: (transaction_id, item) somente onde flag=1\n",
    "#     Usamos 'stack' para transformar colunas em linhas.\n",
    "stack_expr = \"stack({n}, {pairs}) as (flag, item)\".format(\n",
    "    n=len(item_cols),\n",
    "    pairs=\", \".join([f\"`{c}`, '{c}'\" for c in item_cols])\n",
    ")\n",
    "\n",
    "long_flags = (\n",
    "    df_flags.select(\"transaction_id\", F.expr(stack_expr))\n",
    "            .where(F.col(\"flag\") == 1)\n",
    "            .select(\"transaction_id\", \"item\")\n",
    ")\n",
    "\n",
    "# 3.2) Itens por transação (array)\n",
    "tx_items = long_flags.groupBy(\"transaction_id\").agg(F.collect_set(\"item\").alias(\"items\"))\n",
    "\n",
    "# 3.3) Gerar pares (A,B) por transação (i<j)\n",
    "a = tx_items.select(\"transaction_id\", F.posexplode(\"items\").alias(\"i\",\"A\"))\n",
    "b = tx_items.select(\"transaction_id\", F.posexplode(\"items\").alias(\"j\",\"B\"))\n",
    "pairs = (\n",
    "    a.join(b, \"transaction_id\")\n",
    "     .where(F.col(\"i\") < F.col(\"j\"))\n",
    "     .select(F.array_sort(F.array(\"A\",\"B\")).alias(\"AB\"))\n",
    ")\n",
    "\n",
    "pairs = pairs.select(\n",
    "    F.col(\"AB\")[0].alias(\"A\"),\n",
    "    F.col(\"AB\")[1].alias(\"B\")\n",
    ")\n",
    "\n",
    "# 3.4) Suportes 1-item e 2-itens\n",
    "support_1 = (\n",
    "    long_flags.groupBy(\"item\")\n",
    "              .agg(F.countDistinct(\"transaction_id\").alias(\"cnt\"))\n",
    "              .withColumn(\"support\", F.col(\"cnt\")/F.lit(n_trans))\n",
    "              .withColumnRenamed(\"item\",\"X\")\n",
    ")\n",
    "\n",
    "support_2 = (\n",
    "    pairs.groupBy(\"A\",\"B\")\n",
    "         .agg(F.count(\"*\").alias(\"cnt\"))\n",
    "         .withColumn(\"support_ab\", F.col(\"cnt\")/F.lit(n_trans))\n",
    ")\n",
    "\n",
    "# 3.5) Juntar sup(A), sup(B) e calcular métricas\n",
    "rules_pairs = (\n",
    "    support_2\n",
    "      .join(support_1.withColumnRenamed(\"X\",\"A\").withColumnRenamed(\"support\",\"sup_a\"), on=\"A\", how=\"left\")\n",
    "      .join(support_1.withColumnRenamed(\"X\",\"B\").withColumnRenamed(\"support\",\"sup_b\"), on=\"B\", how=\"left\")\n",
    "      .withColumn(\"confidence_a_to_b\", F.col(\"support_ab\")/F.col(\"sup_a\"))\n",
    "      .withColumn(\"confidence_b_to_a\", F.col(\"support_ab\")/F.col(\"sup_b\"))\n",
    "      .withColumn(\"lift\", F.col(\"support_ab\")/(F.col(\"sup_a\")*F.col(\"sup_b\")))\n",
    "      .orderBy(F.desc(\"lift\"), F.desc(\"support_ab\"))\n",
    ")\n",
    "\n",
    "display(rules_pairs.limit(50))\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Heatmap de LIFT dos pares mais relevantes\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "\n",
    "rules_pairs_pdf = rules_pairs.toPandas()\n",
    "if not rules_pairs_pdf.empty:\n",
    "    # Selecionar TOP A e B por maior lift máximo\n",
    "    top_a = (rules_pairs_pdf.groupby(\"A\")[\"lift\"].max().sort_values(ascending=False).head(15)).index.tolist()\n",
    "    top_b = (rules_pairs_pdf.groupby(\"B\")[\"lift\"].max().sort_values(ascending=False).head(15)).index.tolist()\n",
    "\n",
    "    mat = rules_pairs_pdf[\n",
    "        rules_pairs_pdf[\"A\"].isin(top_a) & rules_pairs_pdf[\"B\"].isin(top_b)\n",
    "    ].pivot_table(index=\"A\", columns=\"B\", values=\"lift\", fill_value=0)\n",
    "\n",
    "    if not mat.empty:\n",
    "        plt.figure(figsize=(14,8))\n",
    "        plt.imshow(mat.values, aspect='auto')\n",
    "        plt.xticks(range(len(mat.columns)), mat.columns, rotation=90)\n",
    "        plt.yticks(range(len(mat.index)), mat.index)\n",
    "        plt.title(\"Lift — Pares (A,B)\")\n",
    "        plt.colorbar(label=\"Lift\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Sem matriz suficiente para o heatmap de pares (filtro TOP vazio).\")\n",
    "else:\n",
    "    print(\"Regras de pares vazias; sem heatmap.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Tabela final para slide: TOP pares (filtros úteis)\n",
    "# -----------------------------\n",
    "MIN_SUP_AB   = 0.01   # ≥1% das transações (ajuste conforme volume)\n",
    "MIN_CONF     = 0.20   # ≥20% confiança\n",
    "MIN_LIFT     = 1.2    # lift acima de 1 indica associação positiva\n",
    "\n",
    "rules_pairs_nice = (\n",
    "    rules_pairs\n",
    "      .where( (F.col(\"support_ab\")>=MIN_SUP_AB) &\n",
    "              ((F.col(\"confidence_a_to_b\")>=MIN_CONF) | (F.col(\"confidence_b_to_a\")>=MIN_CONF)) &\n",
    "              (F.col(\"lift\")>=MIN_LIFT) )\n",
    "      .select(\n",
    "          \"A\",\"B\",\n",
    "          (F.col(\"support_ab\")).alias(\"support_ab\"),\n",
    "          F.round(F.col(\"confidence_a_to_b\"),4).alias(\"conf_A_to_B\"),\n",
    "          F.round(F.col(\"confidence_b_to_a\"),4).alias(\"conf_B_to_A\"),\n",
    "          F.round(F.col(\"lift\"),4).alias(\"lift\")\n",
    "      )\n",
    "      .orderBy(F.desc(\"lift\"), F.desc(\"support_ab\"))\n",
    ")\n",
    "\n",
    "display(rules_pairs_nice.limit(50))\n",
    "\n",
    "# -----------------------------\n",
    "# 6) (Opcional) Exportar tabelas ao catálogo\n",
    "# -----------------------------\n",
    "# Ajuste o nome do schema conforme seu ambiente\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS sc_gold\")\n",
    "rules_pairs.write.mode(\"overwrite\").saveAsTable(\"sc_gold.mba_pairs_rules\")\n",
    "df_flags.write.mode(\"overwrite\").saveAsTable(\"sc_gold.mba_basket_flags\")\n",
    "\n",
    "print(\"\\nPronto! Resumo, gráficos e pares com lift gerados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fdba785-39d8-4d90-8bef-ae276a3e368e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "# 1) Ler tabela e criar transaction_id + lista de modelos\n",
    "df = spark.table(\"sc_gold.deals_2\").select(\"conta_name\",\"data_venda\",\"modelos\")\n",
    "\n",
    "df_tx = df.withColumn(\n",
    "    \"transaction_id\",\n",
    "    F.concat_ws(\"_\", \"conta_name\", F.date_format(\"data_venda\", \"yyyyMMdd\"))\n",
    ")\n",
    "\n",
    "df_basket = (\n",
    "    df_tx.groupBy(\"transaction_id\")\n",
    "         .agg(F.collect_set(\"modelos\").alias(\"items\"))\n",
    ")\n",
    "\n",
    "display(df_basket.limit(10))\n",
    "\n",
    "# 2) Treinar o modelo FP-Growth\n",
    "fpGrowth = FPGrowth(\n",
    "    itemsCol=\"items\",\n",
    "    minSupport=0.01,      # ajusta conforme o nº de transações\n",
    "    minConfidence=0.3\n",
    ")\n",
    "\n",
    "model = fpGrowth.fit(df_basket)\n",
    "\n",
    "# 3) Conjuntos frequentes\n",
    "freq_itemsets = model.freqItemsets\n",
    "display(freq_itemsets.orderBy(F.desc(\"freq\")).limit(20))\n",
    "\n",
    "# 4) Regras de associação\n",
    "rules = model.associationRules\n",
    "display(rules.orderBy(F.desc(\"confidence\")).limit(50))\n",
    "\n",
    "# 5) Recomendações por transação (opcional)\n",
    "predictions = model.transform(df_basket)\n",
    "display(predictions.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea270fe8-9818-42c4-9674-4801f4605fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(\"sc_gold.deals_2\").select(\"conta_name\",\"data_venda\",\"modelos\")\n",
    "\n",
    "df_tx = df.withColumn(\n",
    "    \"transaction_id\",\n",
    "    F.concat_ws(\"_\", \"conta_name\", F.date_format(\"data_venda\",\"yyyyMMdd\"))\n",
    ")\n",
    "\n",
    "df_flags = (\n",
    "    df_tx.groupBy(\"transaction_id\")\n",
    "         .pivot(\"modelos\")\n",
    "         .agg(F.lit(1))        # <-- garante que o valor é 1\n",
    "         .fillna(0)            # <-- e o resto é zero\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ce8fb6-7849-4a8a-8ab5-ebf680a5a5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = df_flags.toPandas().set_index(\"transaction_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7052fe1e-38c1-49e4-8572-631555961106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf.dtypes\n",
    "pdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d25de0f-b037-4115-854a-4319e7882b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# 1) Transações = todos os modelos por conta_name\n",
    "df = spark.table(\"sc_gold.deals_2\").select(\"conta_name\", \"modelos\")\n",
    "\n",
    "df_basket = (\n",
    "    df.groupBy(\"conta_name\")\n",
    "      .agg(F.collect_set(\"modelos\").alias(\"items\"))\n",
    ")\n",
    "\n",
    "# 2) Passar para pandas (lista de listas)\n",
    "pdf_basket = df_basket.toPandas()\n",
    "transactions = pdf_basket[\"items\"].tolist()\n",
    "index = pdf_basket[\"conta_name\"].tolist()\n",
    "\n",
    "# 3) One-hot encoding (matriz binária) com TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "pdf = pd.DataFrame(te_ary, columns=te.columns_, index=index)\n",
    "\n",
    "# 4) Apriori – conjuntos frequentes\n",
    "freq_itemsets = apriori(pdf, min_support=0.01, use_colnames=True)\n",
    "freq_itemsets[\"length\"] = freq_itemsets[\"itemsets\"].apply(len)\n",
    "\n",
    "print(\"Itemsets totais:\", freq_itemsets.shape)\n",
    "print(\"Itemsets com 2+ modelos:\")\n",
    "display(freq_itemsets.query(\"length >= 2\").head(10))\n",
    "\n",
    "# 5) Regras de associação\n",
    "rules = association_rules(\n",
    "    freq_itemsets,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=0.1   # ajusta depois se quiseres mais/menos regras\n",
    ")\n",
    "\n",
    "print(\"Nº de regras:\", rules.shape[0])\n",
    "display(rules.sort_values(\"lift\", ascending=False).head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c056e2-07d0-4bc7-b897-521b34bc5902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# 1) Transações = todos os modelos por conta_name\n",
    "df = spark.table(\"sc_gold.deals_2\").select(\"conta_name\", \"modelos\")\n",
    "\n",
    "df_basket = (\n",
    "    df.groupBy(\"conta_name\")\n",
    "      .agg(F.collect_set(\"modelos\").alias(\"items\"))\n",
    ")\n",
    "\n",
    "# Diagnóstico: quantos modelos por conta?\n",
    "(\n",
    "    df_basket\n",
    "    .selectExpr(\"size(items) as n_modelos\")\n",
    "    .groupBy()\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_contas\"),\n",
    "        F.avg(\"n_modelos\").alias(\"media_modelos_por_conta\"),\n",
    "        F.max(\"n_modelos\").alias(\"max_modelos_por_conta\")\n",
    "    )\n",
    "    .show()\n",
    ")\n",
    "\n",
    "# 2) Passar para pandas (lista de listas)\n",
    "pdf_basket = df_basket.toPandas()\n",
    "transactions = pdf_basket[\"items\"].tolist()\n",
    "index = pdf_basket[\"conta_name\"].tolist()\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "pdf = pd.DataFrame(te_ary, columns=te.columns_, index=index)\n",
    "\n",
    "# 3) Apriori com suporte mais baixo\n",
    "freq_itemsets = apriori(pdf, min_support=0.005, use_colnames=True)\n",
    "freq_itemsets[\"length\"] = freq_itemsets[\"itemsets\"].apply(len)\n",
    "\n",
    "print(\"Itemsets totais:\", freq_itemsets.shape)\n",
    "print(\"Distribuição de tamanhos:\")\n",
    "print(freq_itemsets[\"length\"].value_counts())\n",
    "\n",
    "# Ver se AGORA existem conjuntos com 2+ modelos\n",
    "freq_itemsets_2p = freq_itemsets.query(\"length >= 2\")\n",
    "print(\"Itemsets com 2+ modelos:\", freq_itemsets_2p.shape)\n",
    "display(freq_itemsets_2p.head(10))\n",
    "\n",
    "# 4) Se houver itemsets com 2+, gerar regras (senão, isto vai dar vazio)\n",
    "rules = association_rules(\n",
    "    freq_itemsets,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=0.05   # confiança mais baixa para começar a ver alguma coisa\n",
    ")\n",
    "\n",
    "print(\"Nº de regras:\", rules.shape[0])\n",
    "display(rules.sort_values(\"lift\", ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe43458-058b-4297-aa07-60ef4e255a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "P-Growth / Apriori / MBA\tarray<string> (lista de itens)\tÉ o formato nativo esperado pelos algoritmos de cesta\n",
    "Binary flags (One-Hot Encoding)\t1 coluna por item (0/1)\tSó é útil se fores usar modelos tipo regressão, clustering, ML clássico\n",
    "\n",
    "Ou seja:\n",
    "\n",
    "Para descobrir regras de associação → listas são o formato correto\n",
    "\n",
    "Para fazer modelos supervisionados / perfis → binary flags é o formato correto\n",
    "\n",
    "Por isso não transformei para wide format (0/1) — só atrapalha FP-Growth e explode o número de colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8ce1d2-3a2f-4197-9a05-de9bdf6ce231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Confirmar estrutura mínima e amostra\n",
    "df = spark.table(TABLE_SRC).select(\"conta_name\", \"data_venda\", \"modelos\")\n",
    "display(df.limit(20))  # equivalente a SELECT * ... LIMIT 20\n",
    "\n",
    "# (opcional) verificar se há transações com >=2 modelos no mesmo (conta_name, data_venda)\n",
    "check = (\n",
    "    df.where(F.col(\"modelos\").isNotNull())\n",
    "      .groupBy(\"conta_name\",\"data_venda\")\n",
    "      .agg(F.countDistinct(\"modelos\").alias(\"n_modelos\"))\n",
    "      .orderBy(F.desc(\"n_modelos\"))\n",
    ")\n",
    "display(check.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b45526-2891-4003-9e3c-2bb90b248ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#criar identificador da transaçao\n",
    "#Criar o identificador da transação (temp view: deals_tx)\n",
    "deals_tx = (\n",
    "    df.where(F.col(\"modelos\").isNotNull())\n",
    "      .withColumn(\"modelos\", F.trim(\"modelos\"))\n",
    "      .select(\n",
    "          \"conta_name\",\n",
    "          \"data_venda\",\n",
    "          \"modelos\",\n",
    "          F.concat_ws(\"_\", F.col(\"conta_name\"), F.date_format(F.col(\"data_venda\"), \"yyyyMMdd\")).alias(\"transaction_id\")\n",
    "      )\n",
    ")\n",
    "\n",
    "deals_tx.createOrReplaceTempView(\"deals_tx\")  # VIEW TEMPORÁRIA\n",
    "display(deals_tx.limit(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9512d8b7-e259-44a1-a480-3cd731d4afd0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"transaction_id\":219},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762467612397}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#lista de itens por transação\n",
    "#Itens por transação (temp view: tx_grouped)\n",
    "\n",
    "tx_grouped = (\n",
    "    deals_tx\n",
    "      .dropDuplicates([\"transaction_id\", \"modelos\"])               # 1 ocorrência por transação+modelo\n",
    "      .groupBy(\"transaction_id\")\n",
    "      .agg(F.collect_set(\"modelos\").alias(\"models_list\"))\n",
    "      .where(F.size(F.col(\"models_list\")) > 1)                     # HAVING size(...) > 1\n",
    ")\n",
    "\n",
    "tx_grouped.createOrReplaceTempView(\"tx_grouped\")\n",
    "print(\"Nº transações com 2+ modelos:\", tx_grouped.count())\n",
    "display(tx_grouped.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60673dd0-fbcf-48e2-b128-5f73d3be2bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Explodir pares (temp view: tx_pairs)\n",
    "a = tx_grouped.select(\"transaction_id\", F.posexplode(\"models_list\").alias(\"i\", \"model_a\"))\n",
    "b = tx_grouped.select(\"transaction_id\", F.posexplode(\"models_list\").alias(\"j\", \"model_b\"))\n",
    "\n",
    "tx_pairs = (\n",
    "    a.join(b, on=\"transaction_id\", how=\"inner\")\n",
    "     .where(F.col(\"i\") < F.col(\"j\"))                  # evita duplicados e reflexos\n",
    "     .select(\"model_a\", \"model_b\")\n",
    ")\n",
    "\n",
    "tx_pairs.createOrReplaceTempView(\"tx_pairs\")\n",
    "display(tx_pairs.limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40912f7b-ed0b-499e-93c7-bcebc5e8905d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Suportes (temp views: support_1 e support_2)\n",
    "\n",
    "# denominador = nº de transações (com 2+ modelos)\n",
    "N_tx = tx_grouped.count()\n",
    "print(\"N_tx (denominador do suporte) =\", N_tx)\n",
    "\n",
    "support_1 = (\n",
    "    tx_pairs\n",
    "      .groupBy(F.col(\"model_a\").alias(\"model\"))\n",
    "      .agg(F.count(\"*\").alias(\"cnt\"))\n",
    "      .withColumn(\"support\", F.col(\"cnt\") / F.lit(N_tx))\n",
    ")\n",
    "support_1.createOrReplaceTempView(\"support_1\")\n",
    "display(support_1.orderBy(F.desc(\"support\")).limit(20))\n",
    "\n",
    "support_2 = (\n",
    "    tx_pairs\n",
    "      .groupBy(\"model_a\",\"model_b\")\n",
    "      .agg(F.count(\"*\").alias(\"cnt\"))\n",
    "      .withColumn(\"support\", F.col(\"cnt\") / F.lit(N_tx))\n",
    ")\n",
    "support_2.createOrReplaceTempView(\"support_2\")\n",
    "display(support_2.orderBy(F.desc(\"support\")).limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89502e2-bc94-47f2-847e-f16b07f5be2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Regras A→B (confidence, lift) e gravação da tabela final\n",
    "\n",
    "rules_df = (\n",
    "    support_2.alias(\"s2\")\n",
    "      .join(support_1.alias(\"s1\"), F.col(\"s1.model\") == F.col(\"s2.model_a\"), \"inner\")\n",
    "      .join(support_1.alias(\"s3\"), F.col(\"s3.model\") == F.col(\"s2.model_b\"), \"inner\")\n",
    "      .select(\n",
    "          F.col(\"s2.model_a\").alias(\"antecedent\"),\n",
    "          F.col(\"s2.model_b\").alias(\"consequent\"),\n",
    "          F.col(\"s2.support\").alias(\"support_ab\"),\n",
    "          F.col(\"s1.support\").alias(\"support_a\"),\n",
    "          F.col(\"s3.support\").alias(\"support_b\"),\n",
    "          (F.col(\"s2.support\") / F.col(\"s1.support\")).alias(\"confidence\"),\n",
    "          ((F.col(\"s2.support\") / F.col(\"s1.support\")) / F.col(\"s3.support\")).alias(\"lift\")\n",
    "      )\n",
    "      .where(\n",
    "          (F.col(\"support_ab\") >= F.lit(MIN_SUPPORT)) &\n",
    "          (F.col(\"confidence\") >= F.lit(MIN_CONFIDENCE)) &\n",
    "          (F.col(\"lift\") >= F.lit(1.0))\n",
    "      )\n",
    "      .orderBy(F.desc(\"confidence\"), F.desc(\"lift\"), F.desc(\"support_ab\"))\n",
    ")\n",
    "\n",
    "# Tabela permanente com as regras\n",
    "rules_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.mba_modelos_rules\")\n",
    "\n",
    "display(spark.table(\"sc_gold.mba_modelos_rules\").limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be25e873-72b7-44fb-a858-8b6dd7e4cec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Ver rapidamente as top regras\n",
    "display(spark.table(\"sc_gold.mba_modelos_rules\").orderBy(F.desc(\"lift\")).limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141921b9-c86c-4e21-b06f-bbdb0f8d3b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "TOP_K = 20   # ajusta conforme precisares\n",
    "\n",
    "# Top-K modelos por frequência\n",
    "top_models = (\n",
    "    support_1.orderBy(F.desc(\"cnt\"))\n",
    "             .limit(TOP_K)\n",
    "             .select(\"model\")\n",
    "             .toPandas()[\"model\"].tolist()\n",
    ")\n",
    "\n",
    "# Pares apenas entre os Top-K\n",
    "pairs_top = (\n",
    "    support_2\n",
    "      .where(F.col(\"model_a\").isin(top_models) & F.col(\"model_b\").isin(top_models))\n",
    "      .select(\"model_a\",\"model_b\",\"cnt\")\n",
    ")\n",
    "\n",
    "# Passar para matriz (pandas) e desenhar heatmap com matplotlib\n",
    "pdf_pairs = pairs_top.toPandas()\n",
    "\n",
    "# Tornar a matriz simétrica (a,b) e (b,a)\n",
    "import pandas as pd\n",
    "symm = pd.concat([\n",
    "    pdf_pairs.rename(columns={\"model_a\":\"row\",\"model_b\":\"col\",\"cnt\":\"value\"}),\n",
    "    pdf_pairs.rename(columns={\"model_b\":\"row\",\"model_a\":\"col\",\"cnt\":\"value\"})\n",
    "], ignore_index=True)\n",
    "\n",
    "# Adicionar diagonal com contagens individuais\n",
    "pdf_1 = (support_1\n",
    "         .where(F.col(\"model\").isin(top_models))\n",
    "         .select(\"model\",\"cnt\").toPandas())\n",
    "diag = pd.DataFrame({\"row\": pdf_1[\"model\"], \"col\": pdf_1[\"model\"], \"value\": pdf_1[\"cnt\"]})\n",
    "\n",
    "mat = pd.concat([symm, diag], ignore_index=True)\n",
    "pivot = mat.pivot_table(index=\"row\", columns=\"col\", values=\"value\", fill_value=0)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(pivot.values)\n",
    "plt.xticks(range(len(pivot.columns)), pivot.columns, rotation=90)\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "plt.title(\"Co-ocorrência de modelos (Top-{})\".format(TOP_K))\n",
    "plt.colorbar(label=\"# pares na mesma transação\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfcd9d1-75e4-4218-a88e-410126970993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Regras B→A (reversas) e união\n",
    "#  thresholds (podes afinar)\n",
    "MIN_SUPPORT = 0.01\n",
    "MIN_CONFIDENCE = 0.30\n",
    "\n",
    "one_sup_x = support_1.select(F.col(\"model\").alias(\"x\"), F.col(\"support\").alias(\"sup_x\"))\n",
    "one_sup_y = support_1.select(F.col(\"model\").alias(\"y\"), F.col(\"support\").alias(\"sup_y\"))\n",
    "\n",
    "# A->B (se já não tens)\n",
    "rules_ab = (\n",
    "    support_2.alias(\"s2\")\n",
    "      .join(one_sup_x.alias(\"s1\"), F.col(\"s1.x\")==F.col(\"s2.model_a\"))\n",
    "      .join(one_sup_y.alias(\"s3\"), F.col(\"s3.y\")==F.col(\"s2.model_b\"))\n",
    "      .select(\n",
    "          F.col(\"s2.model_a\").alias(\"antecedent\"),\n",
    "          F.col(\"s2.model_b\").alias(\"consequent\"),\n",
    "          F.col(\"s2.support\").alias(\"support_ab\"),\n",
    "          F.col(\"s1.sup_x\").alias(\"support_a\"),\n",
    "          F.col(\"s3.sup_y\").alias(\"support_b\"),\n",
    "          (F.col(\"s2.support\")/F.col(\"s1.sup_x\")).alias(\"confidence\"),\n",
    "          ((F.col(\"s2.support\")/F.col(\"s1.sup_x\"))/F.col(\"s3.sup_y\")).alias(\"lift\")\n",
    "      )\n",
    "      .where( (F.col(\"support_ab\")>=MIN_SUPPORT) & (F.col(\"confidence\")>=MIN_CONFIDENCE) & (F.col(\"lift\")>=1.0) )\n",
    ")\n",
    "\n",
    "# B->A (reversas)\n",
    "rules_ba = (\n",
    "    support_2.alias(\"s2\")\n",
    "      .join(one_sup_x.alias(\"s1\"), F.col(\"s1.x\")==F.col(\"s2.model_b\"))  # agora sup(B)\n",
    "      .join(one_sup_y.alias(\"s3\"), F.col(\"s3.y\")==F.col(\"s2.model_a\"))  # e sup(A)\n",
    "      .select(\n",
    "          F.col(\"s2.model_b\").alias(\"antecedent\"),\n",
    "          F.col(\"s2.model_a\").alias(\"consequent\"),\n",
    "          F.col(\"s2.support\").alias(\"support_ab\"),\n",
    "          F.col(\"s1.sup_x\").alias(\"support_a\"),\n",
    "          F.col(\"s3.sup_y\").alias(\"support_b\"),\n",
    "          (F.col(\"s2.support\")/F.col(\"s1.sup_x\")).alias(\"confidence\"),\n",
    "          ((F.col(\"s2.support\")/F.col(\"s1.sup_x\"))/F.col(\"s3.sup_y\")).alias(\"lift\")\n",
    "      )\n",
    "      .where( (F.col(\"support_ab\")>=MIN_SUPPORT) & (F.col(\"confidence\")>=MIN_CONFIDENCE) & (F.col(\"lift\")>=1.0) )\n",
    ")\n",
    "\n",
    "rules_all = rules_ab.unionByName(rules_ba).orderBy(F.desc(\"confidence\"), F.desc(\"lift\"), F.desc(\"support_ab\"))\n",
    "display(rules_all.limit(100))\n",
    "\n",
    "# (opcional) gravar\n",
    "#rules_all.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.mba_modelos_rules_ba\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aee13f6-390f-4a46-b717-8f03a8425033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Regras 2→1 (A,B→C)\n",
    "#Aqui calculamos triplos (A,B,C) por transação com posexplode 3x e condição i<j<k, depois tiramos confidence e lift de (A,B)→C.\n",
    "\n",
    "# 3.1 gerar triplos distintos por transação\n",
    "a = tx_grouped.select(\"transaction_id\", F.posexplode(\"models_list\").alias(\"i\",\"A\"))\n",
    "b = tx_grouped.select(\"transaction_id\", F.posexplode(\"models_list\").alias(\"j\",\"B\"))\n",
    "c = tx_grouped.select(\"transaction_id\", F.posexplode(\"models_list\").alias(\"k\",\"C\"))\n",
    "\n",
    "triples = (\n",
    "    a.join(b, \"transaction_id\").join(c, \"transaction_id\")\n",
    "     .where((F.col(\"i\") < F.col(\"j\")) & (F.col(\"j\") < F.col(\"k\")))  # i<j<k\n",
    "     .select(\"A\",\"B\",\"C\")\n",
    ")\n",
    "\n",
    "# 3.2 suporte de triplos\n",
    "N_tx = tx_grouped.count()\n",
    "support_3 = (\n",
    "    triples.groupBy(\"A\",\"B\",\"C\")\n",
    "           .agg(F.count(\"*\").alias(\"cnt\"))\n",
    "           .withColumn(\"support_abc\", F.col(\"cnt\")/F.lit(N_tx))\n",
    ")\n",
    "\n",
    "# 3.3 precisamos do sup(A,B) e sup(C)\n",
    "pairs_sup = support_2.select(F.col(\"model_a\").alias(\"A\"), F.col(\"model_b\").alias(\"B\"), F.col(\"support\").alias(\"sup_ab\"))\n",
    "one_sup  = support_1.select(F.col(\"model\").alias(\"C\"), F.col(\"support\").alias(\"sup_c\"))\n",
    "\n",
    "# 3.4 regras (A,B) -> C\n",
    "MIN_SUPPORT_ABC = 0.005   # 0.5% (ajusta)\n",
    "MIN_CONF_2TO1   = 0.20    # 20%\n",
    "\n",
    "rules_2to1 = (\n",
    "    support_3\n",
    "      .join(pairs_sup, on=[\"A\",\"B\"], how=\"inner\")\n",
    "      .join(one_sup, on=[\"C\"], how=\"inner\")\n",
    "      .select(\n",
    "          F.array_sort(F.array(\"A\",\"B\")).alias(\"antecedent_AB\"),\n",
    "          F.col(\"C\").alias(\"consequent\"),\n",
    "          F.col(\"support_abc\"),\n",
    "          F.col(\"sup_ab\"),\n",
    "          F.col(\"sup_c\"),\n",
    "          (F.col(\"support_abc\")/F.col(\"sup_ab\")).alias(\"confidence\"),\n",
    "          ((F.col(\"support_abc\")/F.col(\"sup_ab\"))/F.col(\"sup_c\")).alias(\"lift\")\n",
    "      )\n",
    "      .where(\n",
    "          (F.col(\"support_abc\") >= F.lit(MIN_SUPPORT_ABC)) &\n",
    "          (F.col(\"confidence\")  >= F.lit(MIN_CONF_2TO1)) &\n",
    "          (F.col(\"lift\")        >= F.lit(1.0))\n",
    "      )\n",
    "      .orderBy(F.desc(\"confidence\"), F.desc(\"lift\"), F.desc(\"support_abc\"))\n",
    ")\n",
    "\n",
    "display(rules_2to1.limit(100))\n",
    "\n",
    "# (opcional) gravar\n",
    "rules_2to1.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.mba_modelos_rules_2to1\")\n",
    "\n",
    "\n",
    "#grafico\n",
    "mba_modelos_rules_2to1 = spark.table(\"sc_gold.mba_modelos_rules_2to1\")\n",
    "pdf2 = mba_modelos_rules_2to1.toPandas()\n",
    "if not pdf2.empty:\n",
    "    pdf2[\"AB\"] = pdf2[\"antecedent_AB\"].apply(lambda xs: \" + \".join(xs))\n",
    "    top_ab = (pdf2.groupby(\"AB\")[\"lift\"].max().sort_values(ascending=False).head(15)).index.tolist()\n",
    "    top_c  = (pdf2.groupby(\"consequent\")[\"lift\"].max().sort_values(ascending=False).head(15)).index.tolist()\n",
    "    mat2 = pdf2[pdf2[\"AB\"].isin(top_ab) & pdf2[\"consequent\"].isin(top_c)] \\\n",
    "              .pivot_table(index=\"AB\", columns=\"consequent\", values=\"lift\", fill_value=0)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.imshow(mat2.values)\n",
    "    plt.xticks(range(len(mat2.columns)), mat2.columns, rotation=90)\n",
    "    plt.yticks(range(len(mat2.index)), mat2.index)\n",
    "    plt.title(\"Lift — Regras 2→1 (A,B→C)\")\n",
    "    plt.colorbar(label=\"Lift\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9d9602-605b-42df-b020-61667e11d4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ====== Configurações ======\n",
    "TABLE_RULES = \"sc_gold.mba_modelos_rules\"\n",
    "TOP_N = 15  # quantas regras mostrar\n",
    "# Opcional: foca em certos modelos (case-insensitive). Deixa lista vazia [] se quiseres todas.\n",
    "model_keywords = [\"Ioniq\", \"Tucson\", \"Kauai\", \"I20\", \"I30\", \"Bayon\", \"Santa Fe\"]\n",
    "\n",
    "# ====== Ler regras e (opcional) filtrar por modelos de interesse ======\n",
    "rules = spark.table(TABLE_RULES).select(\n",
    "    \"antecedent\",\"consequent\",\"support_ab\",\"support_a\",\"support_b\",\"confidence\",\"lift\"\n",
    ")\n",
    "\n",
    "if model_keywords:\n",
    "    # cria padrão regex OR, case-insensitive\n",
    "    pattern = \"|\".join([f\"(?i){kw}\" for kw in model_keywords])\n",
    "    rules = rules.where(\n",
    "        F.col(\"antecedent\").rlike(pattern) | F.col(\"consequent\").rlike(pattern)\n",
    "    )\n",
    "\n",
    "# Top-N por lift (desempata por confidence e support)\n",
    "top_rules = (rules\n",
    "             .orderBy(F.desc(\"lift\"), F.desc(\"confidence\"), F.desc(\"support_ab\"))\n",
    "             .limit(TOP_N))\n",
    "\n",
    "# ====== Converter para pandas e criar label para o gráfico ======\n",
    "pdf = top_rules.toPandas().fillna(0)\n",
    "if pdf.empty:\n",
    "    raise ValueError(\"Sem regras para plotar com os filtros atuais. Ajusta TOP_N ou 'model_keywords'.\")\n",
    "\n",
    "pdf[\"rule_label\"] = pdf[\"antecedent\"] + \"  →  \" + pdf[\"consequent\"]\n",
    "\n",
    "# ====== Plot horizontal por LIFT ======\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.barh(pdf[\"rule_label\"], pdf[\"lift\"])\n",
    "plt.xlabel(\"Lift (força da associação)\")\n",
    "plt.title(f\"Top {len(pdf)} Regras Modelo → Modelo (ordenado por Lift)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9dff2d-3160-4d3e-9737-f57ea9aa5863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(pdf[\"support_ab\"], pdf[\"confidence\"], s=40 + 120*(pdf[\"lift\"]-pdf[\"lift\"].min())/(pdf[\"lift\"].max()-pdf[\"lift\"].min()+1e-9))\n",
    "for _, r in pdf.iterrows():\n",
    "    plt.annotate(f\"{r['antecedent']}→{r['consequent']}\", (r[\"support_ab\"], r[\"confidence\"]), fontsize=8, xytext=(3,2), textcoords=\"offset points\")\n",
    "plt.xlabel(\"Support AB\")\n",
    "plt.ylabel(\"Confidence\")\n",
    "plt.title(\"Regras Modelo→Modelo (tamanho ~ lift)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004c4684-7c0f-42f2-8c05-a0f70402c8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from itertools import combinations\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Parâmetros\n",
    "# =========================\n",
    "SRC_GROUPED = \"sc_gold.deals_tx_grouped\"   # transaction_id, models_list\n",
    "SRC_RAW     = \"sc_gold.deals_2\"            # se a grouped não existir\n",
    "MIN_SUPPORT = 0.005                        # 0.5%\n",
    "MIN_CONF    = 0.20                         # 20%\n",
    "MAX_K       = 3                            # itemsets até tamanho 3 no fallback\n",
    "\n",
    "# =========================\n",
    "# Obter/Construir tx_grouped\n",
    "# =========================\n",
    "if spark.catalog.tableExists(SRC_GROUPED):\n",
    "    tx_grouped = spark.table(SRC_GROUPED)\n",
    "else:\n",
    "    df = (spark.table(SRC_RAW)\n",
    "          .select(\"conta_name\",\"data_venda\",\"modelos\")\n",
    "          .where(F.col(\"modelos\").isNotNull()))\n",
    "    deals_tx = (\n",
    "        df.withColumn(\"modelos\", F.trim(\"modelos\"))\n",
    "          .select(\n",
    "              \"conta_name\",\"data_venda\",\"modelos\",\n",
    "              F.concat_ws(\"_\", F.col(\"conta_name\"), F.date_format(F.col(\"data_venda\"), \"yyyyMMdd\")).alias(\"transaction_id\")\n",
    "          )\n",
    "          .dropDuplicates([\"transaction_id\",\"modelos\"])\n",
    "    )\n",
    "    tx_grouped = (\n",
    "        deals_tx.groupBy(\"transaction_id\")\n",
    "                .agg(F.collect_set(\"modelos\").alias(\"models_list\"))\n",
    "                .where(F.size(\"models_list\") > 1)\n",
    "    )\n",
    "\n",
    "N = tx_grouped.count()\n",
    "print(\"N transações (2+ itens):\", N)\n",
    "\n",
    "# =========================\n",
    "# 1) Tentar FP-Growth nativo (se for suportado)\n",
    "# =========================\n",
    "fp_itemsets = None\n",
    "fp_rules_pretty = None\n",
    "\n",
    "try:\n",
    "    from pyspark.ml.fpm import FPGrowth\n",
    "    fpg = FPGrowth(itemsCol=\"models_list\", minSupport=MIN_SUPPORT, minConfidence=MIN_CONF)\n",
    "    model = fpg.fit(tx_grouped)\n",
    "\n",
    "    freq_itemsets = model.freqItemsets  # items (array<string>), freq\n",
    "    fp_itemsets = freq_itemsets.withColumn(\"support\", F.col(\"freq\")/F.lit(N))\n",
    "\n",
    "    rules = model.associationRules       # antecedent (array), consequent (array), confidence, lift, [support?]\n",
    "    if \"support\" in rules.columns:\n",
    "        rules = rules.withColumnRenamed(\"support\",\"support_ab\")\n",
    "    else:\n",
    "        sup_ab = (fp_itemsets\n",
    "                  .select(F.array_sort(\"items\").alias(\"ab\"), F.col(\"support\").alias(\"support_ab\")))\n",
    "        rules = (rules\n",
    "                 .withColumn(\"ab\", F.array_sort(F.array_union(F.col(\"antecedent\"), F.col(\"consequent\"))))\n",
    "                 .join(sup_ab, \"ab\", \"left\")\n",
    "                 .drop(\"ab\"))\n",
    "\n",
    "    join_str = F.udf(lambda xs: \", \".join(xs) if xs else \"\", T.StringType())\n",
    "    fp_rules_pretty = (rules\n",
    "                       .withColumn(\"antecedent_str\", join_str(\"antecedent\"))\n",
    "                       .withColumn(\"consequent_str\", join_str(\"consequent\"))\n",
    "                       .select(\"antecedent_str\",\"consequent_str\",\"support_ab\",\"confidence\",\"lift\"))\n",
    "    print(\"✅ FP-Growth MLlib concluído.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"⚠️ FP-Growth MLlib indisponível — a usar fallback serverless-safe.\")\n",
    "    # =========================\n",
    "    # 2) FALLBACK: frequent itemsets & rules via DataFrame + mapInPandas\n",
    "    # =========================\n",
    "\n",
    "    # --- 2.1 Suporte de 1-item ---\n",
    "    one_items = (\n",
    "        tx_grouped\n",
    "          .select(F.explode(\"models_list\").alias(\"item\"))\n",
    "          .groupBy(\"item\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "          .withColumn(\"support\", F.col(\"cnt\") / F.lit(N))\n",
    "    )\n",
    "    # materializar em Delta para evitar recomputes em Serverless\n",
    "    one_items.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.tmp_one_items\")\n",
    "    one_items = spark.table(\"sc_gold.tmp_one_items\")\n",
    "\n",
    "    # --- 2.2 Suporte de k-itens (k>=2) via mapInPandas ---\n",
    "    def combos_gen(k: int):\n",
    "        schema = T.StructType([T.StructField(f\"i{t+1}\", T.StringType(), True) for t in range(k)])\n",
    "        def _fn(iterator):\n",
    "            for pdf in iterator:\n",
    "                rows = []\n",
    "                for items in pdf[\"models_list\"]:\n",
    "                    if not isinstance(items, (list, tuple)):\n",
    "                        continue\n",
    "                    L = sorted(set(str(x) for x in items if x is not None))\n",
    "                    for combo in combinations(L, k):\n",
    "                        rows.append(combo)\n",
    "                if rows:\n",
    "                    yield pd.DataFrame(rows, columns=[f\"i{t+1}\" for t in range(k)])\n",
    "                else:\n",
    "                    yield pd.DataFrame(columns=[f\"i{t+1}\" for t in range(k)])\n",
    "        return _fn, schema\n",
    "\n",
    "    supports = []\n",
    "\n",
    "    # k=2\n",
    "    fn2, sch2 = combos_gen(2)\n",
    "    sup2 = (tx_grouped.select(\"models_list\")\n",
    "            .mapInPandas(fn2, schema=sch2)\n",
    "            .groupBy(\"i1\",\"i2\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "            .withColumn(\"support\", F.col(\"cnt\")/F.lit(N))\n",
    "            .withColumn(\"items\", F.array(\"i1\",\"i2\"))\n",
    "            .select(\"items\",\"cnt\",\"support\"))\n",
    "    supports.append(sup2)\n",
    "\n",
    "    # k=3 (se MAX_K >= 3)\n",
    "    if MAX_K >= 3:\n",
    "        fn3, sch3 = combos_gen(3)\n",
    "        sup3 = (tx_grouped.select(\"models_list\")\n",
    "                .mapInPandas(fn3, schema=sch3)\n",
    "                .groupBy(\"i1\",\"i2\",\"i3\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "                .withColumn(\"support\", F.col(\"cnt\")/F.lit(N))\n",
    "                .withColumn(\"items\", F.array(\"i1\",\"i2\",\"i3\"))\n",
    "                .select(\"items\",\"cnt\",\"support\"))\n",
    "        supports.append(sup3)\n",
    "\n",
    "    # itemsets frequentes unificados (1,2[,3]-itens)\n",
    "    fp_itemsets = (\n",
    "        one_items.select(F.array(F.col(\"item\")).alias(\"items\"), \"cnt\", \"support\")\n",
    "        .unionByName(reduce(lambda a,b: a.unionByName(b), supports))\n",
    "    )\n",
    "\n",
    "    # --- 2.3 Regras 1→1 (A→B) ---\n",
    "    pairs = sup2.select(F.col(\"items\")[0].alias(\"a\"),\n",
    "                        F.col(\"items\")[1].alias(\"b\"),\n",
    "                        F.col(\"support\").alias(\"support_ab\"))\n",
    "\n",
    "    s1  = one_items.select(F.col(\"item\").alias(\"x\"), F.col(\"support\").alias(\"sup_x\"))\n",
    "    s1b = one_items.select(F.col(\"item\").alias(\"y\"), F.col(\"support\").alias(\"sup_y\"))\n",
    "\n",
    "    rules_ab = (\n",
    "        pairs.join(s1, pairs.a == s1.x)\n",
    "             .join(s1b, pairs.b == s1b.y)\n",
    "             .select(\n",
    "                 F.array(\"a\").alias(\"antecedent\"),\n",
    "                 F.array(\"b\").alias(\"consequent\"),\n",
    "                 \"support_ab\",\n",
    "                 (F.col(\"support_ab\")/F.col(\"sup_x\")).alias(\"confidence\"),\n",
    "                 ((F.col(\"support_ab\")/F.col(\"sup_x\"))/F.col(\"sup_y\")).alias(\"lift\")\n",
    "             )\n",
    "             .where((F.col(\"support_ab\")>=F.lit(MIN_SUPPORT)) & (F.col(\"confidence\")>=F.lit(MIN_CONF)))\n",
    "    )\n",
    "\n",
    "    # --- 2.4 Regras 2→1 (A,B→C) se MAX_K>=3 ---\n",
    "    if MAX_K >= 3:\n",
    "        sup_ab = pairs.select(F.array_sort(F.array(\"a\",\"b\")).alias(\"ab\"),\n",
    "                              F.col(\"support_ab\"))\n",
    "        sup_c = one_items.select(F.col(\"item\").alias(\"C\"),\n",
    "                                 F.col(\"support\").alias(\"sup_c\"))\n",
    "        abc_split = (\n",
    "            sup3.withColumn(\"abc_sorted\", F.array_sort(\"items\"))\n",
    "                .withColumn(\"ab\", F.expr(\"slice(abc_sorted, 1, 2)\"))  # primeiros 2 como AB\n",
    "                .withColumn(\"C\",  F.col(\"abc_sorted\")[2])              # terceiro como C (0-based -> [2])\n",
    "                .select(\"ab\",\"C\", F.col(\"support\").alias(\"support_abc\"))\n",
    "        )\n",
    "        rules_2to1 = (\n",
    "            abc_split.join(sup_ab, \"ab\").join(sup_c, \"C\")\n",
    "                     .select(\n",
    "                         F.col(\"ab\").alias(\"antecedent\"),\n",
    "                         F.array(\"C\").alias(\"consequent\"),\n",
    "                         \"support_abc\",\n",
    "                         (F.col(\"support_abc\")/F.col(\"support_ab\")).alias(\"confidence\"),\n",
    "                         ((F.col(\"support_abc\")/F.col(\"support_ab\"))/F.col(\"sup_c\")).alias(\"lift\")\n",
    "                     )\n",
    "                     .where((F.col(\"support_abc\")>=F.lit(MIN_SUPPORT)) & (F.col(\"confidence\")>=F.lit(MIN_CONF)))\n",
    "        )\n",
    "    else:\n",
    "        rules_2to1 = spark.createDataFrame([], schema=\"antecedent array<string>, consequent array<string>, support_abc double, confidence double, lift double\")\n",
    "\n",
    "    # --- 2.5 Consolidação (formato semelhante ao de FP-Growth) ---\n",
    "    fp_rules = rules_ab.select(\n",
    "        \"antecedent\", \"consequent\",\n",
    "        F.col(\"support_ab\").alias(\"support_ab\"),\n",
    "        \"confidence\", \"lift\"\n",
    "    ).unionByName(\n",
    "        rules_2to1.select(\n",
    "            \"antecedent\", \"consequent\",\n",
    "            F.col(\"support_abc\").alias(\"support_ab\"),\n",
    "            \"confidence\", \"lift\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    jstr = F.udf(lambda xs: \", \".join(xs) if xs else \"\", T.StringType())\n",
    "    fp_rules_pretty = (fp_rules\n",
    "                       .withColumn(\"antecedent_str\", jstr(\"antecedent\"))\n",
    "                       .withColumn(\"consequent_str\", jstr(\"consequent\"))\n",
    "                       .select(\"antecedent_str\",\"consequent_str\",\"support_ab\",\"confidence\",\"lift\"))\n",
    "\n",
    "# =========================\n",
    "# Guardar resultados\n",
    "# =========================\n",
    "fp_itemsets.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.mba_fp_itemsets\")\n",
    "fp_rules_pretty.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.mba_fp_rules\")\n",
    "\n",
    "print(\"Gravado:\")\n",
    "print(\"- sc_gold.mba_fp_itemsets\")\n",
    "print(\"- sc_gold.mba_fp_rules\")\n",
    "\n",
    "display(spark.table(\"sc_gold.mba_fp_rules\").orderBy(F.desc(\"lift\"), F.desc(\"confidence\")).limit(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a44cac-7f9e-4cfb-82e3-7bf8c58f4933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Regras A→B (confidence, lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c45ca2-a5e7-4b00-9dce-9ba5ccd221bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "RULES_1TO1 = \"sc_gold.mba_modelos_rules\"       # A→B já tens\n",
    "TX_GROUPED = \"sc_gold.deals_tx_grouped\"        # transaction_id, models_list (se não existir, crio no passo 4)\n",
    "TOP_N = 20                                     # top itens para gráficos\n",
    "MIN_SUPPORT_ABC = 0.005                        # 0.5% p/ 2→1\n",
    "MIN_CONF_2TO1 = 0.20                           # 20% p/ 2→1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1744edd7-630f-467f-a297-5a01c61e5f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HEATMAP de co-ocorrência (lift) para regras A→B\n",
    "# Ler regras 1→1\n",
    "rules = (spark.table(RULES_1TO1)\n",
    "              .select(\"antecedent\",\"consequent\",\"support_ab\",\"confidence\",\"lift\"))\n",
    "\n",
    "# Top modelos por frequência: usa support_a/ support_b se tiveres, senão top por lift/conf.\n",
    "# Se a tua tabela já tiver support_a/support_b, substitui select acima e calcula top por esses.\n",
    "top_models = (\n",
    "    rules.select(F.col(\"antecedent\").alias(\"m\")).union(rules.select(F.col(\"consequent\").alias(\"m\")))\n",
    "         .groupBy(\"m\").count().orderBy(F.desc(\"count\")).limit(TOP_N)\n",
    "         .select(\"m\").toPandas()[\"m\"].tolist()\n",
    ")\n",
    "\n",
    "# Filtrar pares só entre top_models\n",
    "rules_top = rules.where(F.col(\"antecedent\").isin(top_models) & F.col(\"consequent\").isin(top_models)) \\\n",
    "                 .select(\"antecedent\",\"consequent\",\"lift\")\n",
    "\n",
    "# Pivot para matriz\n",
    "import pandas as pd, numpy as np\n",
    "pdf = rules_top.toPandas()\n",
    "if pdf.empty:\n",
    "    raise ValueError(\"Sem regras suficientes para o heatmap. Ajusta TOP_N.\")\n",
    "mat = pdf.pivot_table(index=\"antecedent\", columns=\"consequent\", values=\"lift\", fill_value=0)\n",
    "\n",
    "# Plot heatmap (matplotlib only)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(mat.values)\n",
    "plt.xticks(range(len(mat.columns)), mat.columns, rotation=90)\n",
    "plt.yticks(range(len(mat.index)), mat.index)\n",
    "plt.title(f\"Lift (A→B) — Top {len(top_models)} modelos\")\n",
    "plt.colorbar(label=\"Lift\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43984329-addd-41a4-991a-da40a2db4cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#REDE (graph) simples de co-compra sem networkx (linhas ponderadas por lift)\n",
    "# Pegar top E arestas por lift entre top modelos\n",
    "E = 60  # nº máximo de arestas a desenhar\n",
    "edges = (rules_top.orderBy(F.desc(\"lift\")).limit(E)).toPandas()\n",
    "\n",
    "# Colocar nós num círculo\n",
    "nodes = sorted(set(edges[\"antecedent\"]).union(set(edges[\"consequent\"])))\n",
    "import math\n",
    "coords = {n: (math.cos(2*math.pi*i/len(nodes)), math.sin(2*math.pi*i/len(nodes))) for i,n in enumerate(nodes)}\n",
    "\n",
    "# Desenhar\n",
    "plt.figure(figsize=(10,10))\n",
    "# desenha arestas\n",
    "for _, r in edges.iterrows():\n",
    "    x1,y1 = coords[r[\"antecedent\"]]; x2,y2 = coords[r[\"consequent\"]]\n",
    "    lw = max(0.5, min(6.0, r[\"lift\"]))  # espessura ~ lift (cap)\n",
    "    plt.plot([x1,x2],[y1,y2], linewidth=lw, alpha=0.6)\n",
    "\n",
    "# desenha nós e labels\n",
    "for n,(x,y) in coords.items():\n",
    "    plt.scatter([x],[y], s=150)\n",
    "    plt.text(x, y, n, ha=\"center\", va=\"center\", fontsize=8)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Rede de co-ocorrência (espessura ~ lift)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc6e43a-37ca-47ad-b9fd-0f830c6797b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#“RECOMENDAR” com base nas regras A→B (e também 2→1 se disponível)\n",
    "from typing import List\n",
    "\n",
    "rules_df = spark.table(RULES_1TO1).select(\"antecedent\",\"consequent\",\"support_ab\",\"confidence\",\"lift\")\n",
    "rules_pdf = rules_df.toPandas()\n",
    "\n",
    "def recommend_from_items(items: List[str], top_k=10, prefer=\"lift\"):\n",
    "    items_norm = set(items)\n",
    "    # A→B: se antecedent em items\n",
    "    cand = rules_pdf[rules_pdf[\"antecedent\"].isin(items_norm)].copy()\n",
    "    # Agregar por consequent (pode haver múltiplos A→mesmo B)\n",
    "    agg = cand.groupby(\"consequent\")[[\"support_ab\",\"confidence\",\"lift\"]].agg(\"max\").reset_index()\n",
    "    return agg.sort_values(prefer, ascending=False).head(top_k)\n",
    "\n",
    "# Exemplo:\n",
    "display(recommend_from_items([\"Ioniq 5\"]).head(10))\n",
    "# Se tiveres a tabela 2→1, podes escrever uma função para dois antecedentes:\n",
    "# recommend_pair([\"Ioniq 5\",\"Ioniq 6\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf2ff4b-eec8-4216-971f-b258c55c9d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Regras 2→1 (A,B→C) e heatmap 2→1 (gera e grava)\n",
    "# 4.1 garantir tx_grouped; se não existir, construir a partir de deals_tx\n",
    "if spark.catalog.tableExists(TX_GROUPED):\n",
    "    tx_grouped = spark.table(TX_GROUPED)\n",
    "else:\n",
    "    # fallback: construir de sc_gold.deals_2\n",
    "    base = (spark.table(\"sc_gold.deals_2\")\n",
    "            .select(\"conta_name\",\"data_venda\",\"modelos\")\n",
    "            .where(F.col(\"modelos\").isNotNull()))\n",
    "    deals_tx = (base.select(\n",
    "                    \"conta_name\",\"data_venda\",\"modelos\",\n",
    "                    F.concat_ws(\"_\", \"conta_name\", F.date_format(\"data_venda\",\"yyyyMMdd\")).alias(\"transaction_id\")\n",
    "                ).dropDuplicates([\"transaction_id\",\"modelos\"]))\n",
    "    tx_grouped = (deals_tx.groupBy(\"transaction_id\")\n",
    "                        .agg(F.collect_set(\"modelos\").alias(\"models_list\"))\n",
    "                        .where(F.size(\"models_list\") > 1))\n",
    "\n",
    "N = tx_grouped.count()\n",
    "\n",
    "# 4.2 suportes 1 e pares\n",
    "one_items = (tx_grouped.select(F.explode(\"models_list\").alias(\"item\"))\n",
    "                        .groupBy(\"item\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "                        .withColumn(\"support\", F.col(\"cnt\")/F.lit(N)))\n",
    "\n",
    "a = tx_grouped.select(\"transaction_id\", F.posexplode(\"models_list\").alias(\"i\",\"A\"))\n",
    "b = tx_grouped.select(\"transaction_id\", F.posexplode(\"models_list\").alias(\"j\",\"B\"))\n",
    "pairs = (a.join(b, \"transaction_id\").where(F.col(\"i\")<F.col(\"j\"))\n",
    "           .groupBy(\"A\",\"B\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "           .withColumn(\"support_ab\", F.col(\"cnt\")/F.lit(N))\n",
    "           .withColumn(\"ab\", F.array_sort(F.array(\"A\",\"B\"))))\n",
    "\n",
    "# 4.3 triplos (A,B,C)\n",
    "c = tx_grouped.select(\"transaction_id\", F.posexplode(\"models_list\").alias(\"k\",\"C\"))\n",
    "triples = (a.join(b,\"transaction_id\").join(c,\"transaction_id\")\n",
    "             .where((F.col(\"i\")<F.col(\"j\")) & (F.col(\"j\")<F.col(\"k\")))\n",
    "             .select(F.array_sort(F.array(\"A\",\"B\")).alias(\"ab\"), F.col(\"C\")))\n",
    "\n",
    "support_abc = (triples.groupBy(\"ab\",\"C\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "                      .withColumn(\"support_abc\", F.col(\"cnt\")/F.lit(N)))\n",
    "\n",
    "# 4.4 juntar e calcular confidence/lift de (A,B)→C\n",
    "sup_c = one_items.select(F.col(\"item\").alias(\"C\"), F.col(\"support\").alias(\"sup_c\"))\n",
    "rules_2to1 = (support_abc.join(pairs.select(\"ab\",\"support_ab\"), \"ab\", \"inner\")\n",
    "                        .join(sup_c, \"C\", \"inner\")\n",
    "                        .select(\n",
    "                            \"ab\", \"C\",\n",
    "                            \"support_abc\",\"support_ab\",\"sup_c\",\n",
    "                            (F.col(\"support_abc\")/F.col(\"support_ab\")).alias(\"confidence\"),\n",
    "                            ((F.col(\"support_abc\")/F.col(\"support_ab\"))/F.col(\"sup_c\")).alias(\"lift\")\n",
    "                        )\n",
    "                        .where((F.col(\"support_abc\")>=MIN_SUPPORT_ABC) & (F.col(\"confidence\")>=MIN_CONF_2TO1))\n",
    "                        .orderBy(F.desc(\"confidence\"), F.desc(\"lift\"), F.desc(\"support_abc\"))\n",
    ")\n",
    "\n",
    "# Gravar\n",
    "rules_2to1_out = (rules_2to1\n",
    "                  .select(F.col(\"ab\").alias(\"antecedent_AB\"),\n",
    "                          F.col(\"C\").alias(\"consequent\"),\n",
    "                          \"support_abc\",\"confidence\",\"lift\"))\n",
    "rules_2to1_out.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.mba_modelos_rules_2to1\")\n",
    "\n",
    "display(rules_2to1_out.limit(50))\n",
    "\n",
    "# 4.5 Heatmap 2→1 (lift)\n",
    "pdf2 = rules_2to1_out.toPandas()\n",
    "if not pdf2.empty:\n",
    "    pdf2[\"AB\"] = pdf2[\"antecedent_AB\"].apply(lambda xs: \" + \".join(xs))\n",
    "    top_ab = (pdf2.groupby(\"AB\")[\"lift\"].max().sort_values(ascending=False).head(15)).index.tolist()\n",
    "    top_c  = (pdf2.groupby(\"consequent\")[\"lift\"].max().sort_values(ascending=False).head(15)).index.tolist()\n",
    "    mat2 = pdf2[pdf2[\"AB\"].isin(top_ab) & pdf2[\"consequent\"].isin(top_c)] \\\n",
    "              .pivot_table(index=\"AB\", columns=\"consequent\", values=\"lift\", fill_value=0)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.imshow(mat2.values)\n",
    "    plt.xticks(range(len(mat2.columns)), mat2.columns, rotation=90)\n",
    "    plt.yticks(range(len(mat2.index)), mat2.index)\n",
    "    plt.title(\"Lift — Regras 2→1 (A,B→C)\")\n",
    "    plt.colorbar(label=\"Lift\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a75c3b-dcbd-4244-ae6f-bf7a7d18464e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#“Story” — interpretação automática rápida dos teus resultados\n",
    "rules = spark.table(RULES_1TO1).select(\"antecedent\",\"consequent\",\"support_ab\",\"confidence\",\"lift\")\n",
    "top = (rules.orderBy(F.desc(\"lift\"), F.desc(\"confidence\"), F.desc(\"support_ab\")).limit(10)).toPandas()\n",
    "\n",
    "print(\"TOP 10 REGRAS (ordenado por lift):\")\n",
    "for _,r in top.iterrows():\n",
    "    print(f\"- {r['antecedent']} → {r['consequent']} | lift={r['lift']:.2f}, conf={float(r['confidence']):.2f}, supAB={float(r['support_ab']):.3%}\")\n",
    "\n",
    "# alguns insights programáticos\n",
    "strong = top[top[\"lift\"]>=3]\n",
    "if not strong.empty:\n",
    "    print(\"\\nRegras MUITO fortes (lift ≥ 3):\")\n",
    "    for _,r in strong.iterrows():\n",
    "        print(f\"  * {r['antecedent']} → {r['consequent']} (lift {r['lift']:.2f})\")\n",
    "\n",
    "ev_pairs = rules.where(F.lower(\"antecedent\").like(\"%ioniq%\") | F.lower(\"consequent\").like(\"%ioniq%\") |\n",
    "                       F.lower(\"antecedent\").like(\"%hev%\")   | F.lower(\"consequent\").like(\"%hev%\")).count()\n",
    "print(f\"\\nRegras envolvendo EV/HEV detetadas: {ev_pairs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27a65031-0146-42f9-87ff-6b612876a34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "B->A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33af90d3-2b88-492d-8075-98278a894d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Parâmetros\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "RULES_1TO1 = \"sc_gold.mba_modelos_rules\"     # tem: antecedent, consequent, support_ab, support_a, support_b, confidence, lift\n",
    "DEALS      = \"sc_gold.deals_2\"               # para recomendações por cliente\n",
    "TOP_K_PER_TRIGGER = 5                         # nº de recomendações por “modelo comprado”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49283d74-3507-40fe-b0f3-c5e6c3e211f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Construir regras B→A (invertidas) e gravar\n",
    "rules_ab = spark.table(RULES_1TO1).select(\n",
    "    \"antecedent\",\"consequent\",\"support_ab\",\"support_a\",\"support_b\",\"confidence\",\"lift\"\n",
    ")\n",
    "\n",
    "rules_ba = (\n",
    "    rules_ab\n",
    "      .select(\n",
    "          F.col(\"consequent\").alias(\"trigger_b\"),     # “o que o cliente comprou”\n",
    "          F.col(\"antecedent\").alias(\"recommend_a\"),   # “o que recomendo”\n",
    "          F.col(\"support_ab\"),\n",
    "          F.col(\"support_b\"),\n",
    "          F.col(\"support_a\"),\n",
    "          (F.col(\"support_ab\")/F.col(\"support_b\")).alias(\"confidence_ba\"),\n",
    "          F.col(\"lift\").alias(\"lift_ba\")\n",
    "      )\n",
    "      .where( (F.col(\"support_b\") > 0) & (F.col(\"support_ab\") > 0) )\n",
    "      .orderBy(F.desc(\"lift_ba\"), F.desc(\"confidence_ba\"), F.desc(\"support_ab\"))\n",
    ")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS sc_gold.mba_modelos_rules_BA\")\n",
    "\n",
    "(rules_ba\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"sc_gold.mba_modelos_rules_BA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96532bb3-77d3-4094-8e88-f71627674cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Ranking por trigger (Top-K recomendações por “modelo comprado”)\n",
    "\n",
    "# rank por trigger, priorizando lift e depois confidence\n",
    "w = W.partitionBy(\"trigger_b\").orderBy(F.desc(\"lift_ba\"), F.desc(\"confidence_ba\"), F.desc(\"support_ab\"))\n",
    "\n",
    "reco_by_trigger = (\n",
    "    rules_ba\n",
    "      .withColumn(\"rank\", F.row_number().over(w))\n",
    "      .where(F.col(\"rank\") <= TOP_K_PER_TRIGGER)\n",
    ")\n",
    "\n",
    "reco_by_trigger.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.reco_modelo_by_trigger\")\n",
    "display(spark.table(\"sc_gold.reco_modelo_by_trigger\").orderBy(\"trigger_b\",\"rank\").limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c94f37-c788-4013-85f8-35d11ef6a720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Next Best Model por cliente (com base na última compra)\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "df = spark.table(DEALS).select(\"conta_name\",\"data_venda\",\"modelos\").where(\n",
    "    F.col(\"conta_name\").isNotNull() & F.col(\"data_venda\").isNotNull() & F.col(\"modelos\").isNotNull()\n",
    ")\n",
    "\n",
    "w_last = W.partitionBy(\"conta_name\").orderBy(F.desc(\"data_venda\"))\n",
    "last_model = (\n",
    "    df.withColumn(\"rn\", F.row_number().over(w_last))\n",
    "      .where(F.col(\"rn\")==1)\n",
    "      .select(F.col(\"conta_name\"), F.col(\"modelos\").alias(\"ultimo_modelo\"))\n",
    ")\n",
    "\n",
    "reco = spark.table(\"sc_gold.reco_modelo_by_trigger\")\n",
    "\n",
    "reco_cliente = (\n",
    "    last_model.join(reco, last_model.ultimo_modelo == reco.trigger_b, \"left\")\n",
    "              .select(\n",
    "                  \"conta_name\",\n",
    "                  \"ultimo_modelo\",\n",
    "                  F.col(\"recommend_a\").alias(\"next_best_model\"),\n",
    "                  \"rank\",\n",
    "                  \"confidence_ba\",\n",
    "                  \"lift_ba\"\n",
    "              )\n",
    "              .orderBy(\"conta_name\",\"rank\")\n",
    ")\n",
    "\n",
    "reco_cliente.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.reco_modelo_cliente_topK\")\n",
    "display(spark.table(\"sc_gold.reco_modelo_cliente_topK\").limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee792744-b6d8-4993-9281-43be69808434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Heatmap B→A (lift) com matplotlib (sem seaborn)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# escolher top triggers e top recomendações pelos melhores lifts\n",
    "rb = spark.table(\"sc_gold.mba_modelos_rules_BA\")\n",
    "\n",
    "TOP_TRIGGERS = 15\n",
    "TOP_RECS     = 15\n",
    "\n",
    "top_triggers = (rb.groupBy(\"trigger_b\").agg(F.max(\"lift_ba\").alias(\"mx\"))\n",
    "                  .orderBy(F.desc(\"mx\")).limit(TOP_TRIGGERS)\n",
    "                  .select(\"trigger_b\").toPandas()[\"trigger_b\"].tolist())\n",
    "\n",
    "top_recs = (rb.groupBy(\"recommend_a\").agg(F.max(\"lift_ba\").alias(\"mx\"))\n",
    "              .orderBy(F.desc(\"mx\")).limit(TOP_RECS)\n",
    "              .select(\"recommend_a\").toPandas()[\"recommend_a\"].tolist())\n",
    "\n",
    "rb_top = (rb.where(F.col(\"trigger_b\").isin(top_triggers) & F.col(\"recommend_a\").isin(top_recs))\n",
    "            .select(\"trigger_b\",\"recommend_a\",\"lift_ba\"))\n",
    "\n",
    "pdf = rb_top.toPandas()\n",
    "if pdf.empty:\n",
    "    raise ValueError(\"Sem dados suficientes para o heatmap. Aumenta TOP_TRIGGERS/TOP_RECS.\")\n",
    "\n",
    "pivot = pdf.pivot_table(index=\"trigger_b\", columns=\"recommend_a\", values=\"lift_ba\", fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.imshow(pivot.values)\n",
    "plt.xticks(range(len(pivot.columns)), pivot.columns, rotation=90)\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "plt.title(\"Heatmap B→A (Lift) — Trigger: modelo comprado (B)  |  Recomendação: A\")\n",
    "plt.colorbar(label=\"Lift\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5266a258-c1a6-46e3-8759-8594ad220722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tabela para Marketing (limpa, 1 linha por trigger & rank)\n",
    "marketing = (\n",
    "    spark.table(\"sc_gold.reco_modelo_by_trigger\")\n",
    "         .select(\n",
    "             F.col(\"trigger_b\").alias(\"modelo_comprado\"),\n",
    "             F.col(\"rank\").alias(\"rank_reco\"),\n",
    "             F.col(\"recommend_a\").alias(\"modelo_recomendado\"),\n",
    "             F.round(\"confidence_ba\", 4).alias(\"confidence\"),\n",
    "             F.round(\"lift_ba\", 4).alias(\"lift\"),\n",
    "             F.round(\"support_ab\", 4).alias(\"support_ab\"),\n",
    "             F.round(\"support_b\", 4).alias(\"support_b\")\n",
    "         )\n",
    "         .orderBy(\"modelo_comprado\",\"rank_reco\")\n",
    ")\n",
    "\n",
    "marketing.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sc_gold.marketing_next_best_model\")\n",
    "display(spark.table(\"sc_gold.marketing_next_best_model\").limit(50))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3022963075707354,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) last hope",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
