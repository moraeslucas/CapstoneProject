{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1af241-1522-41d3-a0ec-b2405405222d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sc_gold.historico_de_servicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565c8e7c-b0cb-493f-bd7e-1f689022cad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.viaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81f6ee7-a04f-4f27-92fa-d5014f89983d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS sc_gold.historico_de_servicos_2;\n",
    "\n",
    "CREATE TABLE sc_gold.historico_de_servicos_2 AS\n",
    "SELECT numero_do_servico_pos_venda,data_de_fecho,data_de_abertura,canal_de_venda,data_servico_pos_venda,descricao_servico_pos_venda,viatura,ordem_reparacao,tipo_de_servico,origem_registo,pedido_do_cliente\n",
    "FROM sc_gold.historico_de_servicos;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292f36a0-f783-41e7-943c-a0bade04b7ae",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760135579565}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.historico_de_servicos_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6a6bc3-9d1c-4629-b66c-b92b2eedb80b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "DROP TABLE IF EXISTS sc_gold.viaturas_2;\n",
    "\n",
    "CREATE TABLE sc_gold.viaturas_2 AS\n",
    "SELECT id,designacao_comercial,modelo,motorizacao,versao,data_de_matricula,cilindrada__cm3_,potencia_maxima__kw_,combustivel,gwms_engine,production_date\n",
    "FROM sc_gold.viaturas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23418cb4-e815-4090-bf32-5d21de728ea0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760135510949}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sc_gold.viaturas_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5789e3-05b8-473a-b11a-42f434f32e8d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761347545464}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "table_name = \"sc_gold.viaturas_2\"\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Get total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate percentage of nulls for each column\n",
    "null_percentages = (\n",
    "    df.select([\n",
    "        (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100)\n",
    "        .alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886dd075-4a6d-4ace-8486-e192e2d65771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType, DecimalType\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0f653d-2cd8-4c7d-ad53-702d9f96ee9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#passo usado para remover linhas (excluir linhas vazias na coluna modelo -486 linhas)\n",
    "table_name = \"sc_gold.viaturas_2\"\n",
    "df0 = spark.table(table_name)\n",
    "\n",
    "\n",
    "# Aplicar o filtro (excluir linhas vazias na coluna modelo -486 linhas)\n",
    "df = df0.filter(\n",
    "    (F.col(\"modelo\").isNotNull())\n",
    "    & (F.trim(F.col(\"modelo\")) != \"\")\n",
    "    & (F.lower(F.trim(F.col(\"modelo\"))) != \"null\")\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Gravar de volta sobrescrevendo a tabela original\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57bf0203-7d18-41aa-bed8-cc0ab4675436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#descobrir o numero de dias em media de diferen√ßa entre a data de produ√ß√£o e a data da matricula\n",
    "\n",
    "# Carregar a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Converter para DATE (com v√°rios formatos tolerados, se necess√°rio)\n",
    "df = df.withColumn(\n",
    "    \"data_de_matricula_dt\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'dd/MM/yyyy')\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"production_date_dt\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(production_date, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'dd/MM/yyyy')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Remover linhas com valores nulos em qualquer das datas\n",
    "df_valid = df.filter(F.col(\"data_de_matricula_dt\").isNotNull() & F.col(\"production_date_dt\").isNotNull())\n",
    "\n",
    "# Calcular diferen√ßa em dias\n",
    "df_valid = df_valid.withColumn(\"diff_days\", \n",
    "                               F.datediff(F.col(\"data_de_matricula_dt\"), F.col(\"production_date_dt\")))\n",
    "\n",
    "# Somat√≥rio e m√©dia\n",
    "agg = df_valid.agg(\n",
    "    F.sum(\"diff_days\").alias(\"soma_dias\"),\n",
    "    F.count(\"diff_days\").alias(\"n_linhas\"),\n",
    "    F.avg(\"diff_days\").alias(\"media_dias\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"üîπ Soma total dos dias:\", agg[\"soma_dias\"])\n",
    "print(\"üîπ N√∫mero de linhas usadas:\", agg[\"n_linhas\"])\n",
    "print(\"üîπ M√©dia de dias:\", round(agg[\"media_dias\"], 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273191fc-4da1-4bfb-b7fa-cfe48d5482ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#substituir data de produ√ß√£o e a data da matricula +/-131 com base no calculo anterior\n",
    "\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Tenta converter em v√°rios formatos comuns\n",
    "df = df.withColumn(\n",
    "    \"data_de_matricula\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'dd/MM/yyyy')\"),\n",
    "        F.expr(\"try_to_date(data_de_matricula, 'yyyy/MM/dd')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"production_date\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(production_date, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'dd/MM/yyyy')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'yyyy/MM/dd')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Aplicar as regras dos ¬±131 dias\n",
    "df = df.withColumn(\n",
    "    \"production_date\",\n",
    "    F.when(F.col(\"production_date\").isNull() & F.col(\"data_de_matricula\").isNotNull(),\n",
    "           F.date_sub(F.col(\"data_de_matricula\"), 131))\n",
    "     .otherwise(F.col(\"production_date\"))\n",
    ").withColumn(\n",
    "    \"data_de_matricula\",\n",
    "    F.when(F.col(\"data_de_matricula\").isNull() & F.col(\"production_date\").isNotNull(),\n",
    "           F.date_add(F.col(\"production_date\"), 131))\n",
    "     .otherwise(F.col(\"data_de_matricula\"))\n",
    ")\n",
    "\n",
    "# Reescrever a tabela (permitindo schema overwrite se necess√°rio)\n",
    "(df.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ea15f6-3227-4af7-9f55-193f5ebb8a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#criar nova coluna que √© o ano de produ√ß√£o do carro\n",
    "# Carregar tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Converter production_date para DATE (caso ainda seja string)\n",
    "df = df.withColumn(\n",
    "    \"production_date_dt\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(production_date, 'dd-MM-yyyy')\"),\n",
    "        F.expr(\"try_to_date(production_date, 'yyyy-MM-dd')\"),\n",
    "     F.expr(\"try_to_date(production_date, 'dd/MM/yyyy')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extrair o ano de produ√ß√£o\n",
    "df = df.withColumn(\"production_year\", F.year(\"production_date_dt\"))\n",
    "\n",
    "# Calcular idade em anos at√© a data de hoje\n",
    "df = df.withColumn(\n",
    "    \"age_year\",\n",
    "    F.floor(F.datediff(F.current_date(), F.col(\"production_date_dt\")) / 365.25)\n",
    ")\n",
    "\n",
    "# Regravar a tabela com as novas colunas\n",
    "(df.write\n",
    "   .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\", \"true\")  # garante que aceita as novas colunas\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61955c29-0746-4e56-ab71-fd513ba3315f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calcular a m√©dia de cilindrada (cilindrada__cm3_) por (gwms_engine + modelo + motoriza√ß√£o)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Janela por atributos do grupo\n",
    "w = W.partitionBy(\"gwms_engine\", \"motorizacao\", \"modelo\")\n",
    "\n",
    "# 3) m√©dia global (fallback) ‚Äî j√° arredondada para inteiro\n",
    "global_avg = df.select(F.avg(\"cilindrada__cm3_\").alias(\"g\")).first()[\"g\"]\n",
    "if global_avg is not None:\n",
    "    global_avg = int(round(global_avg))\n",
    "\n",
    "# 4) Preencher nulos com a m√©dia do grupo arredondada (ou fallback global)\n",
    "df_filled = (\n",
    "    df\n",
    "    .withColumn(\"avg_grupo\", F.avg(\"cilindrada__cm3_\").over(w))\n",
    "    .withColumn(\n",
    "        \"cilindrada__cm3_\",\n",
    "        F.when(\n",
    "            F.col(\"cilindrada__cm3_\").isNull(),\n",
    "            F.coalesce(F.round(F.col(\"avg_grupo\")).cast(IntegerType()), F.lit(global_avg))\n",
    "        ).otherwise(F.col(\"cilindrada__cm3_\").cast(IntegerType()))\n",
    "    )\n",
    "    .drop(\"avg_grupo\")\n",
    ")\n",
    "\n",
    "\n",
    "# 5) Escrever o RESULTADO correto\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a7c3ad1-195b-4815-8a54-582ec1af7a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calcular a m√©dia de potencia (potencia_maxima__kw_) por (gwms_engine + modelo + motorizacao)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Limpeza e cast:\n",
    "#    - troca v√≠rgula decimal por ponto\n",
    "#    - remove qualquer caractere n√£o num√©rico (p.ex. ' kW', espa√ßos, etc.)\n",
    "pot_clean = F.regexp_replace(F.col(\"potencia_maxima__kw_\"), \",\", \".\")\n",
    "pot_clean = F.regexp_replace(pot_clean, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"potencia_maxima__kw_\", pot_clean.cast(DoubleType()))\n",
    "\n",
    "# 3) M√©dia global (fallback), arredondada a 1 casa\n",
    "global_avg = df.select(F.avg(\"potencia_maxima__kw_\").alias(\"g\")).first()[\"g\"]\n",
    "global_avg_1d = round(global_avg, 1) if global_avg is not None else None\n",
    "\n",
    "# 4) M√©dia por grupo\n",
    "keys = [\"gwms_engine\", \"motorizacao\", \"modelo\"]\n",
    "avg_by_group = (\n",
    "    df.groupBy(*keys)\n",
    "      .agg(F.avg(\"potencia_maxima__kw_\").alias(\"avg_grp\"))\n",
    ")\n",
    "\n",
    "# 5) Preencher apenas nulos com a m√©dia do grupo (1 casa decimal);\n",
    "#    se o grupo for todo nulo, usa m√©dia global\n",
    "df_filled = (\n",
    "    df.join(avg_by_group, on=keys, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"potencia_maxima__kw_\",\n",
    "          F.when(\n",
    "              F.col(\"potencia_maxima__kw_\").isNull(),\n",
    "              F.coalesce(F.round(F.col(\"avg_grp\"), 1), F.lit(global_avg_1d))\n",
    "          ).otherwise(F.col(\"potencia_maxima__kw_\"))\n",
    "      )\n",
    "      .drop(\"avg_grp\")\n",
    ")\n",
    "\n",
    "# (Opcional) Se quiser NORMALIZAR toda a coluna para 1 casa decimal, inclusive n√£o nulos:\n",
    "# df_filled = df_filled.withColumn(\"potencia_maxima__kw_\", F.round(F.col(\"potencia_maxima__kw_\"), 1))\n",
    "\n",
    "# (Opcional) Fixar o tipo para Decimal(10,1) no schema (em vez de double):\n",
    "# df_filled = df_filled.withColumn(\"potencia_maxima__kw_\", F.col(\"potencia_maxima__kw_\").cast(DecimalType(10,1)))\n",
    "\n",
    "# 6) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad87d51-6ef6-4213-9f00-efb8860a3f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preencher 'motorizacao' com a MODA por grupo (gwms_engine, modelo, potencia_maxima__kw_, combustivel)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Pot√™ncia: v√≠rgula -> ponto, remover ru√≠do e cast para double (porque √© chave do grupo)\n",
    "pot = F.regexp_replace(F.col(\"potencia_maxima__kw_\"), \",\", \".\")\n",
    "pot = F.regexp_replace(pot, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"potencia_maxima__kw_\", pot.cast(DoubleType()))\n",
    "\n",
    "# 3) Higienizar texto relevante\n",
    "for col in [\"motorizacao\", \"combustivel\", \"gwms_engine\", \"modelo\"]:\n",
    "    df = df.withColumn(col, F.trim(F.col(col)))\n",
    "df = df.withColumn(\"motorizacao\", F.when(F.col(\"motorizacao\") == \"\", None).otherwise(F.col(\"motorizacao\")))\n",
    "df = df.withColumn(\"combustivel\", F.when(F.col(\"combustivel\") == \"\", None).otherwise(F.col(\"combustivel\")))\n",
    "df = df.withColumn(\"gwms_engine\", F.when(F.col(\"gwms_engine\") == \"\", None).otherwise(F.col(\"gwms_engine\")))\n",
    "\n",
    "# 4) Chaves do grupo\n",
    "keys_mot = [\"gwms_engine\", \"modelo\", \"potencia_maxima__kw_\", \"combustivel\"]\n",
    "\n",
    "# 5) Moda de 'motorizacao' por grupo (desempate alfab√©tico)\n",
    "counts_mot = (\n",
    "    df.filter(F.col(\"motorizacao\").isNotNull())\n",
    "      .groupBy(*keys_mot, \"motorizacao\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    ")\n",
    "w_mot = W.partitionBy(*keys_mot).orderBy(F.col(\"cnt\").desc(), F.col(\"motorizacao\").asc())\n",
    "mode_motorizacao = (\n",
    "    counts_mot.withColumn(\"rn\", F.row_number().over(w_mot))\n",
    "              .filter(F.col(\"rn\") == 1)\n",
    "              .select(*keys_mot, F.col(\"motorizacao\").alias(\"mode_motorizacao\"))\n",
    ")\n",
    "\n",
    "# 6) Moda global de 'motorizacao' (fallback opcional)\n",
    "row_mot = (\n",
    "    df.filter(F.col(\"motorizacao\").isNotNull())\n",
    "      .groupBy(\"motorizacao\").agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    "      .orderBy(F.col(\"cnt\").desc(), F.col(\"motorizacao\").asc())\n",
    "      .limit(1).first()\n",
    ")\n",
    "global_mode_mot = row_mot[\"motorizacao\"] if row_mot else None\n",
    "\n",
    "# 7) Preencher APENAS nulos de 'motorizacao' com a moda do grupo (ou global)\n",
    "df_filled = (\n",
    "    df.join(mode_motorizacao, on=keys_mot, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"motorizacao\",\n",
    "          F.when(F.col(\"motorizacao\").isNull(),\n",
    "                 F.coalesce(F.col(\"mode_motorizacao\"), F.lit(global_mode_mot)))\n",
    "           .otherwise(F.col(\"motorizacao\"))\n",
    "      )\n",
    "      .drop(\"mode_motorizacao\")\n",
    ")\n",
    "\n",
    "# 8) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb7c7e9-687b-4165-a410-dd898af267d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preencher 'gwms_engine' pela MODA por grupo (cilindrada__cm3_, potencia_maxima__kw_, modelo, motorizacao)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Higienizar num√©ricos (v√≠rgula -> ponto; remover ru√≠do) e fazer cast\n",
    "cil_clean = F.regexp_replace(F.col(\"cilindrada__cm3_\"), \",\", \".\")\n",
    "cil_clean = F.regexp_replace(cil_clean, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"cilindrada__cm3_\", cil_clean.cast(DoubleType()))\n",
    "\n",
    "pot_clean = F.regexp_replace(F.col(\"potencia_maxima__kw_\"), \",\", \".\")\n",
    "pot_clean = F.regexp_replace(pot_clean, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"potencia_maxima__kw_\", pot_clean.cast(DoubleType()))\n",
    "\n",
    "# 3) Higienizar texto: trim e strings vazias -> NULL\n",
    "df = df.withColumn(\"gwms_engine\", F.when(F.trim(F.col(\"gwms_engine\")) == \"\", None)\n",
    "                                  .otherwise(F.trim(F.col(\"gwms_engine\"))))\n",
    "df = df.withColumn(\"modelo\", F.trim(F.col(\"modelo\")))\n",
    "# \"motorizacao\" sem acento ‚Äî confirme o nome exato na tabela\n",
    "df = df.withColumn(\"motorizacao\", F.when(F.trim(F.col(\"motorizacao\")) == \"\", None)\n",
    "                                   .otherwise(F.trim(F.col(\"motorizacao\"))))\n",
    "\n",
    "# 4) Chaves do grupo\n",
    "keys = [\"cilindrada__cm3_\", \"potencia_maxima__kw_\", \"modelo\", \"motorizacao\"]\n",
    "\n",
    "# 5) Calcular a MODA de gwms_engine por grupo\n",
    "counts = (\n",
    "    df.filter(F.col(\"gwms_engine\").isNotNull())\n",
    "      .groupBy(*keys, \"gwms_engine\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    ")\n",
    "\n",
    "w = W.partitionBy(*keys).orderBy(F.col(\"cnt\").desc(), F.col(\"gwms_engine\").asc())\n",
    "\n",
    "mode_by_group = (\n",
    "    counts.withColumn(\"rn\", F.row_number().over(w))\n",
    "          .filter(F.col(\"rn\") == 1)\n",
    "          .select(*keys, F.col(\"gwms_engine\").alias(\"mode_gwms_engine\"))\n",
    ")\n",
    "\n",
    "# 6) (Opcional) Moda GLOBAL como fallback se o grupo n√£o tiver ocorr√™ncias v√°lidas\n",
    "row_global = (\n",
    "    df.filter(F.col(\"gwms_engine\").isNotNull())\n",
    "      .groupBy(\"gwms_engine\").agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    "      .orderBy(F.col(\"cnt\").desc(), F.col(\"gwms_engine\").asc())\n",
    "      .limit(1).first()\n",
    ")\n",
    "global_mode = row_global[\"gwms_engine\"] if row_global else None\n",
    "\n",
    "# 7) Preencher APENAS nulos com a moda do grupo (ou moda global se necess√°rio)\n",
    "df_filled = (\n",
    "    df.join(mode_by_group, on=keys, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"gwms_engine\",\n",
    "          F.when(\n",
    "              F.col(\"gwms_engine\").isNull(),\n",
    "              F.coalesce(F.col(\"mode_gwms_engine\"), F.lit(global_mode))\n",
    "          ).otherwise(F.col(\"gwms_engine\"))\n",
    "      )\n",
    "      .drop(\"mode_gwms_engine\")\n",
    ")\n",
    "\n",
    "# 8) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27df856f-149c-466a-8f82-e5b8b09d2358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MODA de combustivel por (gwms_engine, cilindrada__cm3_, modelo, motorizacao)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Higienizar colunas num√©ricas (garantir double)\n",
    "cil = F.regexp_replace(F.col(\"cilindrada__cm3_\"), \",\", \".\")\n",
    "cil = F.regexp_replace(cil, r\"[^0-9.]\", \"\")\n",
    "df = df.withColumn(\"cilindrada__cm3_\", cil.cast(DoubleType()))\n",
    "\n",
    "# 3) Higienizar colunas de texto\n",
    "df = df.withColumn(\"gwms_engine\", F.when(F.length(F.trim(\"gwms_engine\")) == 0, None)\n",
    "                                   .otherwise(F.trim(F.col(\"gwms_engine\")).cast(\"string\")))\n",
    "df = df.withColumn(\"modelo\", F.trim(F.col(\"modelo\")).cast(\"string\"))\n",
    "df = df.withColumn(\"motorizacao\", F.when(F.length(F.trim(\"motorizacao\")) == 0, None)\n",
    "                                   .otherwise(F.trim(F.col(\"motorizacao\")).cast(\"string\")))\n",
    "df = df.withColumn(\"combustivel\", F.when(F.length(F.trim(\"combustivel\")) == 0, None)\n",
    "                                   .otherwise(F.trim(F.col(\"combustivel\")).cast(\"string\")))\n",
    "\n",
    "# 4) Chave do grupo\n",
    "keys = [\"gwms_engine\", \"cilindrada__cm3_\", \"modelo\", \"motorizacao\"]\n",
    "\n",
    "# 5) Moda de combustivel por grupo\n",
    "counts = (\n",
    "    df.filter(F.col(\"combustivel\").isNotNull())\n",
    "      .groupBy(*keys, \"combustivel\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    ")\n",
    "\n",
    "w = W.partitionBy(*keys).orderBy(F.col(\"cnt\").desc(), F.col(\"combustivel\").asc())\n",
    "\n",
    "mode_by_group = (\n",
    "    counts.withColumn(\"rn\", F.row_number().over(w))\n",
    "          .filter(F.col(\"rn\") == 1)\n",
    "          .select(*keys, F.col(\"combustivel\").alias(\"mode_combustivel\"))\n",
    ")\n",
    "\n",
    "# 6) Moda global como fallback\n",
    "row_global = (\n",
    "    df.filter(F.col(\"combustivel\").isNotNull())\n",
    "      .groupBy(\"combustivel\").agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    "      .orderBy(F.col(\"cnt\").desc(), F.col(\"combustivel\").asc())\n",
    "      .limit(1).first()\n",
    ")\n",
    "global_mode = row_global[\"combustivel\"] if row_global else None\n",
    "\n",
    "# 7) Preencher apenas nulos\n",
    "df_filled = (\n",
    "    df.join(mode_by_group, on=keys, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"combustivel\",\n",
    "          F.when(\n",
    "              F.col(\"combustivel\").isNull(),\n",
    "              F.coalesce(F.col(\"mode_combustivel\"), F.lit(global_mode))\n",
    "          ).otherwise(F.col(\"combustivel\"))\n",
    "      )\n",
    "      .drop(\"mode_combustivel\")\n",
    ")\n",
    "\n",
    "# 8) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc31894-c0ae-4ad4-b79e-34353bee056c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MODA de designacao_comercial por (gwms_engine, modelo, motorizacao)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Higienizar texto (trim + strings vazias -> NULL)\n",
    "for col in [\"designacao_comercial\", \"modelo\", \"gwms_engine\", \"motorizacao\"]:\n",
    "    df = df.withColumn(col, F.trim(F.col(col)))\n",
    "    df = df.withColumn(col, F.when(F.col(col) == \"\", None).otherwise(F.col(col)))\n",
    "\n",
    "# 3) Chaves do grupo\n",
    "keys = [\"gwms_engine\", \"modelo\", \"motorizacao\"]\n",
    "\n",
    "# 4) Moda por grupo\n",
    "counts = (\n",
    "    df.filter(F.col(\"designacao_comercial\").isNotNull())\n",
    "      .groupBy(*keys, \"designacao_comercial\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    ")\n",
    "\n",
    "w = W.partitionBy(*keys).orderBy(F.col(\"cnt\").desc(), F.col(\"designacao_comercial\").asc())\n",
    "\n",
    "mode_by_group = (\n",
    "    counts.withColumn(\"rn\", F.row_number().over(w))\n",
    "          .filter(F.col(\"rn\") == 1)\n",
    "          .select(*keys, F.col(\"designacao_comercial\").alias(\"mode_designacao_comercial\"))\n",
    ")\n",
    "\n",
    "# 5) Moda global como fallback\n",
    "row_global = (\n",
    "    df.filter(F.col(\"designacao_comercial\").isNotNull())\n",
    "      .groupBy(\"designacao_comercial\").agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    "      .orderBy(F.col(\"cnt\").desc(), F.col(\"designacao_comercial\").asc())\n",
    "      .limit(1).first()\n",
    ")\n",
    "global_mode = row_global[\"designacao_comercial\"] if row_global else None\n",
    "\n",
    "# 6) Preencher apenas nulos\n",
    "df_filled = (\n",
    "    df.join(mode_by_group, on=keys, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"designacao_comercial\",\n",
    "          F.when(\n",
    "              F.col(\"designacao_comercial\").isNull(),\n",
    "              F.coalesce(F.col(\"mode_designacao_comercial\"), F.lit(global_mode))\n",
    "          ).otherwise(F.col(\"designacao_comercial\"))\n",
    "      )\n",
    "      .drop(\"mode_designacao_comercial\")\n",
    ")\n",
    "\n",
    "# 7) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9208e12b-e2fb-4b48-aeb9-64b658155bb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preencher 'versao' com a MODA por grupo (modelo, gwms_engine, motorizacao, age_year)\n",
    "# 1) Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# 2) Higienizar texto (trim e strings vazias -> NULL)\n",
    "for col in [\"versao\", \"modelo\", \"gwms_engine\", \"motorizacao\", \"age_year\"]:\n",
    "    df = df.withColumn(col, F.trim(F.col(col)))\n",
    "    df = df.withColumn(col, F.when(F.col(col) == \"\", None).otherwise(F.col(col)))\n",
    "\n",
    "# 3) Definir chaves do grupo\n",
    "keys = [\"modelo\", \"gwms_engine\", \"motorizacao\", \"age_year\"]\n",
    "\n",
    "# 4) Calcular a moda de 'versao' por grupo\n",
    "counts = (\n",
    "    df.filter(F.col(\"versao\").isNotNull())\n",
    "      .groupBy(*keys, \"versao\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    ")\n",
    "\n",
    "w = W.partitionBy(*keys).orderBy(F.col(\"cnt\").desc(), F.col(\"versao\").asc())\n",
    "\n",
    "mode_by_group = (\n",
    "    counts.withColumn(\"rn\", F.row_number().over(w))\n",
    "          .filter(F.col(\"rn\") == 1)\n",
    "          .select(*keys, F.col(\"versao\").alias(\"mode_versao\"))\n",
    ")\n",
    "\n",
    "# 5) Moda global como fallback\n",
    "row_global = (\n",
    "    df.filter(F.col(\"versao\").isNotNull())\n",
    "      .groupBy(\"versao\").agg(F.count(F.lit(1)).alias(\"cnt\"))\n",
    "      .orderBy(F.col(\"cnt\").desc(), F.col(\"versao\").asc())\n",
    "      .limit(1).first()\n",
    ")\n",
    "global_mode = row_global[\"versao\"] if row_global else None\n",
    "\n",
    "# 6) Preencher apenas nulos de 'versao'\n",
    "df_filled = (\n",
    "    df.join(mode_by_group, on=keys, how=\"left\")\n",
    "      .withColumn(\n",
    "          \"versao\",\n",
    "          F.when(\n",
    "              F.col(\"versao\").isNull(),\n",
    "              F.coalesce(F.col(\"mode_versao\"), F.lit(global_mode))\n",
    "          ).otherwise(F.col(\"versao\"))\n",
    "      )\n",
    "      .drop(\"mode_versao\")\n",
    ")\n",
    "\n",
    "# 7) Escrever resultado\n",
    "(df_filled.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a8a2e76-f06b-4cd5-8bdf-ae118c1fdb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Passo usado para remover nulos das datas (confirmar) e remover coluna duplicada 'data_de_matricula_dt'\n",
    "#  Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Remover linhas onde production_date √© NULL\n",
    "df_clean = df.filter(F.col(\"age_year\").isNotNull())\n",
    "\n",
    "\n",
    "# (opcional) sobrescrever a tabela com o dataset limpo\n",
    "(df_clean.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"sc_gold.viaturas_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12afa2b0-9517-4630-8989-1cb01cb72bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import corr\n",
    "#df.select(corr(\"production_year\", \"age_year\").alias(\"corr\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b352cea-7307-4891-8167-012fd6995473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Passo usado para remover coluna \"production_year\" vai distorcer o clustering porque est√°s a ‚Äúcontar duas vezes‚Äù o mesmo fator \"age_year\". Alem disso removemos \"data_de_matricula\",\"production_date\" pois nao √© bom usar datas para os clusters\n",
    "\n",
    "#  Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "# Remover a coluna 'production_date_dt'\n",
    "df_clean = df.drop(\"production_year\",\"data_de_matricula\",\"production_date\",\"production_date_dt\",\"designacao_comercial\", 'versao','gwms_engine')\n",
    "\n",
    "# (opcional) sobrescrever a tabela com o dataset limpo\n",
    "(df_clean.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50b56d2-fac1-45d8-87df-b60c3c56b676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converter age_year para inteiro\n",
    "df = df.withColumn(\"age_year\", F.col(\"age_year\").cast(IntegerType()))\n",
    "\n",
    "# Guardar a tabela sobrescrevendo a anterior\n",
    "(df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n",
    "\n",
    "# Confirmar no DF atualizado\n",
    "df.select(\"age_year\").distinct().show(20)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506cedef-c156-4a71-8da6-4dfd03fdfaef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#colocar anos em intervalos\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"age_interval\",\n",
    "    F.when(F.col(\"age_year\") >= 20, \"20+\")\n",
    "     .otherwise(\n",
    "         F.concat(\n",
    "             (F.floor(F.col(\"age_year\") / 5) * 5).cast(\"string\"),\n",
    "             F.lit(\"-\"),\n",
    "             ((F.floor(F.col(\"age_year\") / 5) * 5) + 4).cast(\"string\")\n",
    "         )\n",
    "     )\n",
    ")\n",
    "\n",
    "\n",
    "(df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e52b30d-bbf1-4a6b-9a3b-6a0f630b5504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#escolher primeira palavra da coluna modelo para diminuir a granularidade desta coluna\n",
    "#  Ler a tabela\n",
    "df = spark.table(\"sc_gold.viaturas_2\")\n",
    "\n",
    "df = df.withColumn(\"modelo\", F.split(F.col(\"modelo\"), \" \").getItem(0))\n",
    "\n",
    "(df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5767d39-9823-4f9e-b7b3-69d76faa9eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#para reduzir numero de linhas\n",
    "#df = df.filter(F.col(\"age_interval\") != \"20+\")\n",
    "\n",
    "#(df.write\n",
    " #  .mode(\"overwrite\")\n",
    " #  .option(\"overwriteSchema\", \"true\")\n",
    " #  .saveAsTable(\"sc_gold.viaturas_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f905e00e-a74b-4ae1-98ee-2d8c7ad22cda",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760184095279}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "table_name = \"sc_gold.viaturas_2\"\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Get total rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate percentage of nulls for each column\n",
    "null_percentages = (\n",
    "    df.select([\n",
    "        (F.count(F.when(F.col(c).isNull(), c)) / total_rows * 100)\n",
    "        .alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n",
    "\n",
    "display(null_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adde2d9f-956f-4593-b11a-784feea11180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "prepara√ß√£o para K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7ac3a7-dace-4887-98cc-2052cc01bee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"sc_gold.viaturas_2\"\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c09787-f440-46de-b91c-91ef87b7f37e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transformar categorias em vari√°veis num√©ricas\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "#categorical_cols = [\"motorizacao\", \"combustivel\", \"gwms_engine\", \"designacao_comercial\", \"versao\",\"modelo\"]\n",
    "categorical_cols = [\"motorizacao\", \"combustivel\", \"age_interval\",\"modelo\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\") for c in categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da27fe30-103b-4dd8-b065-26ce5d37db6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Normalizar vari√°veis num√©ricas\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "numeric_cols = [\"cilindrada__cm3_\", \"potencia_maxima__kw_\"] \n",
    "\n",
    "\n",
    "assembler_num = VectorAssembler(inputCols=numeric_cols, outputCol=\"numeric_features\")\n",
    "scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric\", withMean=True, withStd=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c84c50-1b81-4408-a6c0-c9d9306986e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Juntar tudo em vetor de features\n",
    "feature_cols = [\"scaled_numeric\"] + [c+\"_vec\" for c in categorical_cols]\n",
    "final_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7286354-999f-4df9-afda-cba2fd4a3c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import FeatureHasher, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# 1) Simplificar categorias: usar MARCA e colapsar raros\n",
    "df = df.withColumn(\"marca\", F.split(F.trim(F.col(\"modelo\")), \" \").getItem(0))\n",
    "\n",
    "def collapse_rare(df, col, min_count=500, new_col=None):\n",
    "    new_col = new_col or f\"{col}_c\"\n",
    "    rare = (df.groupBy(col).count().filter(F.col(\"count\") < min_count)\n",
    "              .select(col).withColumn(\"is_rare\", F.lit(True)))\n",
    "    df = df.join(rare, on=col, how=\"left\")\n",
    "    return df.withColumn(new_col, F.when(F.col(\"is_rare\"), \"OUTROS\").otherwise(F.col(col))).drop(\"is_rare\")\n",
    "\n",
    "df = collapse_rare(df, \"marca\",       500, \"marca_c\")\n",
    "df = collapse_rare(df, \"motorizacao\", 500, \"motorizacao_c\")\n",
    "\n",
    "cat_cols = [\"motorizacao_c\",\"combustivel\",\"age_interval\",\"marca_c\"]\n",
    "num_cols = [\"cilindrada__cm3_\",\"potencia_maxima__kw_\"]\n",
    "\n",
    "# 2) Normaliza√ß√£o manual (sem StandardScalerModel)\n",
    "from pyspark.sql import functions as F\n",
    "stats = df.select(\n",
    "    *[F.mean(c).alias(f\"{c}_mean\") for c in num_cols],\n",
    "    *[F.stddev_samp(c).alias(f\"{c}_std\") for c in num_cols]\n",
    ").collect()[0]\n",
    "for c in num_cols:\n",
    "    mu = float(stats[f\"{c}_mean\"]) if stats[f\"{c}_mean\"] is not None else 0.0\n",
    "    sd = float(stats[f\"{c}_std\"])  if stats[f\"{c}_std\"] not in (None,0) else 1.0\n",
    "    df = df.withColumn(f\"{c}_z\", (F.col(c)-F.lit(mu))/F.lit(sd))\n",
    "num_z_cols = [f\"{c}_z\" for c in num_cols]\n",
    "\n",
    "# 3) Hashing (sem fit) + assemble\n",
    "hasher = FeatureHasher(inputCols=cat_cols, outputCol=\"cat_hashed\", numFeatures=1024)\n",
    "df_h = hasher.transform(df)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "df_ready = VectorAssembler(inputCols=[\"cat_hashed\"]+num_z_cols, outputCol=\"features\").transform(df_h)\n",
    "\n",
    "# 4) KMeans + Silhouette (sem PCA, leve)\n",
    "FEATURES_COL = \"features\"\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(featuresCol=FEATURES_COL, predictionCol=\"prediction\",\n",
    "                                metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "\n",
    "best_k, best_s = None, None\n",
    "for k in range(3,9):\n",
    "    m = KMeans(featuresCol=FEATURES_COL, predictionCol=\"prediction\", k=k, seed=42, maxIter=15).fit(df_ready)\n",
    "    s = evaluator.evaluate(m.transform(df_ready))\n",
    "    print(f\"k={k} | silhouette={s:.4f}\")\n",
    "    if best_s is None or s>best_s or (s==best_s and (best_k is None or k<best_k)): best_k, best_s = k, s\n",
    "\n",
    "final = KMeans(featuresCol=FEATURES_COL, predictionCol=\"cluster\", k=best_k, seed=42, maxIter=20).fit(df_ready)\n",
    "df_clusters = final.transform(df_ready)\n",
    "df_clusters.groupBy(\"cluster\").count().orderBy(\"cluster\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56159da6-a637-4ebe-bd71-1f93c4bed364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "cats = [\"motorizacao_c\",\"combustivel\",\"age_interval\",\"marca_c\"]\n",
    "\n",
    "# Top 10 de cada categ√≥rica por cluster, com % interna\n",
    "for c in cats:\n",
    "    print(f\"\\n=== {c} ===\")\n",
    "    (df_clusters\n",
    "     .groupBy(\"cluster\", c)\n",
    "     .count()\n",
    "     .withColumn(\"pct\", F.col(\"count\")/F.sum(\"count\").over(Window.partitionBy(\"cluster\")))\n",
    "     .orderBy(\"cluster\", F.desc(\"count\"))\n",
    "     .show(10, truncate=False))\n",
    "\n",
    "# M√©dias/medianas num√©ricas por cluster\n",
    "(df_clusters\n",
    " .groupBy(\"cluster\")\n",
    " .agg(\n",
    "     F.count(\"*\").alias(\"n\"),\n",
    "     F.avg(\"cilindrada__cm3_\").alias(\"cilindrada_avg\"),\n",
    "     F.expr(\"percentile_approx(cilindrada__cm3_, 0.5)\").alias(\"cilindrada_med\"),\n",
    "     F.avg(\"potencia_maxima__kw_\").alias(\"pot_kw_avg\"),\n",
    "     F.expr(\"percentile_approx(potencia_maxima__kw_, 0.5)\").alias(\"pot_kw_med\")\n",
    " )\n",
    " .orderBy(\"cluster\")\n",
    " .show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "300f1ef9-554f-47eb-962d-1d95be048c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üü° Cluster 0 (50 992 viaturas, ~33%)\n",
    "\n",
    "Resumo: maioritariamente ve√≠culos antigos, cilindrada alta, gas√≥leo, motores 1.0/1.6 T-GDI? ‚Üí parece um cluster h√≠brido entre comerciais antigos + SUVs Diesel\n",
    "\n",
    "Dimens√£o\tObserva√ß√£o\n",
    "Motoriza√ß√£o\t70 % ‚Äú1.0 T-GDI‚Äù (curioso ‚Äî pode ser efeito do hashing colapsado) + 1.6 T-GDI + 1.6/1.7 CRDi ‚Üí mistura de gasolina turbo e diesel\n",
    "Combust√≠vel\t98 % gas√≥leo (clar√≠ssimo: cluster Diesel)\n",
    "Idade\t65 % com 20+ anos, 16 % entre 15‚Äì19 anos ‚Üí viaturas mais velhas\n",
    "Marca (modelo simplificado)\tDom√≠nio de H-1, Galloper, Getz, Santa Fe, Tucson ‚Üí Hyundai/Kia antigos e comerciais\n",
    "Motor\tCilindrada m√©dia 2078 cm¬≥, mediana 2328 cm¬≥ ‚Üí motores grandes\n",
    "Pot√™ncia\tM√©dia ~75 kW ‚Üí diesel de baixa pot√™ncia t√≠pica de comerciais/antigos\n",
    "\n",
    "‚û°Ô∏è Label poss√≠vel: ‚ÄúDiesel antigos / comerciais / SUV anos 2000‚Äù\n",
    "\n",
    "üü° Cluster 1 (14 454 viaturas, ~9%)\n",
    "\n",
    "Resumo: viaturas novas / semi-novas, pot√™ncia elevada, grande presen√ßa de el√©tricos e h√≠bridos.\n",
    "\n",
    "Dimens√£o\tObserva√ß√£o\n",
    "Combust√≠vel\t39 % el√©trico, 19 % gasolina, 18 % h√≠brido gasolina ‚Üí cluster mais moderno / eletrificado\n",
    "Idade\t67 % entre 0‚Äì4 anos, 28 % entre 5‚Äì9 anos ‚Üí viaturas recentes\n",
    "Motor\tCilindrada m√©dia ~1500 cm¬≥, pot√™ncia m√©dia 138 kW (alto)\n",
    "Marcas\t(n√£o mostrado aqui mas deve ser Hyundai Ioniq, Kona EV, Kia EV6, etc.)\n",
    "\n",
    "‚û°Ô∏è Label poss√≠vel: ‚ÄúNovos / eletrificados / alta pot√™ncia‚Äù\n",
    "\n",
    "üü° Cluster 2 (79 382 viaturas, ~52%)\n",
    "\n",
    "Resumo: viaturas pequenas, baixas pot√™ncias, idade mista mas tend√™ncia interm√©dia, gasolina dominante.\n",
    "\n",
    "Dimens√£o\tObserva√ß√£o\n",
    "Cilindrada\tM√©dia ~1166 cm¬≥, mediana ~1086 ‚Üí segmento pequeno\n",
    "Pot√™ncia\tM√©dia 66 kW ‚Üí citadinos / utilit√°rios\n",
    "Combust√≠vel\t(n√£o mostrado, mas deve ser gasolina/1.0 MPI/etc.)\n",
    "Idade\tmix, mas n√£o t√£o velho como cluster 0 nem novo como cluster 1\n",
    "\n",
    "‚û°Ô∏è Label poss√≠vel: ‚ÄúPequenos utilit√°rios / baixa pot√™ncia / econ√≥mico‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2b00c0-076c-4a1b-8c91-c554b4024064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e635486-a1d7-456a-97c9-c67a240748ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"sc_gold.historico_de_servicos_2\") \\\n",
    "     .select(\"descricao_servico_pos_venda\") \\\n",
    "     .groupBy(\"descricao_servico_pos_venda\") \\\n",
    "     .count() \\\n",
    "     .orderBy(F.desc(\"count\")) \\\n",
    "     .show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "714e87e9-d842-41a5-9659-01da4f4f0cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== Imports ==================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "# ================== 0) Helpers ==================\n",
    "def remove_acentos(col):\n",
    "    # mapa b√°sico pt: √£√°√†√¢√§√©√®√™√´√≠√¨√Æ√Ø√µ√≥√≤√¥√∂√∫√π√ª√º√ß -> aaaaaeeeeiiiiooooouuuuc\n",
    "    accents = \"√£√°√†√¢√§√©√®√™√´√≠√¨√Æ√Ø√µ√≥√≤√¥√∂√∫√π√ª√º√ß√É√Å√Ä√Ç√Ñ√â√à√ä√ã√ç√å√é√è√ï√ì√í√î√ñ√ö√ô√õ√ú√á\"\n",
    "    no_acc  = \"aaaaaeeeeiiiiooooouuuucAAAAAEEEEIIIIOOOOOUUUUC\"\n",
    "    return F.translate(col, accents, no_acc)\n",
    "\n",
    "# ================== 1) Ler hist√≥rico e normalizar texto ==================\n",
    "tabela_servicos = \"sc_gold.historico_de_servicos_2\"\n",
    "\n",
    "df_serv = (spark.table(tabela_servicos)\n",
    "    .select(\n",
    "        F.col(\"viatura\").alias(\"id_viatura\"),\n",
    "        F.lower(remove_acentos(F.col(\"descricao_servico_pos_venda\"))).alias(\"serv_raw\"),\n",
    "        F.col(\"data_de_abertura\").alias(\"data_servico\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# remover nulos/strings vazias (importante)\n",
    "df_serv = df_serv.filter(F.col(\"serv_raw\").isNotNull() & (F.length(F.trim(F.col(\"serv_raw\"))) > 0))\n",
    "\n",
    "# ================== 2) Mapear para categorias t√©cnicas (servico_norm) ==================\n",
    "# op√ß√£o C: t√©cnico + manuten√ß√£o preventiva\n",
    "\n",
    "# palavras-chave ‚Üí categoria\n",
    "mapping = [\n",
    "    (\"revisao_programada\", [\"revis\", \"manutencao\", \"inspecao\", \" km\", \"anos\", \"60.000\", \"45.000\", \"30.000\", \"15.000\", \"60 000\", \"45 000\", \"30 000\", \"15 000\"]),\n",
    "    (\"oleo_motor\",         [\"oleo\", \"filtro oleo\", \"pack\"]),\n",
    "    (\"travagem\",           [\"pastilh\", \"trav\", \"disco\"]),\n",
    "    (\"pneus\",              [\"pneu\"]),\n",
    "    (\"bateria\",            [\"bateria\"]),\n",
    "    (\"ar_condicionado\",    [\"clima\", \"ar cond\", \"ac \", \" ac\", \"climat\"]),\n",
    "    (\"diagnostico_motor\",  [\"luz motor\", \"diagnos\", \"avaria\"]),\n",
    "    (\"sinistro\",           [\"chapa\", \"pintur\", \"sinistr\"]),\n",
    "    (\"ipo\",                [\"ipo\"]),\n",
    "    (\"combustivel\",        [\"combust\"])\n",
    "]\n",
    "\n",
    "# termos a excluir (ru√≠do administrativo/cosm√©tico)\n",
    "excluir = [\"lavag\", \"oferta\", \"check up\", \"tapete\", \"residu\", \"prepar\", \"estacao serv\", \"limpeza\", \"entrega\", \"pintura polimento\", \"chapas matric\"]\n",
    "\n",
    "# come√ßar coluna vazia\n",
    "df_map = df_serv.withColumn(\"servico_norm\", F.lit(None).cast(\"string\"))\n",
    "\n",
    "# aplicar mapeamento\n",
    "for cat, kws in mapping:\n",
    "    cond = F.lit(False)\n",
    "    for kw in kws:\n",
    "        cond = cond | F.col(\"serv_raw\").contains(kw)\n",
    "    df_map = df_map.withColumn(\n",
    "        \"servico_norm\",\n",
    "        F.when(cond, F.lit(cat)).otherwise(F.col(\"servico_norm\"))\n",
    "    )\n",
    "\n",
    "# remover ru√≠do\n",
    "cond_ruido = F.lit(False)\n",
    "for kw in excluir:\n",
    "    cond_ruido = cond_ruido | F.col(\"serv_raw\").contains(kw)\n",
    "\n",
    "df_clean = df_map.filter(~cond_ruido & F.col(\"servico_norm\").isNotNull())\n",
    "\n",
    "# (opcional) ver distribui√ß√£o das categorias\n",
    "df_clean.groupBy(\"servico_norm\").count().orderBy(F.desc(\"count\")).show(50, truncate=False)\n",
    "\n",
    "# ================== 3) Juntar clusters (assumindo df_clusters com colunas: id, cluster) ==================\n",
    "# se o id nas viaturas tiver outro nome, adapta abaixo\n",
    "df_clusters_sel = df_clusters.select(F.col(\"id\").alias(\"id_viatura\"), \"cluster\")\n",
    "\n",
    "base = (df_clean\n",
    "    .join(df_clusters_sel, on=\"id_viatura\", how=\"inner\")\n",
    "    .select(\"id_viatura\", \"cluster\", \"servico_norm\")\n",
    "    .distinct()  # evita duplica√ß√µes do mesmo servi√ßo na mesma viatura\n",
    ")\n",
    "\n",
    "# ================== 4) Construir \"cestos\" por viatura (por cluster) ==================\n",
    "basket = (base\n",
    "    .groupBy(\"cluster\", \"id_viatura\")\n",
    "    .agg(F.collect_set(\"servico_norm\").alias(\"items\"))\n",
    ")\n",
    "\n",
    "print(\"Total de cestos:\", basket.count())\n",
    "\n",
    "# ================== 5) FP-Growth por cluster (minSupport adaptativo) ==================\n",
    "def fp_growth_por_cluster(basket_df, cluster_id, min_conf=0.3, min_support_floor=0.01, min_abs=30):\n",
    "    dfc = basket_df.filter(F.col(\"cluster\")==cluster_id).select(\"items\")\n",
    "    n = dfc.count()\n",
    "    if n == 0:\n",
    "        print(f\"Cluster {cluster_id}: sem cestos.\")\n",
    "        return None, None\n",
    "    min_support = max(min_support_floor, min_abs / n)\n",
    "\n",
    "    print(f\"\\n=== Cluster {cluster_id} | n={n} | minSupport={min_support:.4f} | minConf={min_conf} ===\")\n",
    "    fpg = FPGrowth(itemsCol=\"items\", minSupport=min_support, minConfidence=min_conf)\n",
    "    model = fpg.fit(dfc)\n",
    "\n",
    "    itemsets = (model.freqItemsets\n",
    "        .withColumn(\"support\", F.col(\"freq\")/F.lit(n))\n",
    "        .orderBy(F.desc(\"freq\")))\n",
    "\n",
    "    # acrescentar lift manualmente (confidence / consequentSupport)\n",
    "    rules = (model.associationRules\n",
    "        .withColumn(\"lift\", F.col(\"confidence\")/F.col(\"consequentSupport\"))\n",
    "        .orderBy(F.desc(\"lift\"), F.desc(\"confidence\")))\n",
    "\n",
    "    return itemsets, rules\n",
    "\n",
    "# correr para todos os clusters\n",
    "clusters = [r[\"cluster\"] for r in df_clusters.select(\"cluster\").distinct().collect()]\n",
    "resultados = {}\n",
    "for k in sorted(clusters):\n",
    "    its, rls = fp_growth_por_cluster(basket, k, min_conf=0.30, min_support_floor=0.01, min_abs=30)\n",
    "    resultados[k] = (its, rls)\n",
    "    if its is not None:\n",
    "        print(\"\\n--- Frequent itemsets ---\")\n",
    "        its.show(20, truncate=False)\n",
    "        print(\"\\n--- Association rules ---\")\n",
    "        rls.select(\"antecedent\",\"consequent\",\"confidence\",\"lift\",\"support\").show(20, truncate=False)\n",
    "\n",
    "# ================== 6) (Opcional) guardar resultados ==================\n",
    "salvar = False\n",
    "if salvar:\n",
    "    for k, (its, rls) in resultados.items():\n",
    "        if its is not None:\n",
    "            its.write.mode(\"overwrite\").parquet(f\"/tmp/fp_itemsets_cluster_{k}\")\n",
    "            rls.write.mode(\"overwrite\").parquet(f\"/tmp/fp_rules_cluster_{k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76aa2fc-b149-4401-baac-2d07a1699171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================== Imports ==================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "# ================== Helpers ==================\n",
    "def remove_acentos(col):\n",
    "    # Mapa PT/BR comum (mai√∫sculas inclu√≠das)\n",
    "    accents = \"√£√°√†√¢√§√©√®√™√´√≠√¨√Æ√Ø√µ√≥√≤√¥√∂√∫√π√ª√º√ß√É√Å√Ä√Ç√Ñ√â√à√ä√ã√ç√å√é√è√ï√ì√í√î√ñ√ö√ô√õ√ú√á\"\n",
    "    no_acc  = \"aaaaaeeeeiiiiooooouuuucAAAAAEEEEIIIIOOOOOUUUUC\"\n",
    "    return F.translate(col, accents, no_acc)\n",
    "\n",
    "# ================== 1) Ler hist√≥rico e normalizar ==================\n",
    "tabela_servicos = \"sc_gold.historico_de_servicos_2\"\n",
    "df_serv = (spark.table(tabela_servicos)\n",
    "    .select(\n",
    "        F.col(\"viatura\").alias(\"id_viatura\"),\n",
    "        F.lower(remove_acentos(F.col(\"descricao_servico_pos_venda\"))).alias(\"serv_raw\"),\n",
    "        F.col(\"data_de_abertura\").alias(\"data_servico\")\n",
    "    )\n",
    "    .filter(F.col(\"serv_raw\").isNotNull() & (F.length(F.trim(F.col(\"serv_raw\"))) > 0))\n",
    ")\n",
    "\n",
    "# ================== 2) Mapear para categorias t√©cnicas (servico_norm) ==================\n",
    "# Op√ß√£o C: t√©cnico + manuten√ß√£o preventiva\n",
    "mapping = [\n",
    "    (\"revisao_programada\", [\"revis\", \"manutencao\", \"inspecao\", \" km\", \"anos\", \"60.000\", \"45.000\", \"30.000\", \"15.000\", \"60 000\", \"45 000\", \"30 000\", \"15 000\"]),\n",
    "    (\"oleo_motor\",         [\"oleo\", \"filtro oleo\", \"pack\"]),\n",
    "    (\"travagem\",           [\"pastilh\", \"trav\", \"disco\"]),\n",
    "    (\"pneus\",              [\"pneu\"]),\n",
    "    (\"bateria\",            [\"bateria\"]),\n",
    "    (\"ar_condicionado\",    [\"clima\", \"ar cond\", \" ac\", \"ac \", \"climat\"]),\n",
    "    (\"diagnostico_motor\",  [\"luz motor\", \"diagnos\", \"avaria\"]),\n",
    "    (\"sinistro\",           [\"chapa\", \"pintur\", \"sinistr\"]),\n",
    "    (\"ipo\",                [\"ipo\"]),\n",
    "    (\"combustivel\",        [\"combust\"])\n",
    "]\n",
    "\n",
    "# termos a excluir (ru√≠do administrativo/cosm√©tico)\n",
    "excluir = [\"lavag\", \"oferta\", \"check up\", \"tapete\", \"residu\", \"prepar\", \"estacao serv\", \"limpeza\", \"entrega\", \"pintura polimento\", \"chapas matric\"]\n",
    "\n",
    "df_map = df_serv.withColumn(\"servico_norm\", F.lit(None).cast(\"string\"))\n",
    "\n",
    "for cat, kws in mapping:\n",
    "    cond = F.lit(False)\n",
    "    for kw in kws:\n",
    "        cond = cond | F.col(\"serv_raw\").contains(kw)\n",
    "    df_map = df_map.withColumn(\"servico_norm\", F.when(cond, F.lit(cat)).otherwise(F.col(\"servico_norm\")))\n",
    "\n",
    "cond_ruido = F.lit(False)\n",
    "for kw in excluir:\n",
    "    cond_ruido = cond_ruido | F.col(\"serv_raw\").contains(kw)\n",
    "\n",
    "df_clean = df_map.filter(~cond_ruido & F.col(\"servico_norm\").isNotNull())\n",
    "\n",
    "# (opcional) ver distribui√ß√£o das categorias\n",
    "df_clean.groupBy(\"servico_norm\").count().orderBy(F.desc(\"count\")).show(50, truncate=False)\n",
    "\n",
    "# ================== 3) Juntar clusters (ajusta colunas se necess√°rio) ==================\n",
    "# df_clusters deve existir da fase de clustering e conter: id (viatura), cluster\n",
    "df_clusters_sel = df_clusters.select(F.col(\"id\").alias(\"id_viatura\"), \"cluster\")\n",
    "\n",
    "base = (df_clean\n",
    "    .join(df_clusters_sel, on=\"id_viatura\", how=\"inner\")\n",
    "    .select(\"id_viatura\", \"cluster\", \"servico_norm\")\n",
    "    .distinct()  # evita duplica√ß√£o do mesmo servi√ßo na mesma viatura\n",
    ")\n",
    "\n",
    "# ================== 4) Construir baskets por viatura (por cluster) ==================\n",
    "basket = (base\n",
    "    .groupBy(\"cluster\", \"id_viatura\")\n",
    "    .agg(F.collect_set(\"servico_norm\").alias(\"items\"))\n",
    ")\n",
    "\n",
    "print(\"Total de cestos:\", basket.count())\n",
    "\n",
    "# ================== 5) FP-Growth por cluster (minSupport adaptativo) ==================\n",
    "def fp_growth_por_cluster(basket_df, cluster_id, min_conf=0.3, min_support_floor=0.01, min_abs=30):\n",
    "    dfc = basket_df.filter(F.col(\"cluster\")==cluster_id).select(\"items\")\n",
    "    n = dfc.count()\n",
    "    if n == 0:\n",
    "        print(f\"Cluster {cluster_id}: sem cestos.\")\n",
    "        return None, None\n",
    "    min_support = max(min_support_floor, min_abs / n)\n",
    "    print(f\"\\n=== Cluster {cluster_id} | n={n} | minSupport={min_support:.4f} | minConf={min_conf} ===\")\n",
    "\n",
    "    fpg = FPGrowth(itemsCol=\"items\", minSupport=min_support, minConfidence=min_conf)\n",
    "    model = fpg.fit(dfc)\n",
    "\n",
    "    itemsets = (model.freqItemsets\n",
    "        .withColumn(\"support\", F.col(\"freq\")/F.lit(n))\n",
    "        .orderBy(F.desc(\"freq\")))\n",
    "\n",
    "    rules = (model.associationRules\n",
    "        .withColumn(\"lift\", F.col(\"confidence\")/F.col(\"consequentSupport\"))\n",
    "        .orderBy(F.desc(\"lift\"), F.desc(\"confidence\")))\n",
    "\n",
    "    return itemsets, rules\n",
    "\n",
    "clusters = [r[\"cluster\"] for r in df_clusters.select(\"cluster\").distinct().collect()]\n",
    "resultados = {}\n",
    "for k in sorted(clusters):\n",
    "    its, rls = fp_growth_por_cluster(basket, k, min_conf=0.30, min_support_floor=0.01, min_abs=30)\n",
    "    resultados[k] = (its, rls)\n",
    "    if its is not None:\n",
    "        print(\"\\n--- Frequent itemsets ---\")\n",
    "        its.show(20, truncate=False)\n",
    "        print(\"\\n--- Association rules ---\")\n",
    "        rls.select(\"antecedent\",\"consequent\",\"confidence\",\"lift\",\"support\").show(20, truncate=False)\n",
    "\n",
    "# ================== 6) (Opcional) guardar resultados ==================\n",
    "# salvar = True\n",
    "# if salvar:\n",
    "#     for k, (its, rls) in resultados.items():\n",
    "#         if its is not None:\n",
    "#             its.write.mode(\"overwrite\").parquet(f\"/tmp/fp_itemsets_cluster_{k}\")\n",
    "#             rls.write.mode(\"overwrite\").parquet(f\"/tmp/fp_rules_cluster_{k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1240ac2-2274-4c76-bf84-1bce26edb032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FALHAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98fa92d1-d141-49be-9648-9a7d90850baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Construir pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = []\n",
    "stages += indexers\n",
    "stages += encoders\n",
    "stages += [assembler_num, scaler, final_assembler]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "df_ready = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b090d0b7-e942-4cad-854d-e5c6445010e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Elbow Method\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "costs = []\n",
    "Ks = range(2, 11)\n",
    "\n",
    "for k in Ks:\n",
    "    km = KMeans(featuresCol=\"features\", k=k, seed=42)\n",
    "    model = km.fit(df_ready)\n",
    "    # custo = soma das dist√¢ncias ao centro (in√©rcia)\n",
    "    cost = model.summary.trainingCost\n",
    "    costs.append(cost)\n",
    "\n",
    "plt.plot(Ks, costs, marker=\"o\")\n",
    "plt.xlabel(\"k (n¬∫ clusters)\")\n",
    "plt.ylabel(\"In√©rcia (trainingCost)\")\n",
    "plt.title(\"M√©todo do Cotovelo\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19f78a6-9623-43fb-b537-e4c2d967364b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Silhouette Score\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\",\n",
    "                                metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "\n",
    "results = []\n",
    "for k in Ks:\n",
    "    km = KMeans(featuresCol=\"features\", k=k, seed=42)\n",
    "    model = km.fit(df_ready)\n",
    "    preds = model.transform(df_ready)\n",
    "    score = evaluator.evaluate(preds)\n",
    "    results.append(score)\n",
    "    print(f\"k={k}, silhouette={score:.4f}\")\n",
    "\n",
    "plt.plot(Ks, results, marker=\"o\")\n",
    "plt.xlabel(\"k (n¬∫ clusters)\")\n",
    "plt.ylabel(\"Silhouette\")\n",
    "plt.title(\"Silhouette vs k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d40314-42b9-4ee3-bf0f-03dc4c2520cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Avaliar k por Elbow (trainingCost) e Silhouette e treinar modelo final\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "import gc\n",
    "\n",
    "# 0) Prepara√ß√£o: df_ready tem de ter a coluna 'features'\n",
    "assert \"features\" in df_ready.columns, \"df_ready precisa da coluna 'features'.\"\n",
    "\n",
    "Ks = list(range(2, 11))  # ajusta se quiseres\n",
    "results = []\n",
    "\n",
    "# Usar a coluna default 'prediction' para evitar erros no evaluator\n",
    "evaluator = ClusteringEvaluator(\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"silhouette\",\n",
    "    distanceMeasure=\"squaredEuclidean\"\n",
    ")\n",
    "\n",
    "for k in Ks:\n",
    "    km = KMeans(featuresCol=\"features\", k=k, seed=42)  # predictionCol default = 'prediction'\n",
    "    model = km.fit(df_ready)\n",
    "    preds = model.transform(df_ready)\n",
    "    \n",
    "    # Elbow: in√©rcia (soma das dist√¢ncias ao centro)\n",
    "    inertia = model.summary.trainingCost\n",
    "    \n",
    "    # Silhouette\n",
    "    sil = evaluator.evaluate(preds)\n",
    "    \n",
    "    results.append((k, inertia, sil))\n",
    "    print(f\"k={k:2d} | inertia={inertia:.2f} | silhouette={sil:.4f}\")\n",
    "    \n",
    "    # libertar cache de modelos/DFs (Spark Connect)\n",
    "    del model, preds\n",
    "    gc.collect()\n",
    "\n",
    "# 1) Tabela de resultados\n",
    "res_df = spark.createDataFrame(results, schema=[\"k\", \"inertia\", \"silhouette\"]) \\\n",
    "              .orderBy(\"k\")\n",
    "res_df.show(truncate=False)\n",
    "\n",
    "# 2) Escolher k: m√°ximo silhouette (em caso de empate, menor k)\n",
    "row_best = res_df.orderBy(F.col(\"silhouette\").desc(), F.col(\"k\").asc()).first()\n",
    "best_k = row_best[\"k\"]\n",
    "best_s = row_best[\"silhouette\"]\n",
    "print(f\"\\n>> Melhor k pelo Silhouette: k={best_k} (silhouette={best_s:.4f})\")\n",
    "\n",
    "# (Opcional) Se tamb√©m quiseres um \"cotovelo\" heur√≠stico simples:\n",
    "# calcula a maior queda relativa de in√©rcia\n",
    "from pyspark.sql.window import Window\n",
    "w = Window.orderBy(\"k\")\n",
    "elbow_df = (res_df\n",
    "    .withColumn(\"inertia_prev\", F.lag(\"inertia\").over(w))\n",
    "    .withColumn(\"drop\", (F.col(\"inertia_prev\") - F.col(\"inertia\"))/F.col(\"inertia_prev\"))\n",
    ")\n",
    "elbow_row = elbow_df.orderBy(F.col(\"drop\").desc_nulls_last()).first()\n",
    "elbow_k = elbow_row[\"k\"] if elbow_row and elbow_row[\"drop\"] is not None else None\n",
    "print(f\">> Sugerido pelo 'cotovelo' (heur√≠stico): k={elbow_k}\")\n",
    "\n",
    "# 3) Treinar KMeans final com o k escolhido (Silhouette)\n",
    "final_kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=best_k, seed=42)\n",
    "final_model = final_kmeans.fit(df_ready)\n",
    "df_clusters = final_model.transform(df_ready)\n",
    "\n",
    "# 4) Distribui√ß√£o por cluster\n",
    "df_clusters.groupBy(\"cluster\").count().orderBy(\"cluster\").show()\n",
    "\n",
    "# 5) (Opcional) guardar resultados\n",
    "#df_clusters.write.mode(\"overwrite\").saveAsTable(\"sc_gold.viaturas_2_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b812767-cc59-4521-8b6f-55f4cd0dbb0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Reduzir features a 2 dimens√µes para visualiza√ß√£o\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca2d\")\n",
    "pca_model = pca.fit(df_clusters)\n",
    "df_pca = pca_model.transform(df_clusters).select(\"pca2d\", \"cluster\")\n",
    "\n",
    "# 2) Converter para Pandas para plotar\n",
    "pdf = df_pca.toPandas()\n",
    "pdf[[\"x\", \"y\"]] = pdf[\"pca2d\"].apply(lambda v: pd.Series([float(v[0]), float(v[1])]))\n",
    "\n",
    "# 3) Desenhar gr√°fico\n",
    "plt.figure(figsize=(8,6))\n",
    "for c in sorted(pdf[\"cluster\"].unique()):\n",
    "    subset = pdf[pdf[\"cluster\"] == c]\n",
    "    plt.scatter(subset[\"x\"], subset[\"y\"], label=f\"Cluster {c}\", alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(\"Clusters K-Means (proje√ß√£o PCA 2D)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43eb4bf4-9eb8-409e-af77-c5805149c4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**# dbscan nao consegui por causa da cache **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3903125-afe4-437b-918e-ea572446d4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apaga objetos de modelos j√° criados nesta sess√£o\n",
    "for name in [\"model\", \"pca_model\", \"pca_db_model\", \"pca_2d_model\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0093b634-8ac5-4fc7-a6cf-0073a4b1ad5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"dbscan-viaturas\")\n",
    "         .config(\"spark.connect.ml.cache.size\", str(2 * 1024 * 1024 * 1024))  # 2 GB\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87354bce-ea89-4c6b-a972-ef0ce33de677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_**Association Rules / Market Basket Analysis**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b484b5-1420-4e60-babe-efea44d0f302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Se o teu DF com o resultado do k-means/DBSCAN se chama df_clusters:\n",
    "# certificar que tem as colunas id e cluster\n",
    "if \"cluster\" in df_clusters.columns:\n",
    "    df_id_cluster = df_clusters.select(\"id\", F.col(\"cluster\").cast(\"int\").alias(\"cluster\"))\n",
    "elif \"prediction\" in df_clusters.columns:\n",
    "    df_id_cluster = df_clusters.select(\"id\", F.col(\"prediction\").cast(\"int\").alias(\"cluster\"))\n",
    "else:\n",
    "    raise ValueError(\"O DF de clusters tem de ter 'cluster' ou 'prediction' e a coluna 'id'.\")\n",
    "\n",
    "# (opcional) guardar numa tabela para uso posterior\n",
    "df_id_cluster.write.mode(\"overwrite\").saveAsTable(\"sc_gold.viaturas_clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf440ad4-636f-4e7f-a185-758f21c599c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Limpar nulos no servi√ßo\n",
    "df_hist = spark.table(\"sc_gold.historico_de_servicos_2\")\n",
    "\n",
    "# Usa o c√≥digo do servi√ßo se for mais est√°vel que a descri√ß√£o\n",
    "# Aqui vou usar descricao_servico_pos_venda\n",
    "df_hist = df_hist.filter(df_hist.descricao_servico_pos_venda.isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839f8ba9-d384-47a0-98c6-20a0d5d2adf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join: viatura (hist√≥rico) ‚Üî id (clusters)\n",
    "df_join = df_hist.join(df_clusters, df_hist.viatura == df_clusters.id, how=\"inner\")\n",
    "\n",
    "\n",
    "# tenta usar uma chave de visita se existir na tabela\n",
    "cols = df_join.columns\n",
    "visit_key = None\n",
    "for candidate in [\"numero_do_servico_pos_venda\",\"canal_de_venda\",\"tipo_de_servico\"]:\n",
    "    if candidate in cols:\n",
    "        visit_key = candidate\n",
    "        break\n",
    "\n",
    "group_cols = [\"id\",\"cluster\"] + ([visit_key] if visit_key else [])\n",
    "\n",
    "baskets = (df_join\n",
    "    .groupBy(*group_cols)\n",
    "    .agg(F.collect_set(\"descricao_servico_pos_venda\").alias(\"items\"))\n",
    "    .filter(F.size(\"items\") > 0))\n",
    "\n",
    "# ver formato\n",
    "baskets.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629d6b3f-6eaa-4fd5-b856-7b9dfa2bbfab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#nao sucedi cache offloading is enabled\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "# -------- Par√¢metros --------\n",
    "TABLE_HIST = \"sc_gold.historico_de_servicos_2\"\n",
    "TABLE_CLUST = \"sc_gold.viaturas_clusters\"   # (id, cluster)\n",
    "MIN_SUP = 0.01\n",
    "MIN_CONF = 0.20\n",
    "TOP_N = 30\n",
    "\n",
    "# -------- 1) Hist√≥rico -> baskets --------\n",
    "df_hist = spark.table(TABLE_HIST)\n",
    "\n",
    "# item: usa a DESCRI√á√ÉO (o n√∫mero √© a chave do cesto)\n",
    "item_col = F.col(\"descricao_servico_pos_venda\").alias(\"item_raw\")\n",
    "\n",
    "df_hist = (df_hist\n",
    "           .select(\"*\", item_col)\n",
    "           .filter(F.col(\"viatura\").isNotNull())\n",
    "           .filter(F.col(\"numero_do_servico_pos_venda\").isNotNull())\n",
    "           .filter(F.col(\"item_raw\").isNotNull()))\n",
    "\n",
    "# clusters (id -> cluster)\n",
    "df_clu = spark.table(TABLE_CLUST).select(\"id\",\"cluster\")\n",
    "\n",
    "# join: viatura (hist√≥rico) ‚Üî id (clusters)\n",
    "df_join = df_hist.join(df_clu, df_hist.viatura == df_clu.id, how=\"inner\").drop(df_clu.id)\n",
    "\n",
    "# CHAVE DE CESTO = numero_do_servico_pos_venda\n",
    "visit_key = \"numero_do_servico_pos_venda\"\n",
    "\n",
    "group_cols = [\"viatura\",\"cluster\", visit_key]\n",
    "\n",
    "baskets = (df_join\n",
    "           .groupBy(*group_cols)\n",
    "           .agg(F.collect_set(\"item_raw\").alias(\"items\"))\n",
    "           .filter(F.size(\"items\") > 0))\n",
    "\n",
    "# -------- 2) FP-Growth por cluster --------\n",
    "clusters = [r[\"cluster\"] for r in baskets.select(\"cluster\").distinct().collect()]\n",
    "results = []\n",
    "\n",
    "for c in clusters:\n",
    "    b_c = baskets.filter(F.col(\"cluster\")==c).select(\"items\")\n",
    "    n_trans = b_c.count()\n",
    "    if n_trans == 0:\n",
    "        continue\n",
    "\n",
    "    # >>> CORRIGIDO: n√£o usamos predictionCol=None <<<\n",
    "    fp = FPGrowth(minSupport=MIN_SUP,\n",
    "                  minConfidence=MIN_CONF,\n",
    "                  itemsCol=\"items\")   # usa default predictionCol=\"prediction\"\n",
    "    model = fp.fit(b_c)\n",
    "\n",
    "    rules = model.associationRules      # antecedent, consequent, confidence\n",
    "    freq  = model.freqItemsets          # items, freq\n",
    "\n",
    "    # suportes e lift\n",
    "    sup_all = freq.withColumn(\"items_str\", F.array_join(\"items\",\"|\"))\n",
    "    sup_ab = sup_all.select(F.col(\"items_str\").alias(\"ab_str\"),\n",
    "                            (F.col(\"freq\")/F.lit(n_trans)).alias(\"support_ab\"))\n",
    "    sup_a  = sup_all.select(F.col(\"items_str\").alias(\"a_str\"),\n",
    "                            (F.col(\"freq\")/F.lit(n_trans)).alias(\"support_a\"))\n",
    "    sup_b  = sup_all.select(F.col(\"items_str\").alias(\"b_str\"),\n",
    "                            (F.col(\"freq\")/F.lit(n_trans)).alias(\"support_b\"))\n",
    "\n",
    "    rules_aug = (rules\n",
    "        .withColumn(\"antecedent_str\", F.array_join(\"antecedent\",\"|\"))\n",
    "        .withColumn(\"consequent_str\", F.array_join(\"consequent\",\"|\"))\n",
    "        .join(sup_ab, F.concat_ws(\"|\",\"antecedent_str\",\"consequent_str\")==F.col(\"ab_str\"), \"left\")\n",
    "        .join(sup_a, F.col(\"antecedent_str\")==F.col(\"a_str\"), \"left\")\n",
    "        .join(sup_b, F.col(\"consequent_str\")==F.col(\"b_str\"), \"left\")\n",
    "        .withColumn(\"lift\", F.col(\"support_ab\")/(F.col(\"support_a\")*F.col(\"support_b\")))\n",
    "        .withColumn(\"cluster\", F.lit(c))\n",
    "        .select(\"cluster\",\"antecedent\",\"consequent\",\n",
    "                \"support_a\",\"support_b\",\"support_ab\",\"confidence\",\"lift\"))\n",
    "\n",
    "    rules_top = (rules_aug\n",
    "        .filter(F.size(\"consequent\")==1)\n",
    "        .orderBy(F.col(\"lift\").desc(), F.col(\"support_ab\").desc())\n",
    "        .limit(TOP_N))\n",
    "\n",
    "    results.append(rules_top)\n",
    "\n",
    "rules_all = None\n",
    "if results:\n",
    "    rules_all = results[0]\n",
    "    for r in results[1:]:\n",
    "        rules_all = rules_all.unionByName(r)\n",
    "\n",
    "# -------- 3) Mostrar --------\n",
    "if rules_all is None or rules_all.rdd.isEmpty():\n",
    "    print(\"Sem regras para os par√¢metros atuais. Ajusta MIN_SUP/MIN_CONF.\")\n",
    "else:\n",
    "    rules_all.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5c67cb-3301-43d0-a874-0a754531de1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlxtend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60937168-2b54-404b-96eb-b18dc336f755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#usar python (nao consegui correr)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# -------- Par√¢metros --------\n",
    "TABLE_HIST = \"sc_gold.historico_de_servicos_2\"\n",
    "TABLE_CLUST = \"sc_gold.viaturas_clusters\"   # (id, cluster)\n",
    "MIN_SUP = 0.01\n",
    "MIN_CONF = 0.20\n",
    "TOP_N = 30\n",
    "\n",
    "# -------- 1) Hist√≥rico -> baskets --------\n",
    "df_hist = spark.table(TABLE_HIST)\n",
    "\n",
    "# item: usa a DESCRI√á√ÉO\n",
    "item_col = F.col(\"descricao_servico_pos_venda\").alias(\"item_raw\")\n",
    "\n",
    "df_hist = (df_hist\n",
    "           .select(\"*\", item_col)\n",
    "           .filter(F.col(\"viatura\").isNotNull())\n",
    "           .filter(F.col(\"numero_do_servico_pos_venda\").isNotNull())\n",
    "           .filter(F.col(\"item_raw\").isNotNull()))\n",
    "\n",
    "# clusters (id -> cluster)\n",
    "df_clu = spark.table(TABLE_CLUST).select(\"id\",\"cluster\")\n",
    "\n",
    "# join: viatura (hist√≥rico) ‚Üî id (clusters)\n",
    "df_join = df_hist.join(df_clu, df_hist.viatura == df_clu.id, how=\"inner\").drop(df_clu.id)\n",
    "\n",
    "# CHAVE DE CESTO = numero_do_servico_pos_venda\n",
    "visit_key = \"numero_do_servico_pos_venda\"\n",
    "\n",
    "group_cols = [\"viatura\",\"cluster\", visit_key]\n",
    "\n",
    "baskets = (df_join\n",
    "           .groupBy(*group_cols)\n",
    "           .agg(F.collect_set(\"item_raw\").alias(\"items\"))\n",
    "           .filter(F.size(\"items\") > 0))\n",
    "\n",
    "# -------- 2) Converter para Pandas --------\n",
    "baskets_pd = baskets.toPandas()\n",
    "print(\"Formato dos cestos:\", baskets_pd.head())\n",
    "\n",
    "# -------- 3) Apriori com mlxtend --------\n",
    "!pip install mlxtendfrom mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "results = []\n",
    "\n",
    "for c in sorted(baskets_pd[\"cluster\"].unique()):\n",
    "    basket_c = baskets_pd[baskets_pd[\"cluster\"] == c][\"items\"].tolist()\n",
    "    if not basket_c:\n",
    "        continue\n",
    "\n",
    "    # one-hot encoding\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(basket_c).transform(basket_c)\n",
    "    df_items = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    # Apriori\n",
    "    freq = apriori(df_items, min_support=MIN_SUP, use_colnames=True)\n",
    "\n",
    "    # Regras de associa√ß√£o\n",
    "    rules = association_rules(freq, metric=\"confidence\", min_threshold=MIN_CONF)\n",
    "    rules[\"cluster\"] = c\n",
    "\n",
    "    # Ordenar por lift e limitar\n",
    "    rules_top = rules.sort_values([\"lift\",\"support\"], ascending=[False,False]).head(TOP_N)\n",
    "    results.append(rules_top)\n",
    "\n",
    "# -------- 4) Concatenar resultados --------\n",
    "import pandas as pd\n",
    "if results:\n",
    "    rules_all = pd.concat(results, ignore_index=True)\n",
    "    print(rules_all[[\"cluster\",\"antecedents\",\"consequents\",\"support\",\"confidence\",\"lift\"]])\n",
    "else:\n",
    "    print(\"Sem regras para os par√¢metros definidos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303fd2aa-8dbd-4988-9199-05fe6f2d4c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#usar PANDAS PQ NAO DEU OS ULTIMOS 2 BLOCOS\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "def apriori_from_baskets(baskets, min_support=0.01):\n",
    "    n_trans = len(baskets)\n",
    "    # Contar frequ√™ncias de itens √∫nicos\n",
    "    item_counts = {}\n",
    "    for basket in baskets:\n",
    "        for item in set(basket):\n",
    "            item_counts[item] = item_counts.get(item, 0) + 1\n",
    "    freq_items = {frozenset([k]): v/n_trans for k,v in item_counts.items() if v/n_trans >= min_support}\n",
    "\n",
    "    # Contar pares\n",
    "    pair_counts = {}\n",
    "    for basket in baskets:\n",
    "        for a,b in combinations(set(basket), 2):\n",
    "            pair = frozenset([a,b])\n",
    "            pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "    freq_pairs = {k:v/n_trans for k,v in pair_counts.items() if v/n_trans >= min_support}\n",
    "\n",
    "    return freq_items, freq_pairs\n",
    "\n",
    "# Exemplo de uso (baskets_pd j√° criado antes)\n",
    "baskets_cluster0 = baskets_pd[baskets_pd[\"cluster\"]==0][\"items\"].tolist()\n",
    "freq1, freq2 = apriori_from_baskets(baskets_cluster0, min_support=0.01)\n",
    "\n",
    "print(\"Itens frequentes:\", freq1)\n",
    "print(\"Pares frequentes:\", freq2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9255e045-f472-4c5b-9d91-f90f31688274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calcular confidence e lift\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "def apriori_from_baskets(baskets, min_support=0.01):\n",
    "    n_trans = len(baskets)\n",
    "    # Contar frequ√™ncias de itens √∫nicos\n",
    "    item_counts = {}\n",
    "    for basket in baskets:\n",
    "        for item in set(basket):\n",
    "            item_counts[item] = item_counts.get(item, 0) + 1\n",
    "    freq_items = {frozenset([k]): v/n_trans for k,v in item_counts.items() if v/n_trans >= min_support}\n",
    "\n",
    "    # Contar pares\n",
    "    pair_counts = {}\n",
    "    for basket in baskets:\n",
    "        for a,b in combinations(set(basket), 2):\n",
    "            pair = frozenset([a,b])\n",
    "            pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "    freq_pairs = {k:v/n_trans for k,v in pair_counts.items() if v/n_trans >= min_support}\n",
    "\n",
    "    return freq_items, freq_pairs\n",
    "\n",
    "def generate_rules(freq_items, freq_pairs):\n",
    "    rules = []\n",
    "    for pair, sup_ab in freq_pairs.items():   # pares frequentes\n",
    "        items = list(pair)\n",
    "        for i in range(2):\n",
    "            A = frozenset([items[i]])\n",
    "            B = frozenset([items[1-i]])\n",
    "            sup_a = freq_items.get(A, 0)\n",
    "            sup_b = freq_items.get(B, 0)\n",
    "\n",
    "            if sup_a > 0 and sup_b > 0:\n",
    "                conf = sup_ab / sup_a\n",
    "                lift = sup_ab / (sup_a * sup_b)\n",
    "                rules.append({\n",
    "                    \"antecedent\": list(A),\n",
    "                    \"consequent\": list(B),\n",
    "                    \"support_ab\": sup_ab,\n",
    "                    \"support_a\": sup_a,\n",
    "                    \"support_b\": sup_b,\n",
    "                    \"confidence\": conf,\n",
    "                    \"lift\": lift\n",
    "                })\n",
    "    return pd.DataFrame(rules)\n",
    "\n",
    "# --- Exemplo com os cestos de um cluster ---\n",
    "baskets_cluster0 = baskets_pd[baskets_pd[\"cluster\"]==0][\"items\"].tolist()\n",
    "\n",
    "freq1, freq2 = apriori_from_baskets(baskets_cluster0, min_support=0.01)\n",
    "rules_df = generate_rules(freq1, freq2)\n",
    "\n",
    "print(rules_df.sort_values(\"lift\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe708c36-162a-468d-8a30-a0a6125614f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calcular confidence e lift (outra forma)\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "COLS = [\"cluster\",\"antecedent\",\"consequent\",\n",
    "        \"support_ab\",\"support_a\",\"support_b\",\"confidence\",\"lift\"]\n",
    "\n",
    "def apriori_from_baskets(baskets, min_support=0.005):\n",
    "    n_trans = len(baskets)\n",
    "    if n_trans == 0:\n",
    "        return {}, {}\n",
    "    # 1-itemsets\n",
    "    item_counts = {}\n",
    "    for basket in baskets:\n",
    "        for item in set(basket):\n",
    "            item_counts[item] = item_counts.get(item, 0) + 1\n",
    "    freq_items = {frozenset([k]): v/n_trans for k,v in item_counts.items() if v/n_trans >= min_support}\n",
    "    # 2-itemsets\n",
    "    pair_counts = {}\n",
    "    for basket in baskets:\n",
    "        for a,b in combinations(set(basket), 2):\n",
    "            pair = frozenset([a,b])\n",
    "            pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "    freq_pairs = {k:v/n_trans for k,v in pair_counts.items() if v/n_trans >= min_support}\n",
    "    return freq_items, freq_pairs\n",
    "\n",
    "def generate_rules_df(freq_items, freq_pairs, cluster_id=None):\n",
    "    # garante DataFrame com colunas mesmo se vazio\n",
    "    rows = []\n",
    "    for pair, sup_ab in freq_pairs.items():\n",
    "        items = list(pair)\n",
    "        for i in range(2):\n",
    "            A = frozenset([items[i]])\n",
    "            B = frozenset([items[1-i]])\n",
    "            sup_a = freq_items.get(A, 0.0)\n",
    "            sup_b = freq_items.get(B, 0.0)\n",
    "            if sup_a > 0 and sup_b > 0:\n",
    "                conf = sup_ab / sup_a\n",
    "                lift = sup_ab / (sup_a * sup_b)\n",
    "                rows.append({\n",
    "                    \"cluster\": cluster_id,\n",
    "                    \"antecedent\": list(A),\n",
    "                    \"consequent\": list(B),\n",
    "                    \"support_ab\": sup_ab,\n",
    "                    \"support_a\": sup_a,\n",
    "                    \"support_b\": sup_b,\n",
    "                    \"confidence\": conf,\n",
    "                    \"lift\": lift\n",
    "                })\n",
    "    return pd.DataFrame(rows, columns=COLS)\n",
    "\n",
    "# ---- Minerar para todos os clusters (usando baskets_pd j√° criado) ----\n",
    "def mine_all_clusters(baskets_pd, min_support=0.005, top_n=30, min_conf=None):\n",
    "    all_rules = []\n",
    "    for c in sorted(baskets_pd[\"cluster\"].unique()):\n",
    "        baskets_c = baskets_pd[baskets_pd[\"cluster\"] == c][\"items\"].tolist()\n",
    "        freq1, freq2 = apriori_from_baskets(baskets_c, min_support=min_support)\n",
    "        df_rules = generate_rules_df(freq1, freq2, cluster_id=c)\n",
    "\n",
    "        # (opcional) filtrar por confidence m√≠nima\n",
    "        if min_conf is not None and not df_rules.empty:\n",
    "            df_rules = df_rules[df_rules[\"confidence\"] >= min_conf]\n",
    "\n",
    "        # ordenar com seguran√ßa\n",
    "        if not df_rules.empty:\n",
    "            df_rules = df_rules.sort_values([\"lift\",\"support_ab\"], ascending=[False, False]).head(top_n)\n",
    "        all_rules.append(df_rules)\n",
    "\n",
    "    # concatenar garantindo colunas mesmo que todos vazios\n",
    "    if len(all_rules) == 0:\n",
    "        return pd.DataFrame(columns=COLS)\n",
    "    out = pd.concat(all_rules, ignore_index=True) if any([not df.empty for df in all_rules]) else pd.DataFrame(columns=COLS)\n",
    "    return out\n",
    "\n",
    "# ---- Executar ----\n",
    "rules_all = mine_all_clusters(baskets_pd, min_support=0.01, top_n=30, min_conf=0.2)\n",
    "\n",
    "if rules_all.empty:\n",
    "    print(\"Sem regras com os par√¢metros atuais. Tenta diminuir min_support (ex.: 0.005) e/ou min_conf.\")\n",
    "else:\n",
    "    print(rules_all.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 757773506382328,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Association_Rules_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
